"use strict";(globalThis.webpackChunkphysical_ai_humanoid_robotics_book=globalThis.webpackChunkphysical_ai_humanoid_robotics_book||[]).push([[188],{1965(n,e,t){t.r(e),t.d(e,{assets:()=>l,contentTitle:()=>r,default:()=>p,frontMatter:()=>i,metadata:()=>s,toc:()=>c});var o=t(4848),a=t(8453);const i={sidebar_position:11,title:"Chapter 11: Nav2 and Path Planning for Humanoid Robots"},r="Chapter 11: Nav2 and Path Planning for Humanoid Robots",s={id:"part4/chapter11",title:"Chapter 11: Nav2 and Path Planning for Humanoid Robots",description:"Learning Objectives",source:"@site/docs/part4/chapter11.md",sourceDirName:"part4",slug:"/part4/chapter11",permalink:"/Physical-AI-Humanoid-Robotics/docs/part4/chapter11",draft:!1,unlisted:!1,editUrl:"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/part4/chapter11.md",tags:[],version:"current",sidebarPosition:11,frontMatter:{sidebar_position:11,title:"Chapter 11: Nav2 and Path Planning for Humanoid Robots"},sidebar:"tutorialSidebar",previous:{title:"Chapter 10: Isaac ROS and Hardware-Accelerated Perception",permalink:"/Physical-AI-Humanoid-Robotics/docs/part4/chapter10"},next:{title:"Chapter 12: Sim-to-Real Transfer Techniques",permalink:"/Physical-AI-Humanoid-Robotics/docs/part4/chapter12"}},l={},c=[{value:"Learning Objectives",id:"learning-objectives",level:2},{value:"Introduction to Nav2 for Humanoid Robots",id:"introduction-to-nav2-for-humanoid-robots",level:2},{value:"Nav2 Architecture Overview",id:"nav2-architecture-overview",level:3},{value:"Challenges with Humanoid Navigation",id:"challenges-with-humanoid-navigation",level:3},{value:"Nav2 Configuration for Humanoid Robots",id:"nav2-configuration-for-humanoid-robots",level:2},{value:"Basic Nav2 Setup",id:"basic-nav2-setup",level:3},{value:"Humanoid-Specific Parameters",id:"humanoid-specific-parameters",level:3},{value:"Humanoid Behavior Tree",id:"humanoid-behavior-tree",level:3},{value:"Bipedal Path Planning Algorithms",id:"bipedal-path-planning-algorithms",level:2},{value:"Humanoid-Specific Path Planning",id:"humanoid-specific-path-planning",level:3},{value:"Step Planning for Humanoid Locomotion",id:"step-planning-for-humanoid-locomotion",level:3},{value:"Navigation Systems for Humanoid Robots",id:"navigation-systems-for-humanoid-robots",level:2},{value:"Humanoid Navigation Server",id:"humanoid-navigation-server",level:3},{value:"Reinforcement Learning for Robot Control",id:"reinforcement-learning-for-robot-control",level:2},{value:"Deep Reinforcement Learning for Navigation",id:"deep-reinforcement-learning-for-navigation",level:3},{value:"Humanoid Locomotion Control with RL",id:"humanoid-locomotion-control-with-rl",level:3},{value:"Knowledge Check",id:"knowledge-check",level:2},{value:"Summary",id:"summary",level:2},{value:"Next Steps",id:"next-steps",level:2}];function _(n){const e={code:"code",h1:"h1",h2:"h2",h3:"h3",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,a.R)(),...n.components};return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsx)(e.h1,{id:"chapter-11-nav2-and-path-planning-for-humanoid-robots",children:"Chapter 11: Nav2 and Path Planning for Humanoid Robots"}),"\n",(0,o.jsx)(e.h2,{id:"learning-objectives",children:"Learning Objectives"}),"\n",(0,o.jsxs)(e.ul,{children:["\n",(0,o.jsx)(e.li,{children:"Understand Nav2 navigation stack for humanoid robots"}),"\n",(0,o.jsx)(e.li,{children:"Implement path planning algorithms for bipedal locomotion"}),"\n",(0,o.jsx)(e.li,{children:"Design navigation systems for legged robots"}),"\n",(0,o.jsx)(e.li,{children:"Apply reinforcement learning for robot control"}),"\n"]}),"\n",(0,o.jsx)(e.h2,{id:"introduction-to-nav2-for-humanoid-robots",children:"Introduction to Nav2 for Humanoid Robots"}),"\n",(0,o.jsx)(e.p,{children:"Nav2 (Navigation 2) is ROS 2's state-of-the-art navigation stack, designed for mobile robots. For humanoid robots, Nav2 requires specialized adaptations to handle the unique challenges of bipedal locomotion, balance constraints, and dynamic movement patterns."}),"\n",(0,o.jsx)(e.h3,{id:"nav2-architecture-overview",children:"Nav2 Architecture Overview"}),"\n",(0,o.jsx)(e.p,{children:"Nav2 consists of several key components:"}),"\n",(0,o.jsxs)(e.ul,{children:["\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Navigation Server"}),": Main orchestrator of navigation tasks"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Planners"}),": Global and local path planning"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Controllers"}),": Trajectory execution and control"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Behavior Trees"}),": Task orchestration and decision making"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Sensors"}),": Perception and localization integration"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Transforms"}),": Coordinate frame management"]}),"\n"]}),"\n",(0,o.jsx)(e.h3,{id:"challenges-with-humanoid-navigation",children:"Challenges with Humanoid Navigation"}),"\n",(0,o.jsx)(e.p,{children:"Unlike wheeled robots, humanoid robots face unique navigation challenges:"}),"\n",(0,o.jsxs)(e.ul,{children:["\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Dynamic Balance"}),": Maintaining stability during movement"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Step Planning"}),": Careful foot placement on terrain"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Bipedal Kinematics"}),": Complex movement patterns"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Energy Efficiency"}),": Minimizing power consumption"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Terrain Adaptation"}),": Handling various surface types"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Obstacle Avoidance"}),": Navigating with human-like behavior"]}),"\n"]}),"\n",(0,o.jsx)(e.h2,{id:"nav2-configuration-for-humanoid-robots",children:"Nav2 Configuration for Humanoid Robots"}),"\n",(0,o.jsx)(e.h3,{id:"basic-nav2-setup",children:"Basic Nav2 Setup"}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-python",children:"# humanoid_nav2_config.py\nimport os\nfrom launch import LaunchDescription\nfrom launch.actions import DeclareLaunchArgument, SetEnvironmentVariable\nfrom launch.substitutions import LaunchConfiguration\nfrom launch_ros.actions import Node\nfrom nav2_common.launch import RewrittenYaml\n\ndef generate_launch_description():\n    # Launch configuration\n    use_sim_time = LaunchConfiguration('use_sim_time')\n    autostart = LaunchConfiguration('autostart')\n    params_file = LaunchConfiguration('params_file')\n\n    # Create the launch description\n    ld = LaunchDescription()\n\n    # Declare launch arguments\n    ld.add_action(\n        DeclareLaunchArgument(\n            'use_sim_time',\n            default_value='false',\n            description='Use simulation time'))\n\n    ld.add_action(\n        DeclareLaunchArgument(\n            'autostart',\n            default_value='true',\n            description='Automatically start components'))\n\n    ld.add_action(\n        DeclareLaunchArgument(\n            'params_file',\n            default_value=os.path.join(\n                get_package_share_directory('humanoid_nav2_bringup'),\n                'config',\n                'humanoid_nav2_params.yaml'),\n            description='Full path to the ROS2 parameters file to use for all launched nodes'))\n\n    # Create the navigation server\n    navigation_server_node = Node(\n        package='nav2_navigation_server',\n        executable='navigation_server',\n        name='navigation_server',\n        output='screen',\n        parameters=[params_file, {'use_sim_time': use_sim_time}],\n        remappings=[('/tf', 'tf'),\n                   ('/tf_static', 'tf_static')])\n\n    # Lifecycle manager\n    lifecycle_manager = Node(\n        package='nav2_lifecycle_manager',\n        executable='lifecycle_manager',\n        name='lifecycle_manager',\n        output='screen',\n        parameters=[{'use_sim_time': use_sim_time},\n                   {'autostart': autostart},\n                   {'node_names': ['navigation_server',\n                                 'bt_navigator',\n                                 'controller_server',\n                                 'planner_server',\n                                 'recoveries_server',\n                                 'waypoint_follower']}])\n\n    # Add nodes to launch description\n    ld.add_action(navigation_server_node)\n    ld.add_action(lifecycle_manager)\n\n    return ld\n"})}),"\n",(0,o.jsx)(e.h3,{id:"humanoid-specific-parameters",children:"Humanoid-Specific Parameters"}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-yaml",children:'# humanoid_nav2_params.yaml\namcl:\n  ros__parameters:\n    use_sim_time: False\n    alpha1: 0.2\n    alpha2: 0.2\n    alpha3: 0.2\n    alpha4: 0.2\n    alpha5: 0.2\n    base_frame_id: "base_footprint"\n    beam_skip_distance: 0.5\n    beam_skip_error_threshold: 0.9\n    beam_skip_threshold: 0.3\n    do_beamskip: false\n    global_frame_id: "map"\n    lambda_short: 0.1\n    laser_likelihood_max_dist: 2.0\n    laser_max_range: 100.0\n    laser_min_range: -1.0\n    laser_model_type: "likelihood_field"\n    max_beams: 60\n    max_particles: 2000\n    min_particles: 500\n    odom_frame_id: "odom"\n    pf_err: 0.05\n    pf_z: 0.99\n    recovery_alpha_fast: 0.0\n    recovery_alpha_slow: 0.0\n    resample_interval: 1\n    robot_model_type: "nav2_amcl::DifferentialMotionModel"\n    save_pose_rate: 0.5\n    sigma_hit: 0.2\n    tf_broadcast: true\n    transform_tolerance: 1.0\n    update_min_a: 0.2\n    update_min_d: 0.2\n    z_hit: 0.5\n    z_max: 0.05\n    z_rand: 0.5\n    z_short: 0.05\n\nbt_navigator:\n  ros__parameters:\n    use_sim_time: False\n    global_frame: "map"\n    robot_base_frame: "base_footprint"\n    odom_topic: "odom"\n    default_bt_xml_filename: "humanoid_navigator_bt.xml"\n    plugin_lib_names:\n    - nav2_compute_path_to_pose_action_bt_node\n    - nav2_follow_path_action_bt_node\n    - nav2_back_up_action_bt_node\n    - nav2_spin_action_bt_node\n    - nav2_wait_action_bt_node\n    - nav2_clear_costmap_service_bt_node\n    - nav2_is_stuck_condition_bt_node\n    - nav2_goal_reached_condition_bt_node\n    - nav2_goal_updated_condition_bt_node\n    - nav2_initial_pose_received_condition_bt_node\n    - nav2_reinitialize_global_localization_service_bt_node\n    - nav2_rate_controller_bt_node\n    - nav2_distance_controller_bt_node\n    - nav2_speed_controller_bt_node\n    - nav2_truncate_path_action_bt_node\n    - nav2_goal_updater_node_bt_node\n    - nav2_recovery_node_bt_node\n    - nav2_pipeline_sequence_bt_node\n    - nav2_round_robin_node_bt_node\n    - nav2_transform_available_condition_bt_node\n    - nav2_time_expired_condition_bt_node\n    - nav2_path_expiring_timer_condition\n    - nav2_distance_traveled_condition_bt_node\n    - nav2_single_trigger_bt_node\n    - nav2_is_battery_low_condition_bt_node\n    - nav2_navigate_through_poses_action_bt_node\n    - nav2_navigate_to_pose_action_bt_node\n    - nav2_remove_passed_goals_action_bt_node\n    - nav2_planner_selector_bt_node\n    - nav2_controller_selector_bt_node\n    - nav2_goal_checker_selector_bt_node\n\ncontroller_server:\n  ros__parameters:\n    use_sim_time: False\n    controller_frequency: 20.0\n    min_x_velocity_threshold: 0.001\n    min_y_velocity_threshold: 0.5\n    min_theta_velocity_threshold: 0.001\n    progress_checker_plugin: "progress_checker"\n    goal_checker_plugin: "goal_checker"\n    controller_plugins: ["FollowPath"]\n\n    # Humanoid-specific controller\n    FollowPath:\n      plugin: "nav2_mppi_controller::MPPIController"\n      time_steps: 50\n      model_dt: 0.05\n      batch_size: 1000\n      vx_std: 0.2\n      vy_std: 0.2\n      wz_std: 0.3\n      vx_max: 0.5\n      vx_min: -0.2\n      vy_max: 0.3\n      wz_max: 0.3\n      sim_period: 0.05\n      trajectory_visualization: true\n      # Humanoid-specific parameters\n      step_size: 0.3  # Typical humanoid step size\n      max_step_height: 0.1  # Maximum step height\n      balance_threshold: 0.1  # Balance maintenance threshold\n\nlocal_costmap:\n  local_costmap:\n    ros__parameters:\n      update_frequency: 5.0\n      publish_frequency: 2.0\n      global_frame: "odom"\n      robot_base_frame: "base_footprint"\n      use_sim_time: False\n      rolling_window: true\n      width: 6\n      height: 6\n      resolution: 0.05\n      robot_radius: 0.3  # Humanoid radius\n      plugins: ["voxel_layer", "inflation_layer"]\n      inflation_layer:\n        plugin: "nav2_costmap_2d::InflationLayer"\n        cost_scaling_factor: 3.0\n        inflation_radius: 0.55\n      voxel_layer:\n        plugin: "nav2_costmap_2d::VoxelLayer"\n        enabled: True\n        publish_voxel_map: True\n        origin_z: 0.0\n        z_resolution: 0.2\n        z_voxels: 8\n        max_obstacle_height: 2.0\n        mark_threshold: 0\n        observation_sources: scan\n        scan:\n          topic: /scan\n          max_obstacle_height: 2.0\n          clearing: True\n          marking: True\n          data_type: "LaserScan"\n          raytrace_max_range: 3.0\n          raytrace_min_range: 0.0\n          obstacle_max_range: 2.5\n          obstacle_min_range: 0.0\n\nglobal_costmap:\n  global_costmap:\n    ros__parameters:\n      update_frequency: 1.0\n      publish_frequency: 0.5\n      global_frame: "map"\n      robot_base_frame: "base_footprint"\n      use_sim_time: False\n      robot_radius: 0.3\n      resolution: 0.05\n      plugins: ["static_layer", "obstacle_layer", "inflation_layer"]\n      obstacle_layer:\n        plugin: "nav2_costmap_2d::ObstacleLayer"\n        enabled: True\n        observation_sources: scan\n        scan:\n          topic: /scan\n          max_obstacle_height: 2.0\n          clearing: True\n          marking: True\n          data_type: "LaserScan"\n          raytrace_max_range: 3.0\n          raytrace_min_range: 0.0\n          obstacle_max_range: 2.5\n          obstacle_min_range: 0.0\n      static_layer:\n        plugin: "nav2_costmap_2d::StaticLayer"\n        map_subscribe_transient_local: True\n      inflation_layer:\n        plugin: "nav2_costmap_2d::InflationLayer"\n        cost_scaling_factor: 3.0\n        inflation_radius: 0.55\n'})}),"\n",(0,o.jsx)(e.h3,{id:"humanoid-behavior-tree",children:"Humanoid Behavior Tree"}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-xml",children:'\x3c!-- humanoid_navigator_bt.xml --\x3e\n<root main_tree_to_execute="MainTree">\n    <BehaviorTree ID="MainTree">\n        <RecoveryNode number_of_retries="6" name="NavigateRecovery">\n            <PipelineSequence name="NavigateWithReplanning">\n                <RateController hz="1.0">\n                    <ComputePathToPose goal="current_goal" path="path"/>\n                </RateController>\n                <RecoveryNode number_of_retries="1" name="FollowPathRecovery">\n                    <FollowPath path="path" velocity="default_velocity"/>\n                    <ReactiveFallback name="FollowPathWithRecoveryFallback">\n                        <GoalReached goal="current_goal" path="path"/>\n                        <ClearEntireCostmap name="ClearLocalCostmap" service_name="local_costmap/clear_entirely_local_costmap"/>\n                    </ReactiveFallback>\n                </RecoveryNode>\n            </PipelineSequence>\n            <ReactiveFallback name="RecoveryFallback">\n                <GoalUpdated goal="current_goal"/>\n                <ClearEntireCostmap name="ClearGlobalCostmap" service_name="global_costmap/clear_entirely_global_costmap"/>\n            </ReactiveFallback>\n        </RecoveryNode>\n    </BehaviorTree>\n</root>\n'})}),"\n",(0,o.jsx)(e.h2,{id:"bipedal-path-planning-algorithms",children:"Bipedal Path Planning Algorithms"}),"\n",(0,o.jsx)(e.h3,{id:"humanoid-specific-path-planning",children:"Humanoid-Specific Path Planning"}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-python",children:'# humanoid_path_planner.py\nimport numpy as np\nfrom scipy.spatial import KDTree\nimport math\nfrom nav2_msgs.action import ComputePathToPose\nfrom geometry_msgs.msg import PoseStamped, Point\nfrom nav_msgs.msg import Path\nfrom builtin_interfaces.msg import Time\n\nclass HumanoidPathPlanner:\n    def __init__(self):\n        self.step_size = 0.3  # Typical humanoid step size (meters)\n        self.max_step_height = 0.1  # Maximum step height (meters)\n        self.balance_margin = 0.1  # Balance maintenance margin\n        self.support_polygon = self.calculate_support_polygon()\n\n    def calculate_support_polygon(self):\n        """Calculate the support polygon for bipedal balance"""\n        # For a humanoid, the support polygon is between the feet\n        # This is a simplified model - in practice, this would be more complex\n        return [\n            Point(x=-0.1, y=-0.1, z=0.0),  # Left foot\n            Point(x=-0.1, y=0.1, z=0.0),   # Right foot\n        ]\n\n    def plan_bipedal_path(self, start_pose, goal_pose, costmap):\n        """Plan a path suitable for bipedal locomotion"""\n        # Convert to grid coordinates\n        start_grid = self.world_to_grid(start_pose.position.x, start_pose.position.y, costmap)\n        goal_grid = self.world_to_grid(goal_pose.position.x, goal_pose.position.y, costmap)\n\n        # Use A* with humanoid-specific constraints\n        path = self.bipedal_astar(start_grid, goal_grid, costmap)\n\n        # Smooth the path for natural human-like movement\n        smoothed_path = self.smooth_path(path)\n\n        # Convert to ROS Path message\n        ros_path = self.create_ros_path(smoothed_path, costmap)\n\n        return ros_path\n\n    def bipedal_astar(self, start, goal, costmap):\n        """A* algorithm with bipedal constraints"""\n        import heapq\n\n        def heuristic(a, b):\n            return math.sqrt((a[0] - b[0])**2 + (a[1] - b[1])**2)\n\n        # Priority queue: (cost, current, path)\n        frontier = [(0, start, [start])]\n        explored = set()\n\n        while frontier:\n            cost, current, path = heapq.heappop(frontier)\n\n            if current == goal:\n                return path\n\n            if current in explored:\n                continue\n\n            explored.add(current)\n\n            # Get valid neighbors (with bipedal constraints)\n            for neighbor in self.get_valid_neighbors(current, costmap):\n                if neighbor not in explored:\n                    new_cost = cost + 1  # Simple cost model\n                    new_path = path + [neighbor]\n                    priority = new_cost + heuristic(neighbor, goal)\n                    heapq.heappush(frontier, (priority, neighbor, new_path))\n\n        return []  # No path found\n\n    def get_valid_neighbors(self, pos, costmap):\n        """Get valid neighbors considering bipedal constraints"""\n        neighbors = []\n        grid_size = len(costmap.data) // costmap.info.height\n\n        # Consider 8-connectivity for more natural movement\n        for dx in [-1, 0, 1]:\n            for dy in [-1, 0, 1]:\n                if dx == 0 and dy == 0:\n                    continue\n\n                new_x = pos[0] + dx\n                new_y = pos[1] + dy\n\n                # Check bounds\n                if (0 <= new_x < costmap.info.width and\n                    0 <= new_y < costmap.info.height):\n\n                    # Check if cell is free (cost < 50 to avoid unknown areas)\n                    grid_index = new_y * costmap.info.width + new_x\n                    if grid_index < len(costmap.data) and costmap.data[grid_index] < 50:\n                        neighbors.append((new_x, new_y))\n\n        return neighbors\n\n    def smooth_path(self, path):\n        """Smooth the path for natural human-like movement"""\n        if len(path) < 3:\n            return path\n\n        smoothed = [path[0]]\n\n        i = 0\n        while i < len(path) - 1:\n            j = i + 1\n\n            # Try to skip intermediate points while maintaining safety\n            while j < len(path) - 1 and self.is_line_clear(path[i], path[j+1]):\n                j += 1\n\n            smoothed.append(path[j])\n            i = j\n\n        return smoothed\n\n    def is_line_clear(self, start, end):\n        """Check if line between two points is clear of obstacles"""\n        # Simplified implementation - in practice, this would check costmap\n        return True\n\n    def create_ros_path(self, grid_path, costmap):\n        """Convert grid path to ROS Path message"""\n        path_msg = Path()\n        path_msg.header.stamp = Time(sec=0, nanosec=0)  # Will be filled by caller\n        path_msg.header.frame_id = "map"\n\n        for grid_x, grid_y in grid_path:\n            world_x, world_y = self.grid_to_world(grid_x, grid_y, costmap)\n\n            pose = PoseStamped()\n            pose.header.frame_id = "map"\n            pose.pose.position.x = world_x\n            pose.pose.position.y = world_y\n            pose.pose.position.z = 0.0  # Assuming 2D navigation\n\n            # Set orientation to face the next point\n            if grid_path.index((grid_x, grid_y)) < len(grid_path) - 1:\n                next_x, next_y = grid_path[grid_path.index((grid_x, grid_y)) + 1]\n                next_world_x, next_world_y = self.grid_to_world(next_x, next_y, costmap)\n\n                angle = math.atan2(next_world_y - world_y, next_world_x - world_x)\n                pose.pose.orientation.z = math.sin(angle / 2)\n                pose.pose.orientation.w = math.cos(angle / 2)\n\n            path_msg.poses.append(pose)\n\n        return path_msg\n\n    def world_to_grid(self, x, y, costmap):\n        """Convert world coordinates to grid coordinates"""\n        grid_x = int((x - costmap.info.origin.position.x) / costmap.info.resolution)\n        grid_y = int((y - costmap.info.origin.position.y) / costmap.info.resolution)\n        return (grid_x, grid_y)\n\n    def grid_to_world(self, grid_x, grid_y, costmap):\n        """Convert grid coordinates to world coordinates"""\n        world_x = grid_x * costmap.info.resolution + costmap.info.origin.position.x\n        world_y = grid_y * costmap.info.resolution + costmap.info.origin.position.y\n        return (world_x, world_y)\n'})}),"\n",(0,o.jsx)(e.h3,{id:"step-planning-for-humanoid-locomotion",children:"Step Planning for Humanoid Locomotion"}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-python",children:'# step_planner.py\nimport numpy as np\nfrom scipy.spatial import distance\nfrom geometry_msgs.msg import Point\n\nclass StepPlanner:\n    def __init__(self):\n        self.step_length = 0.3  # Typical step length for humanoid\n        self.step_width = 0.2   # Typical step width\n        self.max_step_height = 0.1  # Maximum step height\n        self.balance_margin = 0.05  # Balance margin\n\n    def plan_steps(self, path, robot_pose):\n        """Plan individual steps along the path"""\n        steps = []\n\n        if len(path.poses) < 2:\n            return steps\n\n        current_pose = robot_pose\n        current_foot = "left"  # Start with left foot\n\n        for i in range(len(path.poses) - 1):\n            start_pose = path.poses[i].pose.position\n            end_pose = path.poses[i + 1].pose.position\n\n            # Calculate required step parameters\n            step_vector = np.array([end_pose.x - start_pose.x,\n                                  end_pose.y - start_pose.y])\n            step_distance = np.linalg.norm(step_vector)\n\n            if step_distance > 0:\n                step_direction = step_vector / step_distance\n\n                # Plan steps between start and end\n                num_steps = max(1, int(step_distance / self.step_length))\n\n                for j in range(num_steps):\n                    step_fraction = (j + 1) / num_steps\n                    step_position = start_pose + step_fraction * step_direction\n\n                    # Alternate feet\n                    foot_side = "left" if current_foot == "right" else "right"\n\n                    # Calculate foot placement with slight offset for balance\n                    foot_offset = self.calculate_foot_offset(foot_side, step_direction)\n                    final_position = Point()\n                    final_position.x = step_position[0] + foot_offset[0]\n                    final_position.y = step_position[1] + foot_offset[1]\n                    final_position.z = 0.0  # Ground level\n\n                    step = {\n                        \'position\': final_position,\n                        \'foot\': foot_side,\n                        \'step_number\': len(steps) + 1\n                    }\n\n                    steps.append(step)\n                    current_foot = foot_side\n\n        return steps\n\n    def calculate_foot_offset(self, foot_side, step_direction):\n        """Calculate foot offset for balance"""\n        # Calculate perpendicular vector for foot offset\n        perp_direction = np.array([-step_direction[1], step_direction[0]])\n\n        # Offset based on foot side\n        offset_magnitude = self.step_width / 2\n        if foot_side == "left":\n            offset = perp_direction * offset_magnitude\n        else:  # right foot\n            offset = -perp_direction * offset_magnitude\n\n        return offset\n\n    def validate_step_sequence(self, steps, terrain_map):\n        """Validate step sequence on terrain"""\n        valid_steps = []\n\n        for i, step in enumerate(steps):\n            # Check if step location is valid\n            if self.is_step_valid(step, terrain_map):\n                # Check if transition from previous step is valid\n                if i > 0:\n                    prev_step = steps[i-1]\n                    if self.is_transition_valid(prev_step, step, terrain_map):\n                        valid_steps.append(step)\n                else:\n                    valid_steps.append(step)\n            else:\n                # Try alternative step placement\n                alternative_step = self.find_alternative_step(step, terrain_map)\n                if alternative_step and self.is_step_valid(alternative_step, terrain_map):\n                    valid_steps.append(alternative_step)\n\n        return valid_steps\n\n    def is_step_valid(self, step, terrain_map):\n        """Check if a single step is valid"""\n        # Check if position is on walkable terrain\n        x, y = int(step[\'position\'].x), int(step[\'position\'].y)\n\n        if not (0 <= x < terrain_map.width and 0 <= y < terrain_map.height):\n            return False\n\n        # Check terrain cost (simplified)\n        terrain_cost = terrain_map.get_cost(x, y)\n        if terrain_cost > 50:  # Too costly to step here\n            return False\n\n        # Check step height (simplified)\n        height_diff = abs(step[\'position\'].z - terrain_map.get_height(x, y))\n        if height_diff > self.max_step_height:\n            return False\n\n        return True\n\n    def is_transition_valid(self, prev_step, current_step, terrain_map):\n        """Check if transition between steps is valid"""\n        # Check if step is within reach\n        pos1 = np.array([prev_step[\'position\'].x, prev_step[\'position\'].y])\n        pos2 = np.array([current_step[\'position\'].x, current_step[\'position\'].y])\n\n        distance = np.linalg.norm(pos2 - pos1)\n        if distance > self.step_length * 1.5:  # Allow some flexibility\n            return False\n\n        return True\n\n    def find_alternative_step(self, original_step, terrain_map):\n        """Find alternative step placement if original is invalid"""\n        # Try small adjustments around original position\n        adjustments = [\n            (0.1, 0.0), (-0.1, 0.0), (0.0, 0.1), (0.0, -0.1),  # Direct neighbors\n            (0.1, 0.1), (-0.1, 0.1), (0.1, -0.1), (-0.1, -0.1)  # Diagonal neighbors\n        ]\n\n        for dx, dy in adjustments:\n            alt_step = {\n                \'position\': Point(\n                    x=original_step[\'position\'].x + dx,\n                    y=original_step[\'position\'].y + dy,\n                    z=original_step[\'position\'].z\n                ),\n                \'foot\': original_step[\'foot\'],\n                \'step_number\': original_step[\'step_number\']\n            }\n\n            if self.is_step_valid(alt_step, terrain_map):\n                return alt_step\n\n        return None\n'})}),"\n",(0,o.jsx)(e.h2,{id:"navigation-systems-for-humanoid-robots",children:"Navigation Systems for Humanoid Robots"}),"\n",(0,o.jsx)(e.h3,{id:"humanoid-navigation-server",children:"Humanoid Navigation Server"}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-python",children:'# humanoid_navigation_server.py\nimport rclpy\nfrom rclpy.node import Node\nfrom rclpy.action import ActionServer, CancelResponse, GoalResponse\nfrom rclpy.callback_groups import ReentrantCallbackGroup\nfrom rclpy.executors import MultiThreadedExecutor\nfrom geometry_msgs.msg import PoseStamped, Twist\nfrom nav2_msgs.action import NavigateToPose\nfrom std_msgs.msg import String\nfrom tf2_ros import TransformException\nfrom tf2_ros.buffer import Buffer\nfrom tf2_ros.transform_listener import TransformListener\nimport math\nimport threading\n\nclass HumanoidNavigationServer(Node):\n    def __init__(self):\n        super().__init__(\'humanoid_navigation_server\')\n\n        # Action server\n        self._action_server = ActionServer(\n            self,\n            NavigateToPose,\n            \'navigate_to_pose\',\n            execute_callback=self.execute_navigate_to_pose,\n            callback_group=ReentrantCallbackGroup(),\n            goal_callback=self.goal_callback,\n            cancel_callback=self.cancel_callback)\n\n        # Publishers and subscribers\n        self.cmd_vel_pub = self.create_publisher(Twist, \'cmd_vel\', 10)\n        self.status_pub = self.create_publisher(String, \'navigation_status\', 10)\n\n        # TF listener\n        self.tf_buffer = Buffer()\n        self.tf_listener = TransformListener(self.tf_buffer, self)\n\n        # Path planner and controller\n        self.path_planner = HumanoidPathPlanner()\n        self.step_planner = StepPlanner()\n        self.controller = HumanoidController()\n\n        # Navigation state\n        self.current_goal = None\n        self.navigation_active = False\n        self.navigation_thread = None\n\n        self.get_logger().info(\'Humanoid Navigation Server initialized\')\n\n    def goal_callback(self, goal_request):\n        """Accept or reject navigation goal"""\n        self.get_logger().info(\'Received navigation goal\')\n        return GoalResponse.ACCEPT\n\n    def cancel_callback(self, goal_handle):\n        """Accept or reject navigation cancel request"""\n        self.get_logger().info(\'Received navigation cancel request\')\n        return CancelResponse.ACCEPT\n\n    async def execute_navigate_to_pose(self, goal_handle):\n        """Execute navigation to pose goal"""\n        self.get_logger().info(\'Executing navigation to pose\')\n\n        goal = goal_handle.request.pose\n        feedback = NavigateToPose.Feedback()\n        result = NavigateToPose.Result()\n\n        # Set current goal\n        self.current_goal = goal\n        self.navigation_active = True\n\n        try:\n            # Plan path\n            self.get_logger().info(\'Planning path...\')\n            path = self.path_planner.plan_bipedal_path(\n                self.get_robot_pose(), goal, self.get_costmap())\n\n            if not path.poses:\n                self.get_logger().error(\'Failed to plan path\')\n                result.result = result.FAILURE\n                goal_handle.succeed()\n                return result\n\n            # Plan steps\n            self.get_logger().info(\'Planning steps...\')\n            steps = self.step_planner.plan_steps(path, self.get_robot_pose())\n\n            if not steps:\n                self.get_logger().error(\'Failed to plan steps\')\n                result.result = result.FAILURE\n                goal_handle.succeed()\n                return result\n\n            # Execute navigation\n            self.get_logger().info(\'Executing navigation...\')\n            success = self.controller.execute_navigation_steps(steps)\n\n            if success:\n                result.result = result.SUCCEEDED\n                self.get_logger().info(\'Navigation completed successfully\')\n            else:\n                result.result = result.FAILURE\n                self.get_logger().error(\'Navigation failed\')\n\n        except Exception as e:\n            self.get_logger().error(f\'Navigation error: {e}\')\n            result.result = result.FAILURE\n\n        goal_handle.succeed()\n        self.navigation_active = False\n        return result\n\n    def get_robot_pose(self):\n        """Get current robot pose from TF"""\n        try:\n            transform = self.tf_buffer.lookup_transform(\n                \'map\', \'base_footprint\', rclpy.time.Time(), timeout=rclpy.duration.Duration(seconds=1.0))\n\n            pose = PoseStamped()\n            pose.pose.position.x = transform.transform.translation.x\n            pose.pose.position.y = transform.transform.translation.y\n            pose.pose.position.z = transform.transform.translation.z\n            pose.pose.orientation = transform.transform.rotation\n\n            return pose.pose\n        except TransformException as ex:\n            self.get_logger().error(f\'Could not transform: {ex}\')\n            return None\n\n    def get_costmap(self):\n        """Get current costmap (simplified)"""\n        # In practice, this would get the actual costmap from Nav2\n        from nav_msgs.msg import OccupancyGrid\n        costmap = OccupancyGrid()\n        costmap.header.frame_id = \'map\'\n        costmap.info.resolution = 0.05\n        costmap.info.width = 100\n        costmap.info.height = 100\n        costmap.info.origin.position.x = -2.5\n        costmap.info.origin.position.y = -2.5\n        costmap.data = [0] * (costmap.info.width * costmap.info.height)  # Empty costmap\n        return costmap\n\nclass HumanoidController:\n    def __init__(self):\n        self.balance_controller = BalanceController()\n        self.step_controller = StepController()\n        self.speed = 0.1  # m/s\n        self.step_duration = 1.0  # seconds per step\n\n    def execute_navigation_steps(self, steps):\n        """Execute planned steps for navigation"""\n        for step in steps:\n            if not self.execute_single_step(step):\n                return False  # Navigation failed\n\n            # Check balance after each step\n            if not self.balance_controller.is_balanced():\n                self.get_logger().error(\'Robot lost balance during navigation\')\n                return False\n\n        return True\n\n    def execute_single_step(self, step):\n        """Execute a single step"""\n        try:\n            # Move to step position using appropriate humanoid locomotion\n            success = self.step_controller.move_to_step(step)\n\n            if success:\n                # Wait for step completion\n                import time\n                time.sleep(self.step_duration)\n\n            return success\n        except Exception as e:\n            self.get_logger().error(f\'Step execution failed: {e}\')\n            return False\n\nclass BalanceController:\n    def __init__(self):\n        self.balance_threshold = 0.1  # Balance threshold in meters\n        self.com_height = 0.8  # Center of mass height\n\n    def is_balanced(self):\n        """Check if robot is currently balanced"""\n        # This would interface with actual balance sensors\n        # For simulation, return True\n        return True\n\n    def adjust_balance(self, target_com_position):\n        """Adjust robot balance to target COM position"""\n        # Implementation would use joint control to adjust balance\n        pass\n\nclass StepController:\n    def __init__(self):\n        self.step_height = 0.05  # Step height for lifting foot\n        self.step_duration = 1.0  # Duration for each step\n\n    def move_to_step(self, step):\n        """Move robot foot to specified step position"""\n        # This would control the actual humanoid joints\n        # For simulation, return True\n        return True\n\ndef main(args=None):\n    rclpy.init(args=args)\n\n    node = HumanoidNavigationServer()\n\n    # Use multi-threaded executor to handle callbacks\n    executor = MultiThreadedExecutor(num_threads=4)\n    executor.add_node(node)\n\n    try:\n        executor.spin()\n    except KeyboardInterrupt:\n        pass\n    finally:\n        node.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == \'__main__\':\n    main()\n'})}),"\n",(0,o.jsx)(e.h2,{id:"reinforcement-learning-for-robot-control",children:"Reinforcement Learning for Robot Control"}),"\n",(0,o.jsx)(e.h3,{id:"deep-reinforcement-learning-for-navigation",children:"Deep Reinforcement Learning for Navigation"}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-python",children:'# humanoid_rl_navigation.py\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport numpy as np\nimport random\nfrom collections import deque\nimport gym\nfrom gym import spaces\n\nclass HumanoidNavigationEnv(gym.Env):\n    """Custom environment for humanoid navigation with RL"""\n\n    def __init__(self):\n        super(HumanoidNavigationEnv, self).__init__()\n\n        # Define action and observation space\n        # Actions: [linear_velocity, angular_velocity]\n        self.action_space = spaces.Box(\n            low=np.array([-0.5, -0.5]),  # min linear, min angular\n            high=np.array([0.5, 0.5]),   # max linear, max angular\n            dtype=np.float32\n        )\n\n        # Observations: [robot_x, robot_y, robot_yaw, goal_x, goal_y, obstacle_distances...]\n        self.observation_space = spaces.Box(\n            low=np.array([-np.inf] * 20),  # 20 observation values\n            high=np.array([np.inf] * 20),\n            dtype=np.float32\n        )\n\n        # Initialize state\n        self.reset()\n\n    def reset(self):\n        """Reset the environment to initial state"""\n        # Random start and goal positions\n        self.robot_pos = np.random.uniform(-5, 5, 2)\n        self.goal_pos = np.random.uniform(-5, 5, 2)\n\n        # Random obstacles\n        self.obstacles = np.random.uniform(-6, 6, (5, 2))\n\n        # Robot orientation\n        self.robot_yaw = np.random.uniform(-np.pi, np.pi)\n\n        return self._get_observation()\n\n    def step(self, action):\n        """Execute one step in the environment"""\n        # Apply action to robot\n        linear_vel, angular_vel = action\n\n        # Update robot position (simplified kinematics)\n        dt = 0.1  # Time step\n        self.robot_yaw += angular_vel * dt\n        self.robot_pos[0] += linear_vel * np.cos(self.robot_yaw) * dt\n        self.robot_pos[1] += linear_vel * np.sin(self.robot_yaw) * dt\n\n        # Calculate reward\n        reward = self._calculate_reward()\n\n        # Check if episode is done\n        done = self._is_done()\n\n        # Get next observation\n        observation = self._get_observation()\n\n        # Info dictionary\n        info = {}\n\n        return observation, reward, done, info\n\n    def _get_observation(self):\n        """Get current observation from environment"""\n        # Robot position and orientation\n        obs = [self.robot_pos[0], self.robot_pos[1], self.robot_yaw]\n\n        # Goal position\n        obs.extend([self.goal_pos[0], self.goal_pos[1]])\n\n        # Distance to obstacles (simplified)\n        for obstacle in self.obstacles:\n            dist = np.linalg.norm(self.robot_pos - obstacle)\n            obs.append(min(dist, 10.0))  # Cap distance at 10m\n\n        # Pad to fixed size\n        while len(obs) < 20:\n            obs.append(0.0)\n\n        return np.array(obs[:20], dtype=np.float32)\n\n    def _calculate_reward(self):\n        """Calculate reward for current state"""\n        # Distance to goal\n        dist_to_goal = np.linalg.norm(self.robot_pos - self.goal_pos)\n\n        # Reward based on getting closer to goal\n        reward = -dist_to_goal * 0.1\n\n        # Bonus for reaching goal\n        if dist_to_goal < 0.5:\n            reward += 100\n\n        # Penalty for collisions\n        for obstacle in self.obstacles:\n            dist_to_obstacle = np.linalg.norm(self.robot_pos - obstacle)\n            if dist_to_obstacle < 0.3:  # Collision threshold\n                reward -= 50\n\n        return reward\n\n    def _is_done(self):\n        """Check if episode is done"""\n        dist_to_goal = np.linalg.norm(self.robot_pos - self.goal_pos)\n\n        # Episode done if goal reached or too far from goal\n        return dist_to_goal < 0.5 or dist_to_goal > 20\n\nclass DQN(nn.Module):\n    """Deep Q-Network for humanoid navigation"""\n\n    def __init__(self, state_size, action_size, hidden_size=128):\n        super(DQN, self).__init__()\n\n        self.fc1 = nn.Linear(state_size, hidden_size)\n        self.fc2 = nn.Linear(hidden_size, hidden_size)\n        self.fc3 = nn.Linear(hidden_size, hidden_size)\n        self.fc4 = nn.Linear(hidden_size, action_size)\n\n        self.relu = nn.ReLU()\n        self.dropout = nn.Dropout(0.1)\n\n    def forward(self, x):\n        x = self.relu(self.fc1(x))\n        x = self.dropout(x)\n        x = self.relu(self.fc2(x))\n        x = self.dropout(x)\n        x = self.relu(self.fc3(x))\n        x = self.fc4(x)\n        return x\n\nclass DQNAgent:\n    """DQN Agent for humanoid navigation"""\n\n    def __init__(self, state_size, action_size, lr=1e-3):\n        self.state_size = state_size\n        self.action_size = action_size\n        self.memory = deque(maxlen=10000)\n        self.epsilon = 1.0  # Exploration rate\n        self.epsilon_min = 0.01\n        self.epsilon_decay = 0.995\n        self.learning_rate = lr\n        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")\n\n        # Neural networks\n        self.q_network = DQN(state_size, action_size).to(self.device)\n        self.target_network = DQN(state_size, action_size).to(self.device)\n        self.optimizer = optim.Adam(self.q_network.parameters(), lr=lr)\n\n        # Update target network\n        self.update_target_network()\n\n    def update_target_network(self):\n        """Update target network with current network weights"""\n        self.target_network.load_state_dict(self.q_network.state_dict())\n\n    def remember(self, state, action, reward, next_state, done):\n        """Store experience in replay memory"""\n        self.memory.append((state, action, reward, next_state, done))\n\n    def act(self, state):\n        """Choose action using epsilon-greedy policy"""\n        if np.random.random() <= self.epsilon:\n            return np.random.choice(self.action_size)\n\n        state_tensor = torch.FloatTensor(state).unsqueeze(0).to(self.device)\n        q_values = self.q_network(state_tensor)\n        return np.argmax(q_values.cpu().data.numpy())\n\n    def replay(self, batch_size=32):\n        """Train the model on a batch of experiences"""\n        if len(self.memory) < batch_size:\n            return\n\n        batch = random.sample(self.memory, batch_size)\n        states = torch.FloatTensor([e[0] for e in batch]).to(self.device)\n        actions = torch.LongTensor([e[1] for e in batch]).to(self.device)\n        rewards = torch.FloatTensor([e[2] for e in batch]).to(self.device)\n        next_states = torch.FloatTensor([e[3] for e in batch]).to(self.device)\n        dones = torch.BoolTensor([e[4] for e in batch]).to(self.device)\n\n        current_q_values = self.q_network(states).gather(1, actions.unsqueeze(1))\n        next_q_values = self.target_network(next_states).max(1)[0].detach()\n        target_q_values = rewards + (0.99 * next_q_values * ~dones)\n\n        loss = nn.MSELoss()(current_q_values.squeeze(), target_q_values)\n\n        self.optimizer.zero_grad()\n        loss.backward()\n        self.optimizer.step()\n\n        if self.epsilon > self.epsilon_min:\n            self.epsilon *= self.epsilon_decay\n\ndef train_humanoid_navigation_agent():\n    """Train the humanoid navigation agent"""\n    env = HumanoidNavigationEnv()\n    state_size = env.observation_space.shape[0]\n    action_size = 2  # linear and angular velocity\n\n    agent = DQNAgent(state_size, action_size)\n\n    episodes = 1000\n    scores = deque(maxlen=100)\n\n    for e in range(episodes):\n        state = env.reset()\n        total_reward = 0\n\n        for time_step in range(500):  # Max steps per episode\n            action = agent.act(state)\n\n            # Convert discrete action to continuous (simplified)\n            continuous_action = np.array([action/10, action/20])  # Map to meaningful velocities\n\n            next_state, reward, done, _ = env.step(continuous_action)\n\n            agent.remember(state, action, reward, next_state, done)\n            state = next_state\n            total_reward += reward\n\n            if done:\n                break\n\n        scores.append(total_reward)\n\n        # Train the agent\n        if len(agent.memory) > 32:\n            agent.replay(32)\n\n        # Update target network periodically\n        if e % 100 == 0:\n            agent.update_target_network()\n\n        print(f"Episode {e}, Score: {total_reward:.2f}, Average Score: {np.mean(scores):.2f}, Epsilon: {agent.epsilon:.2f}")\n\n    return agent\n\n# Example usage\nif __name__ == "__main__":\n    trained_agent = train_humanoid_navigation_agent()\n'})}),"\n",(0,o.jsx)(e.h3,{id:"humanoid-locomotion-control-with-rl",children:"Humanoid Locomotion Control with RL"}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-python",children:'# humanoid_locomotion_rl.py\nimport torch\nimport torch.nn as nn\nimport numpy as np\nimport math\n\nclass HumanoidLocomotionEnv:\n    """Environment for humanoid locomotion learning"""\n\n    def __init__(self):\n        # Robot parameters\n        self.torso_height = 0.8\n        self.step_length = 0.3\n        self.max_joint_angles = np.array([0.5, 1.0, 0.5])  # Hip, knee, ankle limits\n        self.balance_threshold = 0.1  # Balance maintenance threshold\n\n        # State initialization\n        self.reset()\n\n    def reset(self):\n        """Reset to initial standing position"""\n        self.torso_pos = np.array([0.0, 0.0, self.torso_height])\n        self.torso_vel = np.array([0.0, 0.0, 0.0])\n        self.torso_orientation = np.array([0.0, 0.0, 0.0, 1.0])  # Quaternion\n        self.left_foot_pos = np.array([-0.1, 0.1, 0.0])\n        self.right_foot_pos = np.array([-0.1, -0.1, 0.0])\n        self.time = 0.0\n\n        return self.get_state()\n\n    def get_state(self):\n        """Get current state representation"""\n        state = np.concatenate([\n            self.torso_pos,           # 3: torso position\n            self.torso_vel,           # 3: torso velocity\n            self.torso_orientation,   # 4: torso orientation\n            self.left_foot_pos[:2],   # 2: left foot x,y (z is 0)\n            self.right_foot_pos[:2],  # 2: right foot x,y (z is 0)\n            [self.time]               # 1: time\n        ])\n        return state\n\n    def step(self, action):\n        """Execute action and return next state, reward, done"""\n        # Action contains desired joint angles or torques\n        # Simplified physics simulation\n        self.apply_action(action)\n\n        # Update physics\n        self.update_physics()\n\n        # Calculate reward\n        reward = self.calculate_reward()\n\n        # Check termination\n        done = self.is_terminal()\n\n        self.time += 0.05  # 20Hz simulation\n\n        return self.get_state(), reward, done, {}\n\n    def apply_action(self, action):\n        """Apply control action to robot"""\n        # In a real implementation, this would set joint torques or positions\n        # For simulation, we\'ll just use the action to influence movement\n        pass\n\n    def update_physics(self):\n        """Update robot physics"""\n        # Simplified physics update\n        # In reality, this would involve complex dynamics simulation\n        pass\n\n    def calculate_reward(self):\n        """Calculate reward based on current state"""\n        reward = 0.0\n\n        # Reward forward movement\n        if self.torso_pos[0] > 0:  # Moving forward\n            reward += self.torso_pos[0] * 0.1\n\n        # Penalty for falling\n        if abs(self.torso_pos[2] - self.torso_height) > 0.2:  # Fallen\n            reward -= 100\n\n        # Reward balance maintenance\n        if abs(self.torso_pos[1]) < 0.1:  # Staying centered laterally\n            reward += 0.1\n\n        # Penalty for excessive joint angles\n        # This would check actual joint angles in a real implementation\n\n        return reward\n\n    def is_terminal(self):\n        """Check if episode should terminate"""\n        # Fallen or moved too far\n        return (abs(self.torso_pos[2] - self.torso_height) > 0.3 or\n                abs(self.torso_pos[0]) > 10 or\n                self.time > 100)  # Max time\n\nclass PPOAgent:\n    """Proximal Policy Optimization agent for humanoid locomotion"""\n\n    def __init__(self, state_dim, action_dim, lr=3e-4):\n        self.actor = self.build_actor(state_dim, action_dim)\n        self.critic = self.build_critic(state_dim)\n\n        self.actor_optimizer = torch.optim.Adam(self.actor.parameters(), lr=lr)\n        self.critic_optimizer = torch.optim.Adam(self.critic.parameters(), lr=lr)\n\n        self.clip_epsilon = 0.2\n        self.gamma = 0.99\n        self.lam = 0.95\n\n    def build_actor(self, state_dim, action_dim):\n        """Build actor network (policy)"""\n        return nn.Sequential(\n            nn.Linear(state_dim, 256),\n            nn.ReLU(),\n            nn.Linear(256, 256),\n            nn.ReLU(),\n            nn.Linear(256, action_dim),\n            nn.Tanh()  # Actions between -1 and 1\n        )\n\n    def build_critic(self, state_dim):\n        """Build critic network (value function)"""\n        return nn.Sequential(\n            nn.Linear(state_dim, 256),\n            nn.ReLU(),\n            nn.Linear(256, 256),\n            nn.ReLU(),\n            nn.Linear(256, 1)\n        )\n\n    def select_action(self, state):\n        """Select action using current policy"""\n        state_tensor = torch.FloatTensor(state).unsqueeze(0)\n\n        with torch.no_grad():\n            action_mean = self.actor(state_tensor)\n            action = torch.normal(action_mean, 0.1)  # Add noise for exploration\n            action = torch.clamp(action, -1, 1)\n\n        return action.squeeze().numpy()\n\n    def compute_returns(self, rewards, values, dones):\n        """Compute discounted returns for training"""\n        returns = []\n        gae = 0\n\n        for i in reversed(range(len(rewards))):\n            if i == len(rewards) - 1:\n                next_value = 0 if dones[i] else values[i].item()\n            else:\n                next_value = values[i + 1].item()\n\n            delta = rewards[i] + self.gamma * next_value * (1 - dones[i]) - values[i].item()\n            gae = delta + self.gamma * self.lam * (1 - dones[i]) * gae\n            returns.insert(0, gae + values[i].item())\n\n        return torch.FloatTensor(returns)\n\ndef train_locomotion_agent():\n    """Train humanoid locomotion agent"""\n    env = HumanoidLocomotionEnv()\n    agent = PPOAgent(state_dim=env.get_state().shape[0], action_dim=12)  # 12 joint controls\n\n    episodes = 1000\n\n    for episode in range(episodes):\n        state = env.reset()\n        episode_reward = 0\n        steps = 0\n\n        while steps < 1000:  # Max steps per episode\n            action = agent.select_action(state)\n            next_state, reward, done, _ = env.step(action)\n\n            episode_reward += reward\n            steps += 1\n\n            if done:\n                break\n\n            state = next_state\n\n        print(f"Episode {episode}, Reward: {episode_reward:.2f}, Steps: {steps}")\n'})}),"\n",(0,o.jsx)(e.h2,{id:"knowledge-check",children:"Knowledge Check"}),"\n",(0,o.jsxs)(e.ol,{children:["\n",(0,o.jsx)(e.li,{children:"What are the key differences between Nav2 for wheeled robots and humanoid robots?"}),"\n",(0,o.jsx)(e.li,{children:"How do you modify path planning algorithms for bipedal locomotion constraints?"}),"\n",(0,o.jsx)(e.li,{children:"What are the essential components of a humanoid navigation system?"}),"\n",(0,o.jsx)(e.li,{children:"How can reinforcement learning be applied to humanoid robot control?"}),"\n"]}),"\n",(0,o.jsx)(e.h2,{id:"summary",children:"Summary"}),"\n",(0,o.jsx)(e.p,{children:"This chapter covered Nav2 and path planning specifically for humanoid robots, addressing the unique challenges of bipedal locomotion. We explored humanoid-specific Nav2 configuration, step planning algorithms, navigation systems design, and reinforcement learning approaches for robot control. The chapter provided practical implementations for creating navigation systems that account for the balance, kinematic, and dynamic constraints of humanoid robots."}),"\n",(0,o.jsx)(e.h2,{id:"next-steps",children:"Next Steps"}),"\n",(0,o.jsx)(e.p,{children:"In the next chapter, we'll examine sim-to-real transfer techniques, exploring the principles of transferring learned behaviors from simulation to real-world humanoid robots, including domain randomization and best practices for successful deployment."})]})}function p(n={}){const{wrapper:e}={...(0,a.R)(),...n.components};return e?(0,o.jsx)(e,{...n,children:(0,o.jsx)(_,{...n})}):_(n)}},8453(n,e,t){t.d(e,{R:()=>r,x:()=>s});var o=t(6540);const a={},i=o.createContext(a);function r(n){const e=o.useContext(i);return o.useMemo(function(){return"function"==typeof n?n(e):{...e,...n}},[e,n])}function s(n){let e;return e=n.disableParentContext?"function"==typeof n.components?n.components(a):n.components||a:r(n.components),o.createElement(i.Provider,{value:e},n.children)}}}]);