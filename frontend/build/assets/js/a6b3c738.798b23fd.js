"use strict";(globalThis.webpackChunkphysical_ai_humanoid_robotics_book=globalThis.webpackChunkphysical_ai_humanoid_robotics_book||[]).push([[660],{5445(e,n,t){t.r(n),t.d(n,{assets:()=>l,contentTitle:()=>r,default:()=>u,frontMatter:()=>o,metadata:()=>i,toc:()=>c});var s=t(4848),a=t(8453);const o={sidebar_position:17,title:"Chapter 17: Integrating LLMs for Conversational AI in Robots"},r="Chapter 17: Integrating LLMs for Conversational AI in Robots",i={id:"part6/chapter17",title:"Chapter 17: Integrating LLMs for Conversational AI in Robots",description:"Learning Objectives",source:"@site/docs/part6/chapter17.md",sourceDirName:"part6",slug:"/part6/chapter17",permalink:"/Physical-AI-Humanoid-Robotics/docs/part6/chapter17",draft:!1,unlisted:!1,editUrl:"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/part6/chapter17.md",tags:[],version:"current",sidebarPosition:17,frontMatter:{sidebar_position:17,title:"Chapter 17: Integrating LLMs for Conversational AI in Robots"},sidebar:"tutorialSidebar",previous:{title:"Chapter 16: Natural Human-Robot Interaction",permalink:"/Physical-AI-Humanoid-Robotics/docs/part5/chapter16"},next:{title:"Chapter 18: Speech Recognition and Natural Language Understanding",permalink:"/Physical-AI-Humanoid-Robotics/docs/part6/chapter18"}},l={},c=[{value:"Learning Objectives",id:"learning-objectives",level:2},{value:"Introduction to LLM Integration in Robotics",id:"introduction-to-llm-integration-in-robotics",level:2},{value:"Benefits of LLM Integration",id:"benefits-of-llm-integration",level:3},{value:"Challenges in LLM-Robot Integration",id:"challenges-in-llm-robot-integration",level:3},{value:"Conversational AI Principles",id:"conversational-ai-principles",level:2},{value:"Architecture for LLM-Robot Integration",id:"architecture-for-llm-robot-integration",level:3},{value:"Architecture for LLM-Robot Interfaces",id:"architecture-for-llm-robot-interfaces",level:2},{value:"ROS 2 Integration Architecture",id:"ros-2-integration-architecture",level:3},{value:"Multimodal Integration",id:"multimodal-integration",level:3},{value:"Cognitive Planning with LLMs",id:"cognitive-planning-with-llms",level:2},{value:"Planning and Execution Framework",id:"planning-and-execution-framework",level:3},{value:"Knowledge Check",id:"knowledge-check",level:2},{value:"Summary",id:"summary",level:2},{value:"Next Steps",id:"next-steps",level:2}];function p(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,a.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.h1,{id:"chapter-17-integrating-llms-for-conversational-ai-in-robots",children:"Chapter 17: Integrating LLMs for Conversational AI in Robots"}),"\n",(0,s.jsx)(n.h2,{id:"learning-objectives",children:"Learning Objectives"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Understand the integration of Large Language Models in robotics"}),"\n",(0,s.jsx)(n.li,{children:"Implement conversational AI principles for robot interaction"}),"\n",(0,s.jsx)(n.li,{children:"Design architecture for LLM-robot interfaces"}),"\n",(0,s.jsx)(n.li,{children:"Create ROS 2 action servers for LLM command processing"}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"introduction-to-llm-integration-in-robotics",children:"Introduction to LLM Integration in Robotics"}),"\n",(0,s.jsx)(n.p,{children:"Large Language Models (LLMs) have revolutionized the field of natural language processing and are increasingly being integrated into robotics systems to enable more natural and sophisticated human-robot interaction. For humanoid robots, LLMs provide the capability to understand complex natural language commands, engage in meaningful conversations, and translate high-level human instructions into executable robotic actions."}),"\n",(0,s.jsx)(n.h3,{id:"benefits-of-llm-integration",children:"Benefits of LLM Integration"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Natural Language Understanding"}),": Process complex, nuanced human commands"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Contextual Awareness"}),": Maintain conversation context and history"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Flexible Interaction"}),": Handle diverse types of requests and queries"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Learning Capability"}),": Improve over time through interaction"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Multimodal Integration"}),": Combine language with other sensory inputs"]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"challenges-in-llm-robot-integration",children:"Challenges in LLM-Robot Integration"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Latency"}),": LLM inference can be slow, affecting real-time interaction"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Safety"}),": Ensuring LLM responses result in safe robot behaviors"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Grounding"}),": Connecting language to physical world and robot capabilities"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Reliability"}),": Handling cases where LLM provides incorrect information"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Privacy"}),": Managing sensitive information in conversational data"]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"conversational-ai-principles",children:"Conversational AI Principles"}),"\n",(0,s.jsx)(n.h3,{id:"architecture-for-llm-robot-integration",children:"Architecture for LLM-Robot Integration"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'# llm_robot_integration.py\nimport asyncio\nimport json\nimport logging\nfrom typing import Dict, List, Any, Optional, Callable\nfrom dataclasses import dataclass\nfrom enum import Enum\nimport openai\nimport time\n\nclass InteractionMode(Enum):\n    COMMAND = "command"\n    CONVERSATION = "conversation"\n    INFORMATION = "information"\n    TASK = "task"\n\nclass SafetyLevel(Enum):\n    STRICT = "strict"\n    MODERATE = "moderate"\n    RELAXED = "relaxed"\n\n@dataclass\nclass UserIntent:\n    """Represents the user\'s intent extracted from their input"""\n    action: str\n    parameters: Dict[str, Any]\n    confidence: float\n    mode: InteractionMode\n\n@dataclass\nclass RobotResponse:\n    """Represents the robot\'s response to user input"""\n    text: str\n    action: Optional[str] = None\n    parameters: Optional[Dict[str, Any]] = None\n    safety_check_passed: bool = True\n    execution_required: bool = False\n\nclass SafetyChecker:\n    """Checks if LLM responses are safe for robot execution"""\n    def __init__(self, safety_level: SafetyLevel = SafetyLevel.MODERATE):\n        self.safety_level = safety_level\n        self.banned_actions = [\n            \'self_harm\', \'damage_robot\', \'unsafe_movement\', \'inappropriate_behavior\'\n        ]\n\n    def check_response_safety(self, response: RobotResponse) -> bool:\n        """Check if a response is safe to execute"""\n        if not response.action:\n            return True  # No action to check\n\n        # Check for banned actions\n        if response.action.lower() in self.banned_actions:\n            return False\n\n        # Check for unsafe movements based on safety level\n        if self.safety_level == SafetyLevel.STRICT:\n            if response.action.lower() in [\'move\', \'step\', \'walk\']:\n                # Verify movement parameters are safe\n                if response.parameters:\n                    speed = response.parameters.get(\'speed\', 1.0)\n                    if speed > 2.0:  # Too fast\n                        return False\n\n        return True\n\n    def check_intent_safety(self, intent: UserIntent) -> bool:\n        """Check if an intent is safe to process"""\n        # Check for potentially harmful commands\n        dangerous_keywords = [\'harm\', \'damage\', \'break\', \'destroy\', \'hurt\']\n\n        for param_value in intent.parameters.values():\n            if isinstance(param_value, str):\n                if any(keyword in param_value.lower() for keyword in dangerous_keywords):\n                    return False\n\n        return True\n\nclass ContextManager:\n    """Manages conversation context and state"""\n    def __init__(self, max_history: int = 10):\n        self.conversation_history = []\n        self.current_context = {}\n        self.max_history = max_history\n        self.user_profiles = {}\n\n    def add_interaction(self, user_input: str, robot_response: str, user_id: str = "default"):\n        """Add an interaction to the conversation history"""\n        interaction = {\n            \'timestamp\': time.time(),\n            \'user_input\': user_input,\n            \'robot_response\': robot_response,\n            \'user_id\': user_id\n        }\n\n        self.conversation_history.append(interaction)\n\n        # Keep history size manageable\n        if len(self.conversation_history) > self.max_history:\n            self.conversation_history = self.conversation_history[-self.max_history:]\n\n    def get_context_for_prompt(self) -> str:\n        """Get context to include in LLM prompts"""\n        if not self.conversation_history:\n            return "This is the beginning of the conversation."\n\n        context_parts = []\n        recent_interactions = self.conversation_history[-3:]  # Last 3 interactions\n\n        for interaction in recent_interactions:\n            timestamp = time.strftime(\'%H:%M:%S\', time.localtime(interaction[\'timestamp\']))\n            context_parts.append(f"[{timestamp}] User: {interaction[\'user_input\']}")\n            context_parts.append(f"[{timestamp}] Robot: {interaction[\'robot_response\']}")\n\n        return "\\n".join(context_parts)\n\n    def update_user_profile(self, user_id: str, attributes: Dict[str, Any]):\n        """Update user profile with new attributes"""\n        if user_id not in self.user_profiles:\n            self.user_profiles[user_id] = {}\n\n        self.user_profiles[user_id].update(attributes)\n\n    def get_user_profile(self, user_id: str) -> Dict[str, Any]:\n        """Get user profile information"""\n        return self.user_profiles.get(user_id, {})\n\nclass LLMPromptBuilder:\n    """Builds prompts for LLM interaction"""\n    def __init__(self):\n        self.system_prompt = self._get_system_prompt()\n        self.robot_capabilities = self._get_robot_capabilities()\n\n    def _get_system_prompt(self) -> str:\n        """Get the system prompt for the LLM"""\n        return """\nYou are an AI assistant integrated into a humanoid robot. Your role is to:\n1. Understand user commands and questions\n2. Extract actionable intents from natural language\n3. Provide helpful, safe responses\n4. Recognize when to ask for clarification\n5. Only suggest actions that are safe and within the robot\'s capabilities\n\nRobot capabilities include: navigation, object manipulation, conversation, information retrieval, and basic assistance tasks.\n\nAlways prioritize safety and ask for clarification if a request is ambiguous.\n"""\n\n    def _get_robot_capabilities(self) -> List[str]:\n        """Define robot capabilities for grounding"""\n        return [\n            "navigation",\n            "object_grasping",\n            "object_manipulation",\n            "conversation",\n            "information_retrieval",\n            "environmental_sensing",\n            "basic_assistance",\n            "social_interaction"\n        ]\n\n    def build_intent_extraction_prompt(self, user_input: str, context: str = "") -> str:\n        """Build prompt for intent extraction"""\n        prompt = f"""\n{self.system_prompt}\n\nPrevious conversation context:\n{context}\n\nUser input: "{user_input}"\n\nExtract the user\'s intent from their input. Provide your response in JSON format with the following structure:\n{{\n    "action": "the main action the user wants (e.g., \'navigate\', \'grasp\', \'answer_question\')",\n    "parameters": {{"param_name": "param_value", ...}},\n    "confidence": "confidence level (0.0 to 1.0)",\n    "mode": "command, conversation, information, or task"\n}}\n\nExample responses:\n{{\n    "action": "navigate",\n    "parameters": {{"destination": "kitchen", "speed": "normal"}},\n    "confidence": 0.9,\n    "mode": "command"\n}}\n\n{{\n    "action": "answer_question",\n    "parameters": {{"question": "What time is it?", "topic": "time"}},\n    "confidence": 0.8,\n    "mode": "information"\n}}\n\nNow analyze the user input:\n"""\n\n        return prompt\n\n    def build_response_generation_prompt(self, user_input: str, intent: UserIntent, context: str = "") -> str:\n        """Build prompt for generating robot response"""\n        prompt = f"""\n{self.system_prompt}\n\nPrevious conversation context:\n{context}\n\nUser input: "{user_input}"\n\nRecognized intent: {json.dumps(intent.__dict__)}\n\nGenerate an appropriate response for the user. Consider:\n1. The user\'s intent and parameters\n2. The conversation context\n3. The robot\'s capabilities\n4. Safety considerations\n5. Natural, conversational tone\n\nIf the intent requires robot action, include both the response text and the action to take.\n\nProvide your response in JSON format:\n{{\n    "response_text": "the text response to the user",\n    "action": "the action for the robot to take (optional)",\n    "parameters": {{"param_name": "param_value", ...}},\n    "execution_required": true/false\n}}\n\nExample:\n{{\n    "response_text": "I\'ll navigate to the kitchen for you. Is that correct?",\n    "action": "navigate",\n    "parameters": {{"destination": "kitchen"}},\n    "execution_required": true\n}}\n"""\n\n        return prompt\n\nclass LLMRobotInterface:\n    """Main interface for LLM-robot integration"""\n    def __init__(self, api_key: str, model: str = "gpt-3.5-turbo"):\n        self.api_key = api_key\n        self.model = model\n        self.context_manager = ContextManager()\n        self.safety_checker = SafetyChecker()\n        self.prompt_builder = LLMPromptBuilder()\n        self.logger = logging.getLogger(__name__)\n\n        # Initialize OpenAI client\n        openai.api_key = api_key\n\n        # Robot action handlers\n        self.action_handlers = {\n            \'navigate\': self.handle_navigate,\n            \'grasp\': self.handle_grasp,\n            \'answer_question\': self.handle_answer_question,\n            \'conversation\': self.handle_conversation,\n            \'task\': self.handle_task\n        }\n\n    async def process_user_input(self, user_input: str, user_id: str = "default") -> RobotResponse:\n        """Process user input through LLM and return robot response"""\n        try:\n            # Get conversation context\n            context = self.context_manager.get_context_for_prompt()\n\n            # Extract intent using LLM\n            intent = await self.extract_intent(user_input, context)\n\n            # Check intent safety\n            if not self.safety_checker.check_intent_safety(intent):\n                return RobotResponse(\n                    text="I can\'t process that request for safety reasons.",\n                    safety_check_passed=False\n                )\n\n            # Generate response using LLM\n            response = await self.generate_response(user_input, intent, context)\n\n            # Check response safety\n            if not self.safety_checker.check_response_safety(response):\n                response.safety_check_passed = False\n                response.text = "I can\'t perform that action for safety reasons."\n\n            # Add to conversation history\n            self.context_manager.add_interaction(user_input, response.text, user_id)\n\n            return response\n\n        except Exception as e:\n            self.logger.error(f"Error processing user input: {e}")\n            return RobotResponse(\n                text="Sorry, I encountered an error processing your request.",\n                safety_check_passed=False\n            )\n\n    async def extract_intent(self, user_input: str, context: str) -> UserIntent:\n        """Extract user intent using LLM"""\n        prompt = self.prompt_builder.build_intent_extraction_prompt(user_input, context)\n\n        try:\n            response = await openai.ChatCompletion.acreate(\n                model=self.model,\n                messages=[{"role": "user", "content": prompt}],\n                temperature=0.3,\n                max_tokens=200\n            )\n\n            response_text = response.choices[0].message.content.strip()\n\n            # Extract JSON from response\n            json_start = response_text.find(\'{\')\n            json_end = response_text.rfind(\'}\') + 1\n            json_str = response_text[json_start:json_end]\n\n            intent_data = json.loads(json_str)\n\n            return UserIntent(\n                action=intent_data.get(\'action\', \'unknown\'),\n                parameters=intent_data.get(\'parameters\', {}),\n                confidence=intent_data.get(\'confidence\', 0.5),\n                mode=InteractionMode(intent_data.get(\'mode\', \'conversation\'))\n            )\n\n        except json.JSONDecodeError:\n            self.logger.error("Failed to parse LLM response as JSON")\n            return UserIntent(action=\'unknown\', parameters={}, confidence=0.0, mode=InteractionMode.CONVERSATION)\n        except Exception as e:\n            self.logger.error(f"Error extracting intent: {e}")\n            return UserIntent(action=\'unknown\', parameters={}, confidence=0.0, mode=InteractionMode.CONVERSATION)\n\n    async def generate_response(self, user_input: str, intent: UserIntent, context: str) -> RobotResponse:\n        """Generate robot response using LLM"""\n        prompt = self.prompt_builder.build_response_generation_prompt(user_input, intent, context)\n\n        try:\n            response = await openai.ChatCompletion.acreate(\n                model=self.model,\n                messages=[{"role": "user", "content": prompt}],\n                temperature=0.7,\n                max_tokens=300\n            )\n\n            response_text = response.choices[0].message.content.strip()\n\n            # Extract JSON response\n            json_start = response_text.find(\'{\')\n            json_end = response_text.rfind(\'}\') + 1\n            json_str = response_text[json_start:json_end]\n\n            response_data = json.loads(json_str)\n\n            return RobotResponse(\n                text=response_data.get(\'response_text\', \'\'),\n                action=response_data.get(\'action\'),\n                parameters=response_data.get(\'parameters\', {}),\n                execution_required=response_data.get(\'execution_required\', False)\n            )\n\n        except json.JSONDecodeError:\n            self.logger.error("Failed to parse LLM response as JSON")\n            return RobotResponse(text="I understand.", execution_required=False)\n        except Exception as e:\n            self.logger.error(f"Error generating response: {e}")\n            return RobotResponse(text="I\'m not sure how to respond to that.", execution_required=False)\n\n    # Action handlers\n    async def handle_navigate(self, parameters: Dict[str, Any]) -> str:\n        """Handle navigation commands"""\n        destination = parameters.get(\'destination\', \'unknown location\')\n        speed = parameters.get(\'speed\', \'normal\')\n\n        # In practice, this would interface with navigation system\n        return f"Okay, I\'m navigating to the {destination} at {speed} speed."\n\n    async def handle_grasp(self, parameters: Dict[str, Any]) -> str:\n        """Handle object grasping commands"""\n        object_name = parameters.get(\'object\', \'unknown object\')\n\n        # In practice, this would interface with manipulation system\n        return f"Okay, I\'ll grasp the {object_name} for you."\n\n    async def handle_answer_question(self, parameters: Dict[str, Any]) -> str:\n        """Handle information requests"""\n        question = parameters.get(\'question\', \'\')\n\n        # In practice, this might interface with knowledge base or internet\n        return f"I\'d be happy to help with information about \'{question}\'."\n\n    async def handle_conversation(self, parameters: Dict[str, Any]) -> str:\n        """Handle conversational responses"""\n        topic = parameters.get(\'topic\', \'general\')\n        return f"That\'s interesting! Tell me more about {topic}."\n\n    async def handle_task(self, parameters: Dict[str, Any]) -> str:\n        """Handle complex task commands"""\n        task_description = parameters.get(\'description\', \'unknown task\')\n        return f"I\'ll help you with the {task_description} task."\n\n    async def execute_action(self, action: str, parameters: Dict[str, Any]) -> bool:\n        """Execute a robot action"""\n        if action in self.action_handlers:\n            try:\n                result = await self.action_handlers[action](parameters)\n                self.logger.info(f"Executed action \'{action}\' with parameters {parameters}")\n                return True\n            except Exception as e:\n                self.logger.error(f"Error executing action \'{action}\': {e}")\n                return False\n        else:\n            self.logger.warning(f"Unknown action: {action}")\n            return False\n\n# Example usage\nasync def example_llm_robot_integration():\n    """Example of LLM-robot integration"""\n    # This would require an actual OpenAI API key\n    # api_key = "your-openai-api-key"\n    # llm_interface = LLMRobotInterface(api_key)\n\n    print("LLM-Robot Integration Example")\n    print("Note: This example requires a valid OpenAI API key to run")\n\n    # Simulate the interface for demonstration\n    class MockLLMInterface:\n        async def process_user_input(self, user_input, user_id="default"):\n            # Mock response for demonstration\n            responses = {\n                "hello": RobotResponse(text="Hello! How can I help you today?", execution_required=False),\n                "navigate to kitchen": RobotResponse(\n                    text="Okay, I\'ll navigate to the kitchen for you.",\n                    action="navigate",\n                    parameters={"destination": "kitchen"},\n                    execution_required=True\n                ),\n                "grasp the cup": RobotResponse(\n                    text="I\'ll grasp the cup for you.",\n                    action="grasp",\n                    parameters={"object": "cup"},\n                    execution_required=True\n                )\n            }\n\n            key = user_input.lower()\n            return responses.get(key, RobotResponse(text="I\'m not sure how to help with that.", execution_required=False))\n\n    llm_interface = MockLLMInterface()\n\n    # Test inputs\n    test_inputs = [\n        "Hello robot",\n        "Navigate to kitchen",\n        "Grasp the cup",\n        "What\'s the weather like?"\n    ]\n\n    for user_input in test_inputs:\n        print(f"\\nUser: {user_input}")\n\n        response = await llm_interface.process_user_input(user_input)\n        print(f"Robot: {response.text}")\n\n        if response.execution_required and response.action:\n            print(f"Action to execute: {response.action} with parameters {response.parameters}")\n\nif __name__ == "__main__":\n    # Run the example\n    asyncio.run(example_llm_robot_integration())\n'})}),"\n",(0,s.jsx)(n.h2,{id:"architecture-for-llm-robot-interfaces",children:"Architecture for LLM-Robot Interfaces"}),"\n",(0,s.jsx)(n.h3,{id:"ros-2-integration-architecture",children:"ROS 2 Integration Architecture"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'# ros2_llm_integration.py\nimport rclpy\nfrom rclpy.node import Node\nfrom rclpy.action import ActionServer, GoalResponse, CancelResponse\nfrom rclpy.callback_groups import ReentrantCallbackGroup\nfrom rclpy.executors import MultiThreadedExecutor\nfrom std_msgs.msg import String\nfrom geometry_msgs.msg import PoseStamped\nfrom sensor_msgs.msg import Image\nimport asyncio\nimport threading\nfrom concurrent.futures import ThreadPoolExecutor\nimport json\n\n# Define custom action message for LLM commands\n# This would typically be in a custom action file\nclass LLMCommand:\n    class Goal:\n        def __init__(self):\n            self.command = ""\n            self.parameters = ""\n\n    class Result:\n        def __init__(self):\n            self.response = ""\n            self.action_taken = ""\n            self.success = False\n\n    class Feedback:\n        def __init__(self):\n            self.status = ""\n\nclass LLMRobotNode(Node):\n    def __init__(self):\n        super().__init__(\'llm_robot_node\')\n\n        # Initialize LLM interface\n        self.llm_interface = None  # Will be initialized after API key is available\n\n        # Publishers and subscribers\n        self.speech_pub = self.create_publisher(String, \'speech_output\', 10)\n        self.navigation_pub = self.create_publisher(PoseStamped, \'move_base_simple/goal\', 10)\n        self.vision_sub = self.create_subscription(Image, \'camera/image_raw\', self.vision_callback, 10)\n\n        # Action server for LLM commands\n        self.llm_action_server = ActionServer(\n            self,\n            LLMCommand,\n            \'llm_command\',\n            execute_callback=self.execute_llm_command,\n            callback_group=ReentrantCallbackGroup(),\n            goal_callback=self.goal_callback,\n            cancel_callback=self.cancel_callback\n        )\n\n        # Service clients for robot capabilities\n        self.navigation_client = None  # Will be initialized as needed\n        self.manipulation_client = None  # Will be initialized as needed\n\n        # Threading for async LLM operations\n        self.llm_executor = ThreadPoolExecutor(max_workers=2)\n        self.loop = asyncio.new_event_loop()\n        self.loop_thread = threading.Thread(target=self.run_async_loop, args=(self.loop,))\n        self.loop_thread.start()\n\n        self.get_logger().info(\'LLM Robot Node initialized\')\n\n    def run_async_loop(self, loop):\n        """Run the asyncio event loop in a separate thread"""\n        asyncio.set_event_loop(loop)\n        loop.run_forever()\n\n    def goal_callback(self, goal_request):\n        """Accept or reject LLM command goals"""\n        self.get_logger().info(f\'Received LLM command goal: {goal_request.command}\')\n        return GoalResponse.ACCEPT\n\n    def cancel_callback(self, goal_handle):\n        """Accept or reject LLM command cancel requests"""\n        self.get_logger().info(\'Received LLM command cancel request\')\n        return CancelResponse.ACCEPT\n\n    def vision_callback(self, msg):\n        """Handle vision data for multimodal interaction"""\n        # Process vision data for context\n        pass\n\n    async def process_llm_command_async(self, command: str, user_id: str = "default"):\n        """Process LLM command asynchronously"""\n        if not self.llm_interface:\n            return "LLM interface not initialized. Please set API key."\n\n        # Process through LLM interface\n        response = await self.llm_interface.process_user_input(command, user_id)\n\n        # Publish speech response\n        speech_msg = String()\n        speech_msg.data = response.text\n        self.speech_pub.publish(speech_msg)\n\n        # Execute action if required\n        if response.execution_required and response.action:\n            success = await self.execute_robot_action(response.action, response.parameters or {})\n            return f"{response.text} Action execution: {\'Success\' if success else \'Failed\'}"\n\n        return response.text\n\n    async def execute_robot_action(self, action: str, parameters: dict):\n        """Execute robot action based on LLM command"""\n        try:\n            if action == \'navigate\':\n                return await self.execute_navigation(parameters)\n            elif action == \'grasp\':\n                return await self.execute_grasp(parameters)\n            elif action == \'speak\':\n                return await self.execute_speech(parameters)\n            else:\n                self.get_logger().warning(f\'Unknown action: {action}\')\n                return False\n        except Exception as e:\n            self.get_logger().error(f\'Error executing action {action}: {e}\')\n            return False\n\n    async def execute_navigation(self, parameters: dict):\n        """Execute navigation action"""\n        destination = parameters.get(\'destination\', \'unknown\')\n\n        # Create navigation goal\n        goal_msg = PoseStamped()\n        goal_msg.header.stamp = self.get_clock().now().to_msg()\n        goal_msg.header.frame_id = \'map\'\n\n        # Set destination coordinates (simplified)\n        if destination.lower() == \'kitchen\':\n            goal_msg.pose.position.x = 2.0\n            goal_msg.pose.position.y = 1.0\n        elif destination.lower() == \'living_room\':\n            goal_msg.pose.position.x = -1.0\n            goal_msg.pose.position.y = 0.5\n        else:\n            self.get_logger().warning(f\'Unknown destination: {destination}\')\n            return False\n\n        goal_msg.pose.orientation.w = 1.0  # Default orientation\n\n        self.navigation_pub.publish(goal_msg.pose)\n        return True\n\n    async def execute_grasp(self, parameters: dict):\n        """Execute grasp action"""\n        object_name = parameters.get(\'object\', \'unknown\')\n        self.get_logger().info(f\'Attempting to grasp {object_name}\')\n\n        # In practice, this would interface with manipulation system\n        # For now, return success\n        return True\n\n    async def execute_speech(self, parameters: dict):\n        """Execute speech action"""\n        text = parameters.get(\'text\', \'\')\n        speech_msg = String()\n        speech_msg.data = text\n        self.speech_pub.publish(speech_msg)\n        return True\n\n    async def execute_llm_command(self, goal_handle):\n        """Execute the LLM command goal"""\n        self.get_logger().info(f\'Executing LLM command: {goal_handle.request.command}\')\n\n        # Parse command and parameters\n        command = goal_handle.request.command\n        params_str = goal_handle.request.parameters\n\n        try:\n            parameters = json.loads(params_str) if params_str else {}\n        except json.JSONDecodeError:\n            parameters = {}\n            self.get_logger().warning(\'Could not parse parameters as JSON\')\n\n        # Process command asynchronously\n        response_text = await self.process_llm_command_async(command)\n\n        # Create result\n        result = LLMCommand.Result()\n        result.response = response_text\n        result.success = True\n\n        # Set result\n        goal_handle.succeed()\n        return result\n\n    def set_llm_interface(self, llm_interface):\n        """Set the LLM interface"""\n        self.llm_interface = llm_interface\n\ndef main(args=None):\n    rclpy.init(args=args)\n\n    node = LLMRobotNode()\n\n    # Example of how to set up the LLM interface\n    # This would typically be done after getting API keys from parameters or config\n    """\n    api_key = node.declare_parameter(\'openai_api_key\', \'\').value\n    if api_key:\n        llm_interface = LLMRobotInterface(api_key)\n        node.set_llm_interface(llm_interface)\n    """\n\n    # Use multi-threaded executor to handle callbacks\n    executor = MultiThreadedExecutor(num_threads=4)\n    executor.add_node(node)\n\n    try:\n        executor.spin()\n    except KeyboardInterrupt:\n        pass\n    finally:\n        node.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == \'__main__\':\n    main()\n'})}),"\n",(0,s.jsx)(n.h3,{id:"multimodal-integration",children:"Multimodal Integration"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'# multimodal_integration.py\nimport numpy as np\nimport cv2\nfrom PIL import Image\nimport base64\nimport io\nimport requests\nimport json\nfrom typing import Dict, Any, List, Optional\n\nclass MultimodalLLMInterface:\n    """Interface for multimodal LLM integration (text + vision)"""\n    def __init__(self, api_key: str, model: str = "gpt-4-vision-preview"):\n        self.api_key = api_key\n        self.model = model\n        self.base_url = "https://api.openai.com/v1/chat/completions"\n\n    def encode_image(self, image_path: str) -> str:\n        """Encode image to base64 for API"""\n        with open(image_path, "rb") as image_file:\n            return base64.b64encode(image_file.read()).decode(\'utf-8\')\n\n    def encode_pil_image(self, pil_image: Image.Image) -> str:\n        """Encode PIL image to base64"""\n        buffer = io.BytesIO()\n        pil_image.save(buffer, format="JPEG")\n        return base64.b64encode(buffer.getvalue()).decode(\'utf-8\')\n\n    def process_vision_command(self, image: Image.Image, text_command: str) -> str:\n        """Process a command that includes visual context"""\n        base64_image = self.encode_pil_image(image)\n\n        headers = {\n            "Content-Type": "application/json",\n            "Authorization": f"Bearer {self.api_key}"\n        }\n\n        payload = {\n            "model": self.model,\n            "messages": [\n                {\n                    "role": "user",\n                    "content": [\n                        {\n                            "type": "text",\n                            "text": f"Based on this image, please help me: {text_command}"\n                        },\n                        {\n                            "type": "image_url",\n                            "image_url": {\n                                "url": f"data:image/jpeg;base64,{base64_image}"\n                            }\n                        }\n                    ]\n                }\n            ],\n            "max_tokens": 300\n        }\n\n        response = requests.post(self.base_url, headers=headers, json=payload)\n\n        if response.status_code == 200:\n            result = response.json()\n            return result[\'choices\'][0][\'message\'][\'content\']\n        else:\n            raise Exception(f"API request failed: {response.status_code}, {response.text}")\n\n    def describe_scene(self, image: Image.Image) -> str:\n        """Generate a description of the current scene"""\n        base64_image = self.encode_pil_image(image)\n\n        headers = {\n            "Content-Type": "application/json",\n            "Authorization": f"Bearer {self.api_key}"\n        }\n\n        payload = {\n            "model": self.model,\n            "messages": [\n                {\n                    "role": "user",\n                    "content": [\n                        {\n                            "type": "text",\n                            "text": "Please provide a detailed description of this scene, including objects, people, and the general environment."\n                        },\n                        {\n                            "type": "image_url",\n                            "image_url": {\n                                "url": f"data:image/jpeg;base64,{base64_image}"\n                            }\n                        }\n                    ]\n                }\n            ],\n            "max_tokens": 500\n        }\n\n        response = requests.post(self.base_url, headers=headers, json=payload)\n\n        if response.status_code == 200:\n            result = response.json()\n            return result[\'choices\'][0][\'message\'][\'content\']\n        else:\n            raise Exception(f"API request failed: {response.status_code}, {response.text}")\n\n    def identify_objects(self, image: Image.Image) -> List[Dict[str, Any]]:\n        """Identify and locate objects in the image"""\n        base64_image = self.encode_pil_image(image)\n\n        headers = {\n            "Content-Type": "application/json",\n            "Authorization": f"Bearer {self.api_key}"\n        }\n\n        payload = {\n            "model": self.model,\n            "messages": [\n                {\n                    "role": "user",\n                    "content": [\n                        {\n                            "type": "text",\n                            "text": """Identify the objects in this image and provide their locations.\n                            Respond in JSON format with the following structure:\n                            {\n                                "objects": [\n                                    {\n                                        "name": "object name",\n                                        "category": "object category",\n                                        "position": {"x": number, "y": number},\n                                        "confidence": 0.0-1.0\n                                    }\n                                ]\n                            }"""\n                        },\n                        {\n                            "type": "image_url",\n                            "image_url": {\n                                "url": f"data:image/jpeg;base64,{base64_image}"\n                            }\n                        }\n                    ]\n                }\n            ],\n            "max_tokens": 500,\n            "temperature": 0.3\n        }\n\n        response = requests.post(self.base_url, headers=headers, json=payload)\n\n        if response.status_code == 200:\n            result = response.json()\n            response_text = result[\'choices\'][0][\'message\'][\'content\']\n\n            # Extract JSON from response\n            import re\n            json_match = re.search(r\'\\{.*\\}\', response_text, re.DOTALL)\n            if json_match:\n                json_str = json_match.group()\n                return json.loads(json_str).get(\'objects\', [])\n\n        return []\n\n    def answer_vision_question(self, image: Image.Image, question: str) -> str:\n        """Answer a specific question about the image"""\n        base64_image = self.encode_pil_image(image)\n\n        headers = {\n            "Content-Type": "application/json",\n            "Authorization": f"Bearer {self.api_key}"\n        }\n\n        payload = {\n            "model": self.model,\n            "messages": [\n                {\n                    "role": "user",\n                    "content": [\n                        {\n                            "type": "text",\n                            "text": question\n                        },\n                        {\n                            "type": "image_url",\n                            "image_url": {\n                                "url": f"data:image/jpeg;base64,{base64_image}"\n                            }\n                        }\n                    ]\n                }\n            ],\n            "max_tokens": 300\n        }\n\n        response = requests.post(self.base_url, headers=headers, json=payload)\n\n        if response.status_code == 200:\n            result = response.json()\n            return result[\'choices\'][0][\'message\'][\'content\']\n        else:\n            raise Exception(f"API request failed: {response.status_code}, {response.text}")\n\nclass VisionEnhancedRobotInterface(LLMRobotInterface):\n    """Robot interface enhanced with vision capabilities"""\n    def __init__(self, api_key: str, model: str = "gpt-3.5-turbo"):\n        super().__init__(api_key, model)\n        self.vision_interface = MultimodalLLMInterface(api_key, "gpt-4-vision-preview")\n        self.last_seen_objects = []\n\n    async def process_vision_enhanced_input(self, user_input: str, camera_image: Optional[Image.Image] = None) -> RobotResponse:\n        """Process input with vision enhancement"""\n        if camera_image is None:\n            # Process without vision\n            return await self.process_user_input(user_input)\n\n        # Check if the user\'s request involves vision\n        vision_keywords = [\'see\', \'look\', \'there\', \'what\', \'where\', \'find\', \'locate\', \'show\', \'object\', \'thing\']\n        needs_vision = any(keyword in user_input.lower() for keyword in vision_keywords)\n\n        if needs_vision:\n            try:\n                # Process with vision context\n                vision_response = self.vision_interface.process_vision_command(camera_image, user_input)\n\n                # Extract intent from vision-enhanced response\n                context = self.context_manager.get_context_for_prompt()\n                intent_prompt = self.prompt_builder.build_intent_extraction_prompt(\n                    f"{user_input} (Image context: {vision_response})", context\n                )\n\n                # For simplicity, we\'ll return a response that includes vision info\n                return RobotResponse(\n                    text=f"Based on what I see: {vision_response}",\n                    execution_required=False\n                )\n            except Exception as e:\n                self.logger.error(f"Vision processing error: {e}")\n                # Fall back to text-only processing\n                return await self.process_user_input(user_input)\n        else:\n            # Process without vision\n            return await self.process_user_input(user_input)\n\n    def update_seen_objects(self, image: Image.Image):\n        """Update the list of seen objects"""\n        try:\n            objects = self.vision_interface.identify_objects(image)\n            self.last_seen_objects = objects\n            self.logger.info(f"Identified {len(objects)} objects in scene")\n        except Exception as e:\n            self.logger.error(f"Error identifying objects: {e}")\n\n    def get_object_location(self, object_name: str) -> Optional[Dict[str, Any]]:\n        """Get the location of a specific object"""\n        for obj in self.last_seen_objects:\n            if object_name.lower() in obj[\'name\'].lower():\n                return obj\n        return None\n\n# Example usage\ndef example_multimodal_integration():\n    """Example of multimodal integration"""\n    print("Multimodal LLM Integration Example")\n\n    # This would require actual API keys to run\n    # api_key = "your-openai-api-key"\n    # vision_interface = MultimodalLLMInterface(api_key)\n\n    # For demonstration, we\'ll show the structure\n    print("Multimodal interface would connect to vision models like GPT-4 Vision")\n    print("Key capabilities:")\n    print("1. Scene description and understanding")\n    print("2. Object identification and localization")\n    print("3. Question answering about visual content")\n    print("4. Vision-guided action planning")\n\nif __name__ == "__main__":\n    example_multimodal_integration()\n'})}),"\n",(0,s.jsx)(n.h2,{id:"cognitive-planning-with-llms",children:"Cognitive Planning with LLMs"}),"\n",(0,s.jsx)(n.h3,{id:"planning-and-execution-framework",children:"Planning and Execution Framework"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'# cognitive_planning.py\nimport asyncio\nimport json\nfrom typing import Dict, List, Any, Optional, Tuple\nfrom dataclasses import dataclass\nfrom enum import Enum\nimport re\n\nclass PlanStatus(Enum):\n    PLANNING = "planning"\n    EXECUTING = "executing"\n    COMPLETED = "completed"\n    FAILED = "failed"\n    ABORTED = "aborted"\n\nclass ActionStatus(Enum):\n    PENDING = "pending"\n    IN_PROGRESS = "in_progress"\n    SUCCESS = "success"\n    FAILED = "failed"\n\n@dataclass\nclass ActionStep:\n    """A single step in a plan"""\n    id: str\n    action: str\n    parameters: Dict[str, Any]\n    description: str\n    dependencies: List[str]  # IDs of actions this depends on\n    status: ActionStatus = ActionStatus.PENDING\n    result: Optional[Any] = None\n\n@dataclass\nclass CognitivePlan:\n    """A complete cognitive plan"""\n    id: str\n    goal: str\n    steps: List[ActionStep]\n    status: PlanStatus = PlanStatus.PLANNING\n    current_step: int = 0\n    created_at: float = 0.0\n    completed_at: Optional[float] = None\n\nclass PlanExecutor:\n    """Executes cognitive plans with LLM assistance"""\n    def __init__(self, llm_interface):\n        self.llm_interface = llm_interface\n        self.active_plans = {}\n        self.action_handlers = {\n            \'navigate\': self.execute_navigate,\n            \'grasp\': self.execute_grasp,\n            \'speak\': self.execute_speak,\n            \'perceive\': self.execute_perceive,\n            \'wait\': self.execute_wait,\n            \'query\': self.execute_query\n        }\n\n    async def create_plan(self, goal: str, context: str = "") -> CognitivePlan:\n        """Create a plan for achieving a goal using LLM"""\n        plan_prompt = f"""\nCreate a detailed plan to achieve the following goal: "{goal}"\n\nContext: {context}\n\nThe plan should be broken down into specific, executable actions. Each action should be one of: navigate, grasp, speak, perceive, wait, query.\n\nProvide the plan in JSON format:\n{{\n    "steps": [\n        {{\n            "id": "unique_id",\n            "action": "action_type",\n            "parameters": {{"param": "value"}},\n            "description": "what this step does",\n            "dependencies": ["id_of_step_this_depends_on"]\n        }}\n    ]\n}}\n\nExample plan for "Bring me a cup of water from the kitchen":\n{{\n    "steps": [\n        {{\n            "id": "1",\n            "action": "navigate",\n            "parameters": {{"destination": "kitchen"}},\n            "description": "Go to the kitchen",\n            "dependencies": []\n        }},\n        {{\n            "id": "2",\n            "action": "perceive",\n            "parameters": {{"target": "cup"}},\n            "description": "Look for a cup",\n            "dependencies": ["1"]\n        }},\n        {{\n            "id": "3",\n            "action": "grasp",\n            "parameters": {{"object": "cup"}},\n            "description": "Pick up the cup",\n            "dependencies": ["2"]\n        }},\n        {{\n            "id": "4",\n            "action": "navigate",\n            "parameters": {{"destination": "user"}},\n            "description": "Return to user",\n            "dependencies": ["3"]\n        }}\n    ]\n}}\n"""\n\n        try:\n            response = await self.llm_interface.process_user_input(plan_prompt)\n\n            # Extract JSON from response\n            json_match = re.search(r\'\\{.*\\}\', response.text, re.DOTALL)\n            if json_match:\n                json_str = json_match.group()\n                plan_data = json.loads(json_str)\n\n                steps = []\n                for step_data in plan_data.get(\'steps\', []):\n                    step = ActionStep(\n                        id=step_data[\'id\'],\n                        action=step_data[\'action\'],\n                        parameters=step_data.get(\'parameters\', {}),\n                        description=step_data[\'description\'],\n                        dependencies=step_data.get(\'dependencies\', [])\n                    )\n                    steps.append(step)\n\n                plan = CognitivePlan(\n                    id=f"plan_{len(self.active_plans)}",\n                    goal=goal,\n                    steps=steps,\n                    created_at=time.time()\n                )\n\n                self.active_plans[plan.id] = plan\n                return plan\n\n        except Exception as e:\n            print(f"Error creating plan: {e}")\n\n        # Return a default plan if LLM fails\n        return CognitivePlan(\n            id=f"plan_{len(self.active_plans)}",\n            goal=goal,\n            steps=[],\n            status=PlanStatus.FAILED\n        )\n\n    async def execute_plan(self, plan_id: str) -> PlanStatus:\n        """Execute a cognitive plan"""\n        if plan_id not in self.active_plans:\n            return PlanStatus.FAILED\n\n        plan = self.active_plans[plan_id]\n        plan.status = PlanStatus.EXECUTING\n\n        for i, step in enumerate(plan.steps):\n            if step.status == ActionStatus.PENDING:\n                # Check dependencies\n                if not self.check_dependencies_satisfied(step, plan):\n                    continue\n\n                # Execute the step\n                step.status = ActionStatus.IN_PROGRESS\n                success = await self.execute_action_step(step)\n\n                if success:\n                    step.status = ActionStatus.SUCCESS\n                    plan.current_step = i + 1\n                else:\n                    step.status = ActionStatus.FAILED\n                    plan.status = PlanStatus.FAILED\n                    return PlanStatus.FAILED\n\n        # Check if all steps are completed\n        if all(step.status == ActionStatus.SUCCESS for step in plan.steps):\n            plan.status = PlanStatus.COMPLETED\n            plan.completed_at = time.time()\n            return PlanStatus.COMPLETED\n\n        return plan.status\n\n    def check_dependencies_satisfied(self, step: ActionStep, plan: CognitivePlan) -> bool:\n        """Check if all dependencies for a step are satisfied"""\n        for dep_id in step.dependencies:\n            dep_step = next((s for s in plan.steps if s.id == dep_id), None)\n            if dep_step is None or dep_step.status != ActionStatus.SUCCESS:\n                return False\n        return True\n\n    async def execute_action_step(self, step: ActionStep) -> bool:\n        """Execute a single action step"""\n        if step.action in self.action_handlers:\n            try:\n                result = await self.action_handlers[step.action](step.parameters)\n                step.result = result\n                return True\n            except Exception as e:\n                print(f"Error executing action {step.action}: {e}")\n                return False\n        else:\n            print(f"Unknown action: {step.action}")\n            return False\n\n    # Action execution methods\n    async def execute_navigate(self, parameters: Dict[str, Any]) -> bool:\n        """Execute navigation action"""\n        destination = parameters.get(\'destination\', \'unknown\')\n        print(f"Navigating to {destination}")\n        # In practice, this would interface with navigation system\n        return True\n\n    async def execute_grasp(self, parameters: Dict[str, Any]) -> bool:\n        """Execute grasping action"""\n        object_name = parameters.get(\'object\', \'unknown\')\n        print(f"Grasping {object_name}")\n        # In practice, this would interface with manipulation system\n        return True\n\n    async def execute_speak(self, parameters: Dict[str, Any]) -> bool:\n        """Execute speech action"""\n        text = parameters.get(\'text\', \'\')\n        print(f"Speaking: {text}")\n        # In practice, this would interface with TTS system\n        return True\n\n    async def execute_perceive(self, parameters: Dict[str, Any]) -> Dict[str, Any]:\n        """Execute perception action"""\n        target = parameters.get(\'target\', \'environment\')\n        print(f"Perceiving {target}")\n        # In practice, this would interface with perception system\n        return {"success": True, "objects": [target]}\n\n    async def execute_wait(self, parameters: Dict[str, Any]) -> bool:\n        """Execute wait action"""\n        duration = parameters.get(\'duration\', 1.0)\n        print(f"Waiting for {duration} seconds")\n        await asyncio.sleep(duration)\n        return True\n\n    async def execute_query(self, parameters: Dict[str, Any]) -> Dict[str, Any]:\n        """Execute query action"""\n        query = parameters.get(\'query\', \'\')\n        print(f"Querying: {query}")\n        # In practice, this would interface with knowledge system\n        return {"result": "query_result"}\n\nclass CognitiveRobotInterface(LLMRobotInterface):\n    """Robot interface with cognitive planning capabilities"""\n    def __init__(self, api_key: str, model: str = "gpt-3.5-turbo"):\n        super().__init__(api_key, model)\n        self.plan_executor = PlanExecutor(self)\n\n    async def process_complex_task(self, user_input: str) -> RobotResponse:\n        """Process a complex task that requires planning"""\n        # Create a plan for the task\n        plan = await self.plan_executor.create_plan(user_input)\n\n        if plan.status == PlanStatus.FAILED:\n            return RobotResponse(\n                text="I\'m sorry, I couldn\'t create a plan for that task.",\n                execution_required=False\n            )\n\n        # Execute the plan\n        status = await self.plan_executor.execute_plan(plan.id)\n\n        if status == PlanStatus.COMPLETED:\n            return RobotResponse(\n                text=f"I\'ve completed the task: {user_input}",\n                execution_required=False\n            )\n        else:\n            return RobotResponse(\n                text=f"I couldn\'t complete the task: {user_input}. Something went wrong.",\n                execution_required=False\n            )\n\n    async def process_user_input(self, user_input: str, user_id: str = "default") -> RobotResponse:\n        """Enhanced input processing with planning capability"""\n        # Check if this is a complex task that needs planning\n        complex_task_indicators = [\n            \'bring me\', \'get me\', \'go to\', \'find\', \'help me\',\n            \'make\', \'prepare\', \'do\', \'complete\', \'perform\'\n        ]\n\n        is_complex_task = any(indicator in user_input.lower() for indicator in complex_task_indicators)\n\n        if is_complex_task:\n            return await self.process_complex_task(user_input)\n        else:\n            # Use regular processing for simple commands\n            return await super().process_user_input(user_input, user_id)\n\n# Example usage\nasync def example_cognitive_planning():\n    """Example of cognitive planning"""\n    print("Cognitive Planning Example")\n\n    # This would require an API key to run\n    # api_key = "your-openai-api-key"\n    # cognitive_interface = CognitiveRobotInterface(api_key)\n\n    # For demonstration, show the concept\n    print("Cognitive planning involves:")\n    print("1. Breaking down complex goals into action steps")\n    print("2. Managing dependencies between actions")\n    print("3. Executing plans with error handling")\n    print("4. Adapting plans based on execution results")\n\n    # Example plan for a complex task\n    print("\\nExample plan for \'Bring me a cup of water from the kitchen\':")\n    print("Step 1: Navigate to kitchen")\n    print("Step 2: Perceive and locate cup")\n    print("Step 3: Grasp the cup")\n    print("Step 4: Navigate to user")\n    print("Step 5: Deliver cup to user")\n\nif __name__ == "__main__":\n    import time\n    asyncio.run(example_cognitive_planning())\n'})}),"\n",(0,s.jsx)(n.h2,{id:"knowledge-check",children:"Knowledge Check"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsx)(n.li,{children:"What are the main benefits of integrating LLMs into robotics systems?"}),"\n",(0,s.jsx)(n.li,{children:"How does the architecture for LLM-robot interfaces handle safety considerations?"}),"\n",(0,s.jsx)(n.li,{children:"What are the key components of a cognitive planning system for robots?"}),"\n",(0,s.jsx)(n.li,{children:"How does multimodal integration enhance LLM capabilities in robotics?"}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"summary",children:"Summary"}),"\n",(0,s.jsx)(n.p,{children:"This chapter explored the integration of Large Language Models in robotics for conversational AI. We covered the architecture for LLM-robot interfaces, including safety considerations, contextual awareness, and multimodal integration. We also examined cognitive planning systems that use LLMs to break down complex tasks into executable robot actions. The chapter provided practical examples of how to implement these systems with proper safety checks and execution frameworks."}),"\n",(0,s.jsx)(n.h2,{id:"next-steps",children:"Next Steps"}),"\n",(0,s.jsx)(n.p,{children:"In the next chapter, we'll explore speech recognition and natural language understanding in robotics, covering voice command processing, dialogue management, and multi-modal interaction techniques for humanoid robots."})]})}function u(e={}){const{wrapper:n}={...(0,a.R)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(p,{...e})}):p(e)}},8453(e,n,t){t.d(n,{R:()=>r,x:()=>i});var s=t(6540);const a={},o=s.createContext(a);function r(e){const n=s.useContext(o);return s.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function i(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(a):e.components||a:r(e.components),s.createElement(o.Provider,{value:n},e.children)}}}]);