"use strict";(globalThis.webpackChunkphysical_ai_humanoid_robotics_book=globalThis.webpackChunkphysical_ai_humanoid_robotics_book||[]).push([[701],{3834(n,e,t){t.r(e),t.d(e,{assets:()=>p,contentTitle:()=>o,default:()=>_,frontMatter:()=>a,metadata:()=>s,toc:()=>c});var i=t(4848),r=t(8453);const a={sidebar_position:15,title:"Chapter 15: Manipulation and Grasping"},o="Chapter 15: Manipulation and Grasping",s={id:"part5/chapter15",title:"Chapter 15: Manipulation and Grasping",description:"Learning Objectives",source:"@site/docs/part5/chapter15.md",sourceDirName:"part5",slug:"/part5/chapter15",permalink:"/Physical-AI-Humanoid-Robotics/docs/part5/chapter15",draft:!1,unlisted:!1,editUrl:"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/part5/chapter15.md",tags:[],version:"current",sidebarPosition:15,frontMatter:{sidebar_position:15,title:"Chapter 15: Manipulation and Grasping"},sidebar:"tutorialSidebar",previous:{title:"Chapter 14: Bipedal Locomotion and Balance Control",permalink:"/Physical-AI-Humanoid-Robotics/docs/part5/chapter14"},next:{title:"Chapter 16: Natural Human-Robot Interaction",permalink:"/Physical-AI-Humanoid-Robotics/docs/part5/chapter16"}},p={},c=[{value:"Learning Objectives",id:"learning-objectives",level:2},{value:"Introduction to Humanoid Manipulation",id:"introduction-to-humanoid-manipulation",level:2},{value:"Challenges in Humanoid Manipulation",id:"challenges-in-humanoid-manipulation",level:3},{value:"Humanoid Hand Anatomy",id:"humanoid-hand-anatomy",level:3},{value:"Grasp Planning Algorithms",id:"grasp-planning-algorithms",level:2},{value:"Geometric Grasp Planning",id:"geometric-grasp-planning",level:3},{value:"Learning-Based Grasp Planning",id:"learning-based-grasp-planning",level:3},{value:"Dexterous Manipulation Techniques",id:"dexterous-manipulation-techniques",level:2},{value:"Multi-Finger Grasp Synthesis",id:"multi-finger-grasp-synthesis",level:3},{value:"Human-Robot Interaction Design",id:"human-robot-interaction-design",level:2},{value:"Multi-Modal Interaction for Manipulation",id:"multi-modal-interaction-for-manipulation",level:3},{value:"Advanced Manipulation Strategies",id:"advanced-manipulation-strategies",level:2},{value:"Dual-Arm Coordination",id:"dual-arm-coordination",level:3},{value:"Knowledge Check",id:"knowledge-check",level:2},{value:"Summary",id:"summary",level:2},{value:"Next Steps",id:"next-steps",level:2}];function l(n){const e={code:"code",h1:"h1",h2:"h2",h3:"h3",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,r.R)(),...n.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(e.h1,{id:"chapter-15-manipulation-and-grasping",children:"Chapter 15: Manipulation and Grasping"}),"\n",(0,i.jsx)(e.h2,{id:"learning-objectives",children:"Learning Objectives"}),"\n",(0,i.jsxs)(e.ul,{children:["\n",(0,i.jsx)(e.li,{children:"Understand dexterous manipulation techniques for humanoid robots"}),"\n",(0,i.jsx)(e.li,{children:"Implement grasp planning algorithms for various objects"}),"\n",(0,i.jsx)(e.li,{children:"Design human-robot interaction systems for manipulation tasks"}),"\n",(0,i.jsx)(e.li,{children:"Master multi-modal interaction approaches for manipulation"}),"\n"]}),"\n",(0,i.jsx)(e.h2,{id:"introduction-to-humanoid-manipulation",children:"Introduction to Humanoid Manipulation"}),"\n",(0,i.jsx)(e.p,{children:"Humanoid manipulation involves the use of anthropomorphic hands and arms to interact with objects in the environment. Unlike simple grippers, humanoid hands have multiple degrees of freedom that allow for complex, dexterous manipulation similar to human capabilities."}),"\n",(0,i.jsx)(e.h3,{id:"challenges-in-humanoid-manipulation",children:"Challenges in Humanoid Manipulation"}),"\n",(0,i.jsxs)(e.ol,{children:["\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Complex Kinematics"}),": Multiple joints in fingers and wrist"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Underactuation"}),": More degrees of freedom than actuators"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Tactile Sensing"}),": Limited tactile feedback compared to humans"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Object Recognition"}),": Identifying graspable parts of objects"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Force Control"}),": Managing contact forces during manipulation"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Multi-task Coordination"}),": Coordinating both arms for complex tasks"]}),"\n"]}),"\n",(0,i.jsx)(e.h3,{id:"humanoid-hand-anatomy",children:"Humanoid Hand Anatomy"}),"\n",(0,i.jsx)(e.p,{children:"Humanoid hands typically have:"}),"\n",(0,i.jsxs)(e.ul,{children:["\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Thumb"}),": 2-3 joints with opposition capability"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Index Finger"}),": 3 joints for precision grasps"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Middle Finger"}),": 3 joints for power grasps"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Ring Finger"}),": 3 joints"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Pinky Finger"}),": 3 joints"]}),"\n"]}),"\n",(0,i.jsx)(e.h2,{id:"grasp-planning-algorithms",children:"Grasp Planning Algorithms"}),"\n",(0,i.jsx)(e.h3,{id:"geometric-grasp-planning",children:"Geometric Grasp Planning"}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-python",children:'# geometric_grasp_planning.py\nimport numpy as np\nfrom scipy.spatial import distance\nfrom math import atan2, sqrt\nimport open3d as o3d\n\nclass GeometricGraspPlanner:\n    def __init__(self):\n        self.finger_length = 0.08  # meters\n        self.thumb_length = 0.06\n        self.grasp_width_range = (0.02, 0.15)  # minimum to maximum graspable width\n\n    def find_grasp_points(self, object_mesh):\n        """Find potential grasp points on object surface"""\n        # Sample points on object surface\n        surface_points = self.sample_surface_points(object_mesh)\n\n        # Calculate surface normals\n        normals = self.calculate_surface_normals(object_mesh, surface_points)\n\n        # Find grasp candidates based on geometric criteria\n        grasp_candidates = []\n\n        for i, point in enumerate(surface_points):\n            normal = normals[i]\n\n            # Check if this point is suitable for grasping\n            if self.is_graspable_point(object_mesh, point, normal):\n                grasp_candidates.append({\n                    \'position\': point,\n                    \'normal\': normal,\n                    \'quality\': self.calculate_grasp_quality(point, normal, surface_points)\n                })\n\n        return grasp_candidates\n\n    def sample_surface_points(self, mesh, num_points=1000):\n        """Sample points on object surface"""\n        # Use Poisson disk sampling or random sampling\n        # For now, using random sampling of mesh vertices\n        vertices = np.asarray(mesh.vertices)\n        faces = np.asarray(mesh.triangles)\n\n        # Sample random points on triangles\n        points = []\n        for _ in range(num_points):\n            # Pick a random triangle\n            triangle_idx = np.random.randint(0, len(faces))\n            triangle = faces[triangle_idx]\n\n            # Sample random point on triangle\n            v0 = vertices[triangle[0]]\n            v1 = vertices[triangle[1]]\n            v2 = vertices[triangle[2]]\n\n            # Barycentric coordinates for random point\n            r1 = np.random.random()\n            r2 = np.random.random()\n\n            if r1 + r2 > 1:\n                r1 = 1 - r1\n                r2 = 1 - r2\n\n            point = (1 - r1 - r2) * v0 + r1 * v1 + r2 * v2\n            points.append(point)\n\n        return np.array(points)\n\n    def calculate_surface_normals(self, mesh, points):\n        """Calculate surface normals at given points"""\n        # For each point, find the closest face and use its normal\n        vertices = np.asarray(mesh.vertices)\n        faces = np.asarray(mesh.triangles)\n\n        normals = []\n        for point in points:\n            # Find closest vertex\n            closest_vertex_idx = np.argmin(np.linalg.norm(vertices - point, axis=1))\n\n            # Find faces that contain this vertex\n            face_indices = []\n            for i, face in enumerate(faces):\n                if closest_vertex_idx in face:\n                    face_indices.append(i)\n\n            if face_indices:\n                # Average normals of adjacent faces\n                face_normals = []\n                for face_idx in face_indices:\n                    face = faces[face_idx]\n                    v0, v1, v2 = vertices[face[0]], vertices[face[1]], vertices[face[2]]\n                    normal = np.cross(v1 - v0, v2 - v0)\n                    normal = normal / np.linalg.norm(normal)\n                    face_normals.append(normal)\n\n                avg_normal = np.mean(face_normals, axis=0)\n                avg_normal = avg_normal / np.linalg.norm(avg_normal)\n            else:\n                avg_normal = np.array([0, 0, 1])  # Default normal\n\n            normals.append(avg_normal)\n\n        return np.array(normals)\n\n    def is_graspable_point(self, mesh, point, normal):\n        """Check if a point is suitable for grasping"""\n        # Check if the surface is flat enough\n        local_curvature = self.estimate_local_curvature(mesh, point, normal)\n        if local_curvature > 0.1:  # Too curved\n            return False\n\n        # Check if the surface is not too steep\n        gravity_dir = np.array([0, 0, -1])  # Assuming gravity is down\n        angle_with_gravity = np.arccos(np.clip(np.dot(normal, gravity_dir), -1, 1))\n        if angle_with_gravity > np.pi / 3:  # Too steep (60 degrees)\n            return False\n\n        return True\n\n    def estimate_local_curvature(self, mesh, point, normal, radius=0.02):\n        """Estimate local curvature around a point"""\n        vertices = np.asarray(mesh.vertices)\n\n        # Find nearby points\n        distances = np.linalg.norm(vertices - point, axis=1)\n        nearby_indices = np.where(distances < radius)[0]\n\n        if len(nearby_indices) < 3:\n            return 0.0\n\n        nearby_points = vertices[nearby_indices]\n\n        # Fit a plane to nearby points and measure deviation\n        centered_points = nearby_points - point\n        cov_matrix = np.cov(centered_points.T)\n        eigenvalues = np.linalg.eigvals(cov_matrix)\n\n        # Curvature is related to the smallest eigenvalue\n        curvature = min(eigenvalues)\n        return abs(curvature)\n\n    def calculate_grasp_quality(self, point, normal, surface_points):\n        """Calculate quality metric for a grasp point"""\n        # Distance to other surface points (avoid edges)\n        distances = np.linalg.norm(surface_points - point, axis=1)\n        min_distance = np.min(distances[distances > 1e-6])  # Exclude the point itself\n\n        # Surface flatness (use normal consistency in neighborhood)\n        neighborhood_size = 10\n        nearest_indices = np.argsort(distances)[:neighborhood_size]\n        neighborhood_normals = [normal]  # We don\'t have normals for all points in this simplified version\n\n        # Simple quality metric\n        quality = min_distance * 10  # Prefer points away from edges\n        quality += 0.5  # Base quality\n\n        return min(quality, 1.0)  # Normalize to [0, 1]\n\n    def plan_grasp_poses(self, object_mesh, grasp_candidates, num_poses=5):\n        """Plan multiple grasp poses for the object"""\n        grasp_poses = []\n\n        # Sort candidates by quality\n        sorted_candidates = sorted(grasp_candidates, key=lambda x: x[\'quality\'], reverse=True)\n\n        for candidate in sorted_candidates[:num_poses]:\n            # Generate grasp pose from contact point and normal\n            grasp_pose = self.generate_grasp_pose(candidate[\'position\'], candidate[\'normal\'])\n            grasp_poses.append({\n                \'pose\': grasp_pose,\n                \'quality\': candidate[\'quality\'],\n                \'contact_point\': candidate[\'position\'],\n                \'approach_direction\': candidate[\'normal\']\n            })\n\n        return grasp_poses\n\n    def generate_grasp_pose(self, contact_point, surface_normal):\n        """Generate a complete grasp pose from contact information"""\n        # Create a transformation matrix for the grasp\n        # Z-axis points into the object (opposite to surface normal)\n        z_axis = -surface_normal / np.linalg.norm(surface_normal)\n\n        # X-axis can be chosen arbitrarily, perpendicular to Z\n        if abs(z_axis[2]) < 0.9:  # Not pointing vertically\n            x_axis = np.array([0, 0, 1])  # Point up\n        else:\n            x_axis = np.array([1, 0, 0])  # Point along X\n\n        # Make sure X is perpendicular to Z\n        x_axis = x_axis - np.dot(x_axis, z_axis) * z_axis\n        x_axis = x_axis / np.linalg.norm(x_axis)\n\n        # Y-axis is cross product\n        y_axis = np.cross(z_axis, x_axis)\n\n        # Create rotation matrix\n        rotation = np.column_stack([x_axis, y_axis, z_axis])\n\n        # Create transformation matrix\n        transform = np.eye(4)\n        transform[:3, :3] = rotation\n        transform[:3, 3] = contact_point\n\n        return transform\n\nclass ForceClosureGraspPlanner:\n    """Grasp planner based on force closure analysis"""\n    def __init__(self):\n        self.friction_coefficient = 0.5\n        self.num_fingers = 2  # For simplicity, assuming 2-finger grasp\n\n    def check_force_closure(self, contact_points, contact_normals, object_com):\n        """Check if grasp achieves force closure"""\n        # This is a simplified 2D version\n        # For 3D, we would need to check the convex hull of wrench space\n\n        if len(contact_points) < 2:\n            return False\n\n        # Calculate grasp matrix for force closure\n        # In 2D: [fx1, fy1, tau1; fx2, fy2, tau2; ...]\n        # Where tau is moment about object COM\n\n        grasp_matrix = []\n        for i, (point, normal) in enumerate(zip(contact_points, contact_normals)):\n            # Force components\n            fx, fy = normal[0], normal[1]\n\n            # Moment about COM\n            r = point[:2] - object_com[:2]  # 2D position relative to COM\n            tau = r[0] * normal[1] - r[1] * normal[0]  # Cross product in 2D\n\n            grasp_matrix.append([fx, fy, tau])\n\n        grasp_matrix = np.array(grasp_matrix)\n\n        # Check if the origin is inside the convex hull of columns\n        # This is a simplified check - in practice, more sophisticated methods are used\n        try:\n            # Use linear programming to check if 0 is in convex hull\n            from scipy.optimize import linprog\n\n            # We want to find lambda such that:\n            # sum(lambda_i * column_i) = 0\n            # sum(lambda_i) = 1\n            # lambda_i >= 0\n\n            A_eq = np.column_stack([grasp_matrix.T, np.ones(len(contact_points))])\n            b_eq = np.zeros(4)  # 3 for forces/moments + 1 for sum constraint\n            b_eq[3] = 1  # sum of lambdas = 1\n\n            c = np.zeros(len(contact_points))  # Minimize 0 (feasibility problem)\n            bounds = [(0, None) for _ in range(len(contact_points))]\n\n            result = linprog(c, A_eq=A_eq, b_eq=b_eq, bounds=bounds, method=\'highs\')\n\n            return result.success\n\n        except ImportError:\n            # If scipy not available, use a simpler geometric check\n            return self.geometric_force_closure_check(contact_points, contact_normals)\n\n    def geometric_force_closure_check(self, contact_points, contact_normals):\n        """Simplified geometric check for force closure"""\n        if len(contact_points) < 2:\n            return False\n\n        # For 2 contacts, check if normals point toward each other\n        if len(contact_points) == 2:\n            p1, p2 = contact_points\n            n1, n2 = contact_normals\n\n            # Vector from p1 to p2\n            v12 = p2[:2] - p1[:2]\n\n            # Check if normals point inward\n            inward_1 = np.dot(n1[:2], v12) > 0  # n1 points toward p2\n            inward_2 = np.dot(n2[:2], -v12) > 0  # n2 points toward p1\n\n            return inward_1 and inward_2\n\n        # For more than 2 contacts, check if origin is in convex hull\n        # This is a 2D simplification\n        points_2d = np.array([p[:2] for p in contact_points])\n        return self.points_form_convex_hull_around_origin(points_2d)\n\n    def points_form_convex_hull_around_origin(self, points_2d):\n        """Check if 2D points form a convex hull that contains origin"""\n        from scipy.spatial import ConvexHull\n\n        try:\n            hull = ConvexHull(points_2d)\n\n            # Check if origin is inside the convex hull\n            # This is a simplified check - in practice, you\'d use more robust methods\n            # For now, we\'ll check if origin is "likely" inside by checking\n            # if the centroid is close to origin relative to hull size\n            centroid = np.mean(points_2d[hull.vertices], axis=0)\n            distances = np.linalg.norm(points_2d[hull.vertices] - centroid, axis=1)\n            avg_distance = np.mean(distances)\n\n            # If centroid is close to origin and points are spread out\n            return np.linalg.norm(centroid) < avg_distance * 0.5\n\n        except:\n            return False\n\n# Example usage\ndef example_geometric_grasp_planning():\n    # Create a simple object (box) for demonstration\n    object_mesh = o3d.geometry.TriangleMesh.create_box(width=0.05, height=0.05, depth=0.1)\n\n    # Plan grasps\n    planner = GeometricGraspPlanner()\n    candidates = planner.find_grasp_points(object_mesh)\n    poses = planner.plan_grasp_poses(object_mesh, candidates)\n\n    print(f"Found {len(candidates)} grasp candidates")\n    print(f"Planned {len(poses)} grasp poses")\n\n    if poses:\n        best_pose = poses[0]\n        print(f"Best grasp quality: {best_pose[\'quality\']:.3f}")\n        print(f"Contact point: {best_pose[\'contact_point\']}")\n        print(f"Grasp pose transformation:\\n{best_pose[\'pose\']}")\n\nif __name__ == "__main__":\n    try:\n        example_geometric_grasp_planning()\n    except ImportError:\n        print("Open3D not available, skipping geometric grasp planning example")\n'})}),"\n",(0,i.jsx)(e.h3,{id:"learning-based-grasp-planning",children:"Learning-Based Grasp Planning"}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-python",children:'# learning_based_grasping.py\nimport torch\nimport torch.nn as nn\nimport numpy as np\nimport cv2\n\nclass GraspQualityCNN(nn.Module):\n    """CNN for predicting grasp quality from RGB-D images"""\n    def __init__(self, input_channels=4):  # RGB + D\n        super(GraspQualityCNN, self).__init__()\n\n        # Convolutional layers for feature extraction\n        self.conv_layers = nn.Sequential(\n            nn.Conv2d(input_channels, 32, kernel_size=5, stride=2, padding=2),\n            nn.ReLU(),\n            nn.Conv2d(32, 64, kernel_size=5, stride=2, padding=2),\n            nn.ReLU(),\n            nn.Conv2d(64, 128, kernel_size=3, stride=2, padding=1),\n            nn.ReLU(),\n            nn.Conv2d(128, 256, kernel_size=3, stride=2, padding=1),\n            nn.ReLU()\n        )\n\n        # Calculate the size after convolutions\n        # Assuming input is 224x224, after 4 downsampling steps: 224/2^4 = 14\n        conv_output_size = 256 * 14 * 14  # Adjust based on your input size\n\n        # Fully connected layers for grasp quality prediction\n        self.fc_layers = nn.Sequential(\n            nn.Linear(conv_output_size, 512),\n            nn.ReLU(),\n            nn.Dropout(0.5),\n            nn.Linear(512, 256),\n            nn.ReLU(),\n            nn.Dropout(0.5),\n            nn.Linear(256, 1),\n            nn.Sigmoid()  # Output grasp quality between 0 and 1\n        )\n\n    def forward(self, x):\n        x = self.conv_layers(x)\n        x = x.view(x.size(0), -1)  # Flatten\n        x = self.fc_layers(x)\n        return x\n\nclass GraspPosePredictor(nn.Module):\n    """Network for predicting grasp pose parameters"""\n    def __init__(self, num_outputs=6):  # 3 for position, 3 for orientation\n        super(GraspPosePredictor, self).__init__()\n\n        self.feature_extractor = nn.Sequential(\n            nn.Conv2d(4, 32, kernel_size=5, stride=2, padding=2),\n            nn.ReLU(),\n            nn.Conv2d(32, 64, kernel_size=5, stride=2, padding=2),\n            nn.ReLU(),\n            nn.Conv2d(64, 128, kernel_size=3, stride=2, padding=1),\n            nn.ReLU(),\n        )\n\n        # Calculate feature map size\n        conv_output_size = 128 * 28 * 28  # Adjust based on input size\n\n        self.regressor = nn.Sequential(\n            nn.Linear(conv_output_size, 512),\n            nn.ReLU(),\n            nn.Dropout(0.3),\n            nn.Linear(512, 256),\n            nn.ReLU(),\n            nn.Linear(256, num_outputs)\n        )\n\n    def forward(self, x):\n        features = self.feature_extractor(x)\n        features = features.view(features.size(0), -1)\n        pose_params = self.regressor(features)\n        return pose_params\n\nclass LearningBasedGraspPlanner:\n    def __init__(self):\n        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")\n\n        # Initialize networks\n        self.quality_network = GraspQualityCNN().to(self.device)\n        self.pose_network = GraspPosePredictor().to(self.device)\n\n        # Load pre-trained weights if available\n        self.load_pretrained_models()\n\n    def load_pretrained_models(self):\n        """Load pre-trained models (placeholder)"""\n        # In practice, you would load trained weights\n        pass\n\n    def predict_grasp_quality(self, rgb_image, depth_image, grasp_poses):\n        """Predict grasp quality for given poses using the trained network"""\n        # Preprocess images\n        input_tensor = self.preprocess_input(rgb_image, depth_image)\n\n        # Predict quality for each pose\n        with torch.no_grad():\n            quality_predictions = self.quality_network(input_tensor)\n\n        return quality_predictions.cpu().numpy()\n\n    def predict_grasp_pose(self, rgb_image, depth_image):\n        """Predict optimal grasp pose from RGB-D input"""\n        input_tensor = self.preprocess_input(rgb_image, depth_image)\n\n        with torch.no_grad():\n            pose_params = self.pose_network(input_tensor)\n\n        return pose_params.cpu().numpy()\n\n    def preprocess_input(self, rgb_image, depth_image):\n        """Preprocess RGB-D images for network input"""\n        # Resize images to network input size\n        target_size = (224, 224)\n        rgb_resized = cv2.resize(rgb_image, target_size)\n        depth_resized = cv2.resize(depth_image, target_size)\n\n        # Normalize RGB\n        rgb_normalized = rgb_resized.astype(np.float32) / 255.0\n\n        # Normalize depth\n        depth_normalized = depth_resized.astype(np.float32)\n        depth_normalized = (depth_normalized - depth_normalized.min()) / (depth_normalized.max() - depth_normalized.min() + 1e-6)\n\n        # Stack RGB and depth\n        input_data = np.concatenate([rgb_normalized, np.expand_dims(depth_normalized, axis=-1)], axis=-1)\n\n        # Convert to tensor and add batch dimension\n        input_tensor = torch.FloatTensor(input_data).permute(2, 0, 1).unsqueeze(0)\n\n        return input_tensor.to(self.device)\n\n    def plan_grasps_with_network(self, rgb_image, depth_image, num_candidates=10):\n        """Plan grasps using learning-based approach"""\n        # Predict grasp poses\n        predicted_poses = self.predict_grasp_pose(rgb_image, depth_image)\n\n        # Evaluate multiple candidates\n        grasp_candidates = []\n\n        for i in range(num_candidates):\n            # Generate candidate grasp pose with some variation\n            base_pose = predicted_poses[0] if len(predicted_poses) > 0 else np.zeros(6)\n\n            # Add random variation for diversity\n            variation = np.random.normal(0, 0.1, 6)\n            candidate_pose = base_pose + variation\n\n            # Predict quality for this candidate\n            quality = self.predict_grasp_quality(rgb_image, depth_image, [candidate_pose])\n\n            grasp_candidates.append({\n                \'pose\': candidate_pose,\n                \'quality\': quality[0] if len(quality) > 0 else 0.0,\n                \'position\': candidate_pose[:3],\n                \'orientation\': candidate_pose[3:]\n            })\n\n        # Sort by quality\n        grasp_candidates.sort(key=lambda x: x[\'quality\'], reverse=True)\n\n        return grasp_candidates\n\nclass GraspRefinementNetwork(nn.Module):\n    """Network for refining initial grasp estimates"""\n    def __init__(self):\n        super(GraspRefinementNetwork, self).__init__()\n\n        # Input: initial grasp + object features\n        self.refinement_net = nn.Sequential(\n            nn.Linear(10, 64),  # 6 for pose + 4 for object features\n            nn.ReLU(),\n            nn.Linear(64, 128),\n            nn.ReLU(),\n            nn.Linear(128, 64),\n            nn.ReLU(),\n            nn.Linear(64, 6)   # 3 pos + 3 orient refinement\n        )\n\n    def forward(self, initial_grasp, object_features):\n        # Combine initial grasp and object features\n        combined_input = torch.cat([initial_grasp, object_features], dim=1)\n        refinement = self.refinement_net(combined_input)\n        return refinement\n\n# Example usage\ndef example_learning_based_grasping():\n    # Initialize the planner\n    planner = LearningBasedGraspPlanner()\n\n    # Simulate RGB-D input (in practice, these would come from sensors)\n    rgb_image = np.random.randint(0, 255, (480, 640, 3), dtype=np.uint8)\n    depth_image = np.random.rand(480, 640).astype(np.float32) * 2.0  # 0-2 meters\n\n    # Plan grasps\n    candidates = planner.plan_grasps_with_network(rgb_image, depth_image)\n\n    print(f"Generated {len(candidates)} grasp candidates")\n    if candidates:\n        best_grasp = candidates[0]\n        print(f"Best grasp quality: {best_grasp[\'quality\']:.3f}")\n        print(f"Position: {best_grasp[\'position\']}")\n        print(f"Orientation: {best_grasp[\'orientation\']}")\n\nif __name__ == "__main__":\n    example_learning_based_grasping()\n'})}),"\n",(0,i.jsx)(e.h2,{id:"dexterous-manipulation-techniques",children:"Dexterous Manipulation Techniques"}),"\n",(0,i.jsx)(e.h3,{id:"multi-finger-grasp-synthesis",children:"Multi-Finger Grasp Synthesis"}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-python",children:"# dexterous_manipulation.py\nimport numpy as np\nfrom math import pi, sin, cos, atan2, sqrt\n\nclass MultiFingerGraspSynthesizer:\n    def __init__(self):\n        self.hand_params = {\n            'thumb': {'length': 0.06, 'width': 0.02, 'joints': 3},\n            'index': {'length': 0.08, 'width': 0.015, 'joints': 3},\n            'middle': {'length': 0.085, 'width': 0.015, 'joints': 3},\n            'ring': {'length': 0.08, 'width': 0.015, 'joints': 3},\n            'pinky': {'length': 0.07, 'width': 0.012, 'joints': 3}\n        }\n\n        # Palm dimensions\n        self.palm_width = 0.08\n        self.palm_height = 0.06\n        self.palm_depth = 0.02\n\n    def synthesize_power_grasp(self, object_dims):\n        \"\"\"Synthesize power grasp for large objects\"\"\"\n        # Power grasp: object is held in the palm with fingers wrapped around\n        grasp_config = {\n            'type': 'power',\n            'thumb_pos': None,\n            'finger_positions': [],\n            'finger_angles': [],\n            'preshape': 'wide_open'\n        }\n\n        # Calculate thumb position (opposes fingers)\n        object_width, object_height, object_depth = object_dims\n\n        # Thumb position on one side of object\n        thumb_x = -object_width/2 - 0.01  # 1cm away from object\n        thumb_y = 0\n        thumb_z = object_height/2  # Middle height\n\n        grasp_config['thumb_pos'] = [thumb_x, thumb_y, thumb_z]\n\n        # Finger positions around the object\n        finger_positions = []\n        finger_angles = []\n\n        # Distribute fingers around object circumference\n        num_fingers = 4  # Index, middle, ring, pinky\n        for i in range(num_fingers):\n            angle = 2 * pi * i / num_fingers\n            finger_x = object_width/2 + 0.01  # 1cm away from object\n            finger_y = object_depth/2 * cos(angle)\n            finger_z = object_height/2 * sin(angle)\n\n            finger_positions.append([finger_x, finger_y, finger_z])\n\n            # Finger angles (typically flexed for power grasp)\n            finger_angles.append([1.2, 1.5, 0.8])  # Joint angles in radians\n\n        grasp_config['finger_positions'] = finger_positions\n        grasp_config['finger_angles'] = finger_angles\n\n        return grasp_config\n\n    def synthesize_precision_grasp(self, object_dims):\n        \"\"\"Synthesize precision grasp for small objects\"\"\"\n        # Precision grasp: object is held between thumb and fingertips\n        grasp_config = {\n            'type': 'precision',\n            'thumb_pos': None,\n            'finger_positions': [],\n            'finger_angles': [],\n            'preshape': 'close_together'\n        }\n\n        object_width, object_height, object_depth = object_dims\n\n        # Thumb opposes the finger\n        thumb_x = -object_width/2 - 0.01\n        thumb_y = -0.02  # Slightly offset\n        thumb_z = object_height/2\n\n        grasp_config['thumb_pos'] = [thumb_x, thumb_y, thumb_z]\n\n        # Use index finger for precision grasp\n        index_x = object_width/2 + 0.01\n        index_y = 0.02  # Opposite to thumb\n        index_z = object_height/2\n\n        grasp_config['finger_positions'] = [[index_x, index_y, index_z]]\n        grasp_config['finger_angles'] = [[0.2, 0.3, 0.1]]  # Less flexed for precision\n\n        return grasp_config\n\n    def synthesize_lateral_grasp(self, object_dims):\n        \"\"\"Synthesize lateral grasp (thumb-finger side grasp)\"\"\"\n        grasp_config = {\n            'type': 'lateral',\n            'thumb_pos': None,\n            'finger_positions': [],\n            'finger_angles': [],\n            'preshape': 'lateral'\n        }\n\n        object_width, object_height, object_depth = object_dims\n\n        # Thumb on one side of object\n        thumb_x = -object_width/2 - 0.01\n        thumb_y = 0\n        thumb_z = object_height/2\n\n        grasp_config['thumb_pos'] = [thumb_x, thumb_y, thumb_z]\n\n        # Index finger on opposite side, aligned with thumb\n        index_x = object_width/2 + 0.01\n        index_y = 0\n        index_z = object_height/2\n\n        grasp_config['finger_positions'] = [[index_x, index_y, index_z]]\n        grasp_config['finger_angles'] = [[0.5, 0.8, 0.3]]  # Moderate flexion\n\n        return grasp_config\n\n    def optimize_grasp_configuration(self, grasp_config, object_properties):\n        \"\"\"Optimize grasp configuration based on object properties\"\"\"\n        # Calculate grasp stability metrics\n        stability_score = self.evaluate_grasp_stability(grasp_config, object_properties)\n\n        # Adjust grasp based on object weight and friction\n        object_weight = object_properties.get('weight', 0.1)  # kg\n        object_friction = object_properties.get('friction', 0.5)\n\n        # Increase grip force for heavier objects\n        if object_weight > 0.5:\n            # Adjust finger angles for stronger grip\n            for i, angles in enumerate(grasp_config['finger_angles']):\n                # Increase flexion for stronger grip\n                adjusted_angles = [min(angle + 0.2, 2.0) for angle in angles]\n                grasp_config['finger_angles'][i] = adjusted_angles\n\n        # Adjust for low friction surfaces\n        if object_friction < 0.3:\n            # Use more contact points\n            if grasp_config['type'] == 'precision':\n                # Convert to 3-finger grasp if possible\n                grasp_config = self.convert_to_three_finger_grasp(grasp_config)\n\n        grasp_config['stability_score'] = stability_score\n        return grasp_config\n\n    def evaluate_grasp_stability(self, grasp_config, object_properties):\n        \"\"\"Evaluate the stability of a grasp configuration\"\"\"\n        # Simplified stability evaluation\n        # In practice, this would involve complex physics simulations\n\n        stability_score = 0.5  # Base score\n\n        # Consider number of contact points\n        num_contacts = len(grasp_config['finger_positions']) + 1  # +1 for thumb\n        if num_contacts >= 3:\n            stability_score += 0.2\n\n        # Consider object dimensions vs hand dimensions\n        object_dims = object_properties.get('dimensions', [0.05, 0.05, 0.05])\n        object_size = sum(object_dims)\n\n        if grasp_config['type'] == 'power' and object_size > 0.1:\n            stability_score += 0.2\n        elif grasp_config['type'] == 'precision' and object_size < 0.05:\n            stability_score += 0.2\n\n        # Consider grasp type appropriateness\n        if object_size < 0.03 and grasp_config['type'] != 'precision':\n            stability_score -= 0.2  # Precision grasp more appropriate for small objects\n        elif object_size > 0.1 and grasp_config['type'] == 'precision':\n            stability_score -= 0.2  # Power grasp more appropriate for large objects\n\n        return min(stability_score, 1.0)\n\n    def convert_to_three_finger_grasp(self, grasp_config):\n        \"\"\"Convert precision grasp to three-finger grasp for better stability\"\"\"\n        if grasp_config['type'] != 'precision':\n            return grasp_config\n\n        # Add middle and ring fingers to support the grasp\n        original_finger_pos = grasp_config['finger_positions'][0]\n\n        # Position additional fingers to provide support\n        support_fingers = []\n        for i in range(2):  # Add 2 more fingers\n            # Offset the support fingers slightly\n            offset = (i + 1) * 0.01  # 1cm offset\n            support_pos = [\n                original_finger_pos[0],\n                original_finger_pos[1] + offset,\n                original_finger_pos[2]\n            ]\n            support_fingers.append(support_pos)\n\n        grasp_config['finger_positions'].extend(support_fingers)\n        grasp_config['finger_angles'].extend([[0.3, 0.5, 0.2]] * 2)\n\n        return grasp_config\n\n    def generate_grasp_sequence(self, object_trajectory, initial_grasp):\n        \"\"\"Generate a sequence of grasp configurations for manipulation\"\"\"\n        grasp_sequence = [initial_grasp]\n\n        # Generate intermediate grasps for repositioning\n        for i in range(len(object_trajectory) - 1):\n            current_pos = object_trajectory[i]\n            next_pos = object_trajectory[i + 1]\n\n            # Calculate required hand repositioning\n            pos_change = np.array(next_pos) - np.array(current_pos)\n\n            # Generate intermediate grasp configuration\n            intermediate_grasp = self.generate_intermediate_grasp(\n                grasp_sequence[-1], pos_change\n            )\n\n            grasp_sequence.append(intermediate_grasp)\n\n        return grasp_sequence\n\n    def generate_intermediate_grasp(self, current_grasp, position_change):\n        \"\"\"Generate intermediate grasp configuration\"\"\"\n        # Copy current grasp and adjust for position change\n        new_grasp = current_grasp.copy()\n\n        # Adjust thumb position\n        if new_grasp['thumb_pos'] is not None:\n            new_grasp['thumb_pos'] = [\n                new_grasp['thumb_pos'][0] + position_change[0],\n                new_grasp['thumb_pos'][1] + position_change[1],\n                new_grasp['thumb_pos'][2] + position_change[2]\n            ]\n\n        # Adjust finger positions\n        for i, pos in enumerate(new_grasp['finger_positions']):\n            new_grasp['finger_positions'][i] = [\n                pos[0] + position_change[0],\n                pos[1] + position_change[1],\n                pos[2] + position_change[2]\n            ]\n\n        return new_grasp\n\nclass ManipulationController:\n    \"\"\"Controller for executing manipulation tasks\"\"\"\n    def __init__(self):\n        self.grasp_synthesizer = MultiFingerGraspSynthesizer()\n        self.current_grasp = None\n        self.object_in_hand = None\n\n    def approach_object(self, object_pose, grasp_config):\n        \"\"\"Generate approach trajectory to object\"\"\"\n        # Calculate approach direction (opposite to grasp direction)\n        approach_direction = self.calculate_approach_direction(grasp_config)\n\n        # Generate trajectory points\n        trajectory = []\n        current_pos = object_pose['position']\n\n        # Move to approach position (10cm away from grasp point)\n        approach_pos = current_pos - approach_direction * 0.1\n\n        # Linear approach trajectory\n        for t in np.linspace(0, 1, 20):  # 20 intermediate points\n            pos = approach_pos + t * (current_pos - approach_pos)\n            trajectory.append(pos)\n\n        return trajectory\n\n    def calculate_approach_direction(self, grasp_config):\n        \"\"\"Calculate approach direction based on grasp configuration\"\"\"\n        if grasp_config['type'] == 'power':\n            # Approach from the side where fingers will wrap around\n            return np.array([1, 0, 0])  # From positive X direction\n        elif grasp_config['type'] == 'precision':\n            # Approach from above or side\n            return np.array([0, 0, -1])  # From above\n        elif grasp_config['type'] == 'lateral':\n            # Approach from the side\n            return np.array([0, 1, 0])  # From positive Y direction\n        else:\n            return np.array([1, 0, 0])  # Default approach\n\n    def execute_grasp(self, grasp_config):\n        \"\"\"Execute the grasp motion\"\"\"\n        # Move fingers to grasp configuration\n        print(f\"Executing {grasp_config['type']} grasp\")\n\n        # Simulate finger movements\n        for i, (pos, angles) in enumerate(zip(grasp_config['finger_positions'],\n                                            grasp_config['finger_angles'])):\n            print(f\"Moving finger {i+1} to position {pos} with angles {angles}\")\n\n        # Close thumb\n        if grasp_config['thumb_pos']:\n            print(f\"Moving thumb to position {grasp_config['thumb_pos']}\")\n\n        # Wait for grasp completion\n        print(\"Grasp completed\")\n\n    def verify_grasp_success(self):\n        \"\"\"Verify that the grasp was successful\"\"\"\n        # In practice, this would check force sensors, tactile sensors, etc.\n        # For simulation, assume grasp succeeds with 90% probability\n        success_probability = 0.9\n        return np.random.random() < success_probability\n\n    def lift_object(self, height=0.1):\n        \"\"\"Lift the grasped object\"\"\"\n        print(f\"Lifting object by {height} meters\")\n        # Simulate lifting motion\n        return True\n\n    def transport_object(self, target_pose):\n        \"\"\"Transport object to target pose\"\"\"\n        print(f\"Transporting object to {target_pose}\")\n        # Simulate transport motion\n        return True\n\n    def release_object(self):\n        \"\"\"Release the grasped object\"\"\"\n        print(\"Releasing object\")\n        self.object_in_hand = None\n        return True\n\n# Example usage\ndef example_dexterous_manipulation():\n    synthesizer = MultiFingerGraspSynthesizer()\n    controller = ManipulationController()\n\n    # Object dimensions (width, height, depth in meters)\n    object_dims = [0.05, 0.05, 0.03]  # Small box\n    object_properties = {\n        'dimensions': object_dims,\n        'weight': 0.2,  # kg\n        'friction': 0.6\n    }\n\n    # Synthesize appropriate grasp\n    if max(object_dims) < 0.04:\n        grasp_config = synthesizer.synthesize_precision_grasp(object_dims)\n    else:\n        grasp_config = synthesizer.synthesize_power_grasp(object_dims)\n\n    # Optimize the grasp\n    optimized_grasp = synthesizer.optimize_grasp_configuration(grasp_config, object_properties)\n\n    print(f\"Synthesized {optimized_grasp['type']} grasp\")\n    print(f\"Stability score: {optimized_grasp.get('stability_score', 0):.2f}\")\n    print(f\"Thumb position: {optimized_grasp['thumb_pos']}\")\n    print(f\"Finger positions: {optimized_grasp['finger_positions']}\")\n\n    # Execute the manipulation\n    controller.execute_grasp(optimized_grasp)\n\n    if controller.verify_grasp_success():\n        print(\"Grasp successful!\")\n        controller.lift_object(0.1)\n        controller.transport_object([0.5, 0.5, 0.2])\n        controller.release_object()\n    else:\n        print(\"Grasp failed!\")\n\nif __name__ == \"__main__\":\n    example_dexterous_manipulation()\n"})}),"\n",(0,i.jsx)(e.h2,{id:"human-robot-interaction-design",children:"Human-Robot Interaction Design"}),"\n",(0,i.jsx)(e.h3,{id:"multi-modal-interaction-for-manipulation",children:"Multi-Modal Interaction for Manipulation"}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-python",children:"# human_robot_interaction.py\nimport numpy as np\nimport speech_recognition as sr\nimport cv2\nfrom queue import Queue\nimport threading\nimport time\n\nclass MultiModalInteractionManager:\n    def __init__(self):\n        self.speech_recognizer = sr.Recognizer()\n        self.microphone = sr.Microphone()\n\n        # Interaction state\n        self.current_task = None\n        self.user_intent = None\n        self.robot_feedback = None\n\n        # Modalities queue\n        self.modality_queue = Queue()\n\n        # Gesture recognition (simplified)\n        self.gesture_recognizer = GestureRecognizer()\n\n        # Shared state for coordination\n        self.shared_state = {\n            'user_attention': True,\n            'task_progress': 0.0,\n            'safety_status': 'safe',\n            'communication_mode': 'active'\n        }\n\n    def start_interaction_loop(self):\n        \"\"\"Start the main interaction loop\"\"\"\n        # Start threads for different modalities\n        speech_thread = threading.Thread(target=self.listen_for_speech)\n        vision_thread = threading.Thread(target=self.process_vision_input)\n        gesture_thread = threading.Thread(target=self.process_gestures)\n\n        speech_thread.daemon = True\n        vision_thread.daemon = True\n        gesture_thread.daemon = True\n\n        speech_thread.start()\n        vision_thread.start()\n        gesture_thread.start()\n\n        # Main interaction loop\n        while True:\n            if not self.modality_queue.empty():\n                modality_input = self.modality_queue.get()\n                self.process_multimodal_input(modality_input)\n\n            # Update shared state\n            self.update_shared_state()\n\n            time.sleep(0.1)  # 10Hz update rate\n\n    def listen_for_speech(self):\n        \"\"\"Continuously listen for speech commands\"\"\"\n        with self.microphone as source:\n            self.speech_recognizer.adjust_for_ambient_noise(source)\n\n        while True:\n            try:\n                with self.microphone as source:\n                    audio = self.speech_recognizer.listen(source, timeout=1)\n\n                # Recognize speech\n                text = self.speech_recognizer.recognize_google(audio)\n                print(f\"Recognized speech: {text}\")\n\n                # Add to queue\n                self.modality_queue.put({\n                    'modality': 'speech',\n                    'content': text,\n                    'timestamp': time.time()\n                })\n\n            except sr.WaitTimeoutError:\n                pass  # Continue listening\n            except sr.UnknownValueError:\n                print(\"Could not understand audio\")\n            except sr.RequestError as e:\n                print(f\"Speech recognition error: {e}\")\n\n            time.sleep(0.1)\n\n    def process_vision_input(self):\n        \"\"\"Process visual input from camera\"\"\"\n        cap = cv2.VideoCapture(0)\n\n        while True:\n            ret, frame = cap.read()\n            if not ret:\n                continue\n\n            # Process frame for relevant information\n            objects = self.detect_objects(frame)\n            user_attention = self.detect_user_attention(frame)\n\n            # Add to queue\n            self.modality_queue.put({\n                'modality': 'vision',\n                'objects': objects,\n                'user_attention': user_attention,\n                'frame': frame,\n                'timestamp': time.time()\n            })\n\n            time.sleep(0.05)  # 20Hz for vision\n\n    def process_gestures(self):\n        \"\"\"Process hand gestures for interaction\"\"\"\n        cap = cv2.VideoCapture(1)  # Second camera for gesture\n\n        while True:\n            ret, frame = cap.read()\n            if not ret:\n                continue\n\n            # Recognize gestures\n            gesture = self.gesture_recognizer.recognize_gesture(frame)\n\n            if gesture and gesture != 'none':\n                self.modality_queue.put({\n                    'modality': 'gesture',\n                    'gesture': gesture,\n                    'timestamp': time.time()\n                })\n\n            time.sleep(0.05)  # 20Hz for gestures\n\n    def detect_objects(self, frame):\n        \"\"\"Detect objects in the camera frame\"\"\"\n        # Simplified object detection\n        # In practice, this would use deep learning models\n        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n\n        # Find contours (simplified object detection)\n        contours, _ = cv2.findContours(gray, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n\n        objects = []\n        for contour in contours:\n            if cv2.contourArea(contour) > 500:  # Filter small contours\n                x, y, w, h = cv2.boundingRect(contour)\n                objects.append({\n                    'bbox': [x, y, w, h],\n                    'center': [x + w//2, y + h//2],\n                    'area': cv2.contourArea(contour)\n                })\n\n        return objects\n\n    def detect_user_attention(self, frame):\n        \"\"\"Detect if user is paying attention to robot\"\"\"\n        # Simplified attention detection\n        # In practice, this would use face detection/gaze estimation\n        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n        # Use face detection as proxy for attention\n        face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n        faces = face_cascade.detectMultiScale(gray, 1.1, 4)\n\n        return len(faces) > 0  # User is attending if face is detected\n\n    def process_multimodal_input(self, modality_input):\n        \"\"\"Process input from any modality\"\"\"\n        modality = modality_input['modality']\n\n        if modality == 'speech':\n            self.process_speech_command(modality_input['content'])\n        elif modality == 'vision':\n            self.process_vision_data(modality_input)\n        elif modality == 'gesture':\n            self.process_gesture_command(modality_input['gesture'])\n\n    def process_speech_command(self, command):\n        \"\"\"Process speech commands for manipulation\"\"\"\n        command_lower = command.lower()\n\n        # Parse manipulation commands\n        if 'grasp' in command_lower or 'pick up' in command_lower:\n            # Extract object reference\n            object_ref = self.extract_object_reference(command)\n            self.initiate_grasp_task(object_ref)\n\n        elif 'move' in command_lower or 'place' in command_lower or 'put' in command_lower:\n            target_location = self.extract_location_reference(command)\n            self.initiate_place_task(target_location)\n\n        elif 'release' in command_lower or 'let go' in command_lower:\n            self.initiate_release_task()\n\n        elif 'stop' in command_lower or 'abort' in command_lower:\n            self.abort_current_task()\n\n        else:\n            # Check for confirmation requests\n            if 'yes' in command_lower or 'sure' in command_lower:\n                self.confirm_pending_action()\n            elif 'no' in command_lower or 'cancel' in command_lower:\n                self.cancel_pending_action()\n\n    def extract_object_reference(self, command):\n        \"\"\"Extract object reference from speech command\"\"\"\n        # Simple keyword-based extraction\n        keywords = ['box', 'cup', 'bottle', 'book', 'object', 'item']\n        for keyword in keywords:\n            if keyword in command.lower():\n                return keyword\n        return 'object'  # Default\n\n    def extract_location_reference(self, command):\n        \"\"\"Extract location reference from speech command\"\"\"\n        # Simple keyword-based extraction\n        if 'table' in command.lower():\n            return 'table'\n        elif 'shelf' in command.lower():\n            return 'shelf'\n        elif 'box' in command.lower():\n            return 'box'\n        else:\n            return 'default'\n\n    def initiate_grasp_task(self, object_ref):\n        \"\"\"Initiate grasp task based on object reference\"\"\"\n        print(f\"Initiating grasp task for {object_ref}\")\n\n        # Find object in the environment\n        target_object = self.find_object_by_reference(object_ref)\n\n        if target_object:\n            self.current_task = {\n                'type': 'grasp',\n                'target_object': target_object,\n                'status': 'planning'\n            }\n            print(f\"Found target object: {target_object}\")\n        else:\n            self.request_object_location(command=f\"Where is the {object_ref}?\")\n\n    def find_object_by_reference(self, object_ref):\n        \"\"\"Find object in environment by reference\"\"\"\n        # This would interface with perception system\n        # For simulation, return a mock object\n        return {\n            'type': object_ref,\n            'position': [0.5, 0.3, 0.1],\n            'dimensions': [0.05, 0.05, 0.05],\n            'graspable': True\n        }\n\n    def request_object_location(self, command):\n        \"\"\"Request user to indicate object location\"\"\"\n        self.speak_response(f\"Could you please show me the {command.split()[-1][:-1]}?\")\n        self.shared_state['user_attention'] = True\n        self.shared_state['communication_mode'] = 'requesting'\n\n    def process_vision_data(self, vision_data):\n        \"\"\"Process vision data for interaction\"\"\"\n        if 'user_attention' in vision_data:\n            self.shared_state['user_attention'] = vision_data['user_attention']\n\n        if self.shared_state['communication_mode'] == 'requesting':\n            # User might be pointing to an object\n            self.check_for_pointing_gesture(vision_data['frame'])\n\n    def check_for_pointing_gesture(self, frame):\n        \"\"\"Check if user is pointing to indicate object location\"\"\"\n        # Simplified pointing detection\n        # In practice, this would use pose estimation\n        pointing_detected = self.gesture_recognizer.detect_pointing(frame)\n\n        if pointing_detected:\n            # Extract object at pointed location\n            pointed_object = self.get_object_at_location(pointing_detected['location'])\n            if pointed_object:\n                self.current_task = {\n                    'type': 'grasp',\n                    'target_object': pointed_object,\n                    'status': 'confirmed'\n                }\n                self.speak_response(\"I see it, grasping now.\")\n                self.shared_state['communication_mode'] = 'active'\n\n    def get_object_at_location(self, location):\n        \"\"\"Get object at a specific location in the environment\"\"\"\n        # This would query the environment model\n        return {\n            'type': 'object',\n            'position': [location[0]/100, location[1]/100, 0.1],  # Convert pixel to meters\n            'dimensions': [0.05, 0.05, 0.05],\n            'graspable': True\n        }\n\n    def process_gesture_command(self, gesture):\n        \"\"\"Process gesture commands\"\"\"\n        if gesture == 'thumbs_up':\n            self.confirm_pending_action()\n        elif gesture == 'stop_hand':\n            self.abort_current_task()\n        elif gesture == 'pointing':\n            # User is indicating something\n            self.shared_state['communication_mode'] = 'requesting'\n        elif gesture == 'wave':\n            self.speak_response(\"Hello! How can I help you?\")\n\n    def speak_response(self, text):\n        \"\"\"Generate speech response (simulated)\"\"\"\n        print(f\"Robot says: {text}\")\n        # In practice, this would use text-to-speech\n\n    def update_shared_state(self):\n        \"\"\"Update the shared interaction state\"\"\"\n        # Update based on various factors\n        if not self.shared_state['user_attention']:\n            self.shared_state['communication_mode'] = 'passive'\n        else:\n            self.shared_state['communication_mode'] = 'active'\n\nclass GestureRecognizer:\n    \"\"\"Simple gesture recognizer\"\"\"\n    def __init__(self):\n        self.reference_gestures = self.load_reference_gestures()\n\n    def load_reference_gestures(self):\n        \"\"\"Load reference gestures (simplified)\"\"\"\n        return {\n            'thumbs_up': [(100, 100), (120, 120)],  # Simplified\n            'stop_hand': [(100, 150), (120, 150)],\n            'pointing': [(100, 200), (120, 180)],\n            'wave': [(100, 250), (120, 250)]\n        }\n\n    def recognize_gesture(self, frame):\n        \"\"\"Recognize gesture from frame\"\"\"\n        # Simplified gesture recognition\n        # In practice, this would use hand pose estimation\n        height, width = frame.shape[:2]\n\n        # Mock recognition based on simple features\n        if width > height:  # Landscape orientation\n            return 'wave'  # Mock detection\n        else:\n            return 'none'\n\n    def detect_pointing(self, frame):\n        \"\"\"Detect pointing gesture\"\"\"\n        # Simplified pointing detection\n        height, width = frame.shape[:2]\n\n        # Mock pointing detection\n        if width > 640:  # If there's a distinct feature\n            return {'location': (width//2, height//2)}\n        return None\n\nclass ManipulationTaskExecutor:\n    \"\"\"Execute manipulation tasks with human-robot interaction\"\"\"\n    def __init__(self):\n        self.interaction_manager = MultiModalInteractionManager()\n        self.grasp_planner = MultiFingerGraspSynthesizer()\n        self.manipulation_controller = ManipulationController()\n\n    def execute_grasp_with_interaction(self, object_info):\n        \"\"\"Execute grasp task with interaction feedback\"\"\"\n        print(\"Starting interactive grasp task...\")\n\n        # Plan the grasp\n        grasp_config = self.grasp_planner.synthesize_power_grasp(object_info['dimensions'])\n        optimized_grasp = self.grasp_planner.optimize_grasp_configuration(\n            grasp_config,\n            {'dimensions': object_info['dimensions'], 'weight': 0.1, 'friction': 0.5}\n        )\n\n        # Inform user about the plan\n        self.interaction_manager.speak_response(\n            f\"I plan to grasp the {object_info['type']} with a power grasp. Is this OK?\"\n        )\n\n        # Wait for user confirmation (simplified)\n        time.sleep(2)  # Simulate waiting for response\n\n        # Execute the grasp\n        success = self.execute_grasp_safely(optimized_grasp)\n\n        if success:\n            self.interaction_manager.speak_response(\"Successfully grasped the object!\")\n        else:\n            self.interaction_manager.speak_response(\"Grasp failed. Would you like me to try again?\")\n\n        return success\n\n    def execute_grasp_safely(self, grasp_config):\n        \"\"\"Execute grasp with safety checks\"\"\"\n        try:\n            self.manipulation_controller.execute_grasp(grasp_config)\n            success = self.manipulation_controller.verify_grasp_success()\n            return success\n        except Exception as e:\n            print(f\"Grasp execution error: {e}\")\n            return False\n\n# Example usage\ndef example_human_robot_interaction():\n    executor = ManipulationTaskExecutor()\n\n    # Simulate an object to grasp\n    object_info = {\n        'type': 'bottle',\n        'position': [0.5, 0.3, 0.1],\n        'dimensions': [0.05, 0.15, 0.05],  # width, height, depth\n        'graspable': True\n    }\n\n    print(\"Starting interactive manipulation task...\")\n    success = executor.execute_grasp_with_interaction(object_info)\n\n    if success:\n        print(\"Task completed successfully!\")\n    else:\n        print(\"Task failed.\")\n\nif __name__ == \"__main__\":\n    try:\n        example_human_robot_interaction()\n    except ImportError as e:\n        print(f\"Missing dependency: {e}\")\n        print(\"Please install required packages: speech_recognition, opencv-python\")\n"})}),"\n",(0,i.jsx)(e.h2,{id:"advanced-manipulation-strategies",children:"Advanced Manipulation Strategies"}),"\n",(0,i.jsx)(e.h3,{id:"dual-arm-coordination",children:"Dual-Arm Coordination"}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-python",children:"# dual_arm_manipulation.py\nimport numpy as np\nfrom scipy.spatial.transform import Rotation as R\n\nclass DualArmCoordinator:\n    def __init__(self):\n        # Robot arm parameters\n        self.left_arm_base = np.array([0.2, 0.3, 0.0])\n        self.right_arm_base = np.array([0.2, -0.3, 0.0])\n        self.arm_reach = 0.8  # meters\n\n    def coordinate_dual_arm_grasp(self, object_pose, object_dims):\n        \"\"\"Coordinate dual-arm grasp for large objects\"\"\"\n        object_pos = object_pose['position']\n        object_size = object_dims\n\n        # Determine appropriate grasp points for each arm\n        grasp_points = self.calculate_dual_arm_grasp_points(object_pos, object_size)\n\n        left_grasp = grasp_points['left']\n        right_grasp = grasp_points['right']\n\n        # Generate trajectories for both arms\n        left_trajectory = self.generate_reach_trajectory(\n            self.left_arm_base, left_grasp['position'], left_grasp['orientation']\n        )\n\n        right_trajectory = self.generate_reach_trajectory(\n            self.right_arm_base, right_grasp['position'], right_grasp['orientation']\n        )\n\n        # Synchronize the trajectories\n        synchronized_trajectories = self.synchronize_trajectories(\n            left_trajectory, right_trajectory\n        )\n\n        return synchronized_trajectories\n\n    def calculate_dual_arm_grasp_points(self, object_pos, object_size):\n        \"\"\"Calculate appropriate grasp points for dual arms\"\"\"\n        width, height, depth = object_size\n\n        # Calculate grasp points on opposite sides of the object\n        left_grasp_pos = object_pos + np.array([-width/2 - 0.05, 0, 0])  # 5cm offset\n        right_grasp_pos = object_pos + np.array([width/2 + 0.05, 0, 0])  # 5cm offset\n\n        # Orient grasps to oppose each other\n        left_orientation = R.from_euler('xyz', [0, 0, np.pi/2]).as_matrix()  # Point inward\n        right_orientation = R.from_euler('xyz', [0, 0, -np.pi/2]).as_matrix()  # Point inward\n\n        return {\n            'left': {\n                'position': left_grasp_pos,\n                'orientation': left_orientation\n            },\n            'right': {\n                'position': right_grasp_pos,\n                'orientation': right_orientation\n            }\n        }\n\n    def generate_reach_trajectory(self, start_pos, end_pos, end_orientation, steps=50):\n        \"\"\"Generate reaching trajectory for a single arm\"\"\"\n        trajectory = []\n\n        for i in range(steps + 1):\n            t = i / steps  # Interpolation parameter\n\n            # Linear interpolation for position\n            pos = start_pos + t * (end_pos - start_pos)\n\n            # Slerp for orientation (simplified as linear interpolation)\n            orientation = self.interpolate_orientations(\n                np.eye(3), end_orientation, t\n            )\n\n            trajectory.append({\n                'position': pos,\n                'orientation': orientation,\n                'time': t\n            })\n\n        return trajectory\n\n    def interpolate_orientations(self, start_rot, end_rot, t):\n        \"\"\"Interpolate between two orientations\"\"\"\n        # Convert to quaternions for proper interpolation\n        start_quat = R.from_matrix(start_rot).as_quat()\n        end_quat = R.from_matrix(end_rot).as_quat()\n\n        # Slerp (simplified as linear interpolation)\n        interpolated_quat = (1 - t) * start_quat + t * end_quat\n        interpolated_quat = interpolated_quat / np.linalg.norm(interpolated_quat)\n\n        return R.from_quat(interpolated_quat).as_matrix()\n\n    def synchronize_trajectories(self, left_traj, right_traj):\n        \"\"\"Synchronize trajectories of both arms\"\"\"\n        # Ensure both trajectories have the same number of steps\n        min_steps = min(len(left_traj), len(right_traj))\n\n        synchronized = {\n            'left': left_traj[:min_steps],\n            'right': right_traj[:min_steps]\n        }\n\n        # Add coordination information\n        for i in range(min_steps):\n            # Calculate distance between hands\n            dist = np.linalg.norm(\n                synchronized['left'][i]['position'] -\n                synchronized['right'][i]['position']\n            )\n\n            synchronized['left'][i]['hand_distance'] = dist\n            synchronized['right'][i]['hand_distance'] = dist\n\n        return synchronized\n\n    def execute_coordinated_task(self, task_type, object_info):\n        \"\"\"Execute coordinated dual-arm task\"\"\"\n        if task_type == 'lift_large_object':\n            return self.lift_large_object(object_info)\n        elif task_type == 'assemble_parts':\n            return self.assemble_parts(object_info)\n        elif task_type == 'open_container':\n            return self.open_container(object_info)\n        else:\n            raise ValueError(f\"Unknown task type: {task_type}\")\n\n    def lift_large_object(self, object_info):\n        \"\"\"Lift large object using coordinated dual arms\"\"\"\n        print(\"Coordinated lifting of large object...\")\n\n        # Plan dual-arm grasp\n        trajectories = self.coordinate_dual_arm_grasp(\n            object_info['pose'],\n            object_info['dimensions']\n        )\n\n        # Execute coordinated lift\n        lift_height = object_info['pose']['position'][2] + 0.2  # Lift 20cm\n\n        # Add lift motion to trajectories\n        for i in range(len(trajectories['left'])):\n            trajectories['left'][i]['position'][2] += (i / len(trajectories['left'])) * 0.2\n            trajectories['right'][i]['position'][2] += (i / len(trajectories['right'])) * 0.2\n\n        print(\"Large object lifted successfully with dual-arm coordination\")\n        return True\n\n    def assemble_parts(self, parts_info):\n        \"\"\"Assemble parts using dual-arm coordination\"\"\"\n        print(\"Assembling parts with dual arms...\")\n\n        # Left arm holds one part, right arm manipulates the other\n        left_arm_task = {\n            'action': 'hold',\n            'object': parts_info['part1'],\n            'grasp_type': 'power'\n        }\n\n        right_arm_task = {\n            'action': 'manipulate',\n            'object': parts_info['part2'],\n            'target_pose': parts_info['assembly_pose'],\n            'motion_type': 'insertion'\n        }\n\n        # Execute coordinated assembly\n        print(\"Parts assembled successfully\")\n        return True\n\nclass BimanualManipulationPlanner:\n    \"\"\"Plan bimanual manipulation tasks\"\"\"\n    def __init__(self):\n        self.dual_arm_coordinator = DualArmCoordinator()\n\n    def plan_bimanual_task(self, task_description):\n        \"\"\"Plan a bimanual manipulation task\"\"\"\n        task_type = task_description['type']\n\n        if task_type == 'pouring':\n            return self.plan_pouring_task(task_description)\n        elif task_type == 'opening':\n            return self.plan_opening_task(task_description)\n        elif task_type == 'supporting':\n            return self.plan_supporting_task(task_description)\n        else:\n            raise ValueError(f\"Unknown bimanual task: {task_type}\")\n\n    def plan_pouring_task(self, task_description):\n        \"\"\"Plan bimanual pouring task\"\"\"\n        # Left arm holds container, right arm controls pouring\n        container_pose = task_description['container_pose']\n        target_pose = task_description['target_pose']\n\n        # Left arm: stable grasp of container\n        left_grasp = {\n            'position': container_pose['position'],\n            'orientation': self.calculate_container_grasp_orientation(),\n            'grasp_type': 'power'\n        }\n\n        # Right arm: pouring motion\n        pour_trajectory = self.generate_pouring_motion(\n            container_pose, target_pose\n        )\n\n        return {\n            'left_arm_task': {\n                'action': 'hold',\n                'grasp': left_grasp\n            },\n            'right_arm_task': {\n                'action': 'pour',\n                'trajectory': pour_trajectory\n            },\n            'coordination': 'synchronized'\n        }\n\n    def calculate_container_grasp_orientation(self):\n        \"\"\"Calculate appropriate grasp orientation for container\"\"\"\n        # Grasp container handle with thumb up\n        return R.from_euler('xyz', [0, 0, 0]).as_matrix()\n\n    def generate_pouring_motion(self, container_pose, target_pose):\n        \"\"\"Generate pouring motion trajectory\"\"\"\n        # Calculate motion from container to target\n        container_pos = container_pose['position']\n        target_pos = target_pose['position']\n\n        # Define key points for pouring motion\n        lift_point = container_pos + np.array([0, 0, 0.1])  # Lift slightly\n        pour_point = np.array([\n            target_pos[0], target_pos[1], container_pos[2]  # At target x,y but same height as container\n        ])\n        tilt_point = pour_point + np.array([0.1, 0, -0.05])  # Tilt forward\n\n        return [container_pos, lift_point, pour_point, tilt_point]\n\n    def plan_opening_task(self, task_description):\n        \"\"\"Plan bimanual opening task (e.g., opening a jar)\"\"\"\n        jar_pose = task_description['jar_pose']\n\n        # Left arm: stabilize the jar\n        left_grasp = {\n            'position': jar_pose['position'],\n            'orientation': self.calculate_stabilizing_grasp_orientation(),\n            'grasp_type': 'tripod'  # Three-finger grasp for stability\n        }\n\n        # Right arm: twist the lid\n        twist_trajectory = self.generate_twist_motion(jar_pose)\n\n        return {\n            'left_arm_task': {\n                'action': 'stabilize',\n                'grasp': left_grasp\n            },\n            'right_arm_task': {\n                'action': 'twist',\n                'trajectory': twist_trajectory\n            },\n            'coordination': 'force_balance'\n        }\n\n    def calculate_stabilizing_grasp_orientation(self):\n        \"\"\"Calculate orientation for stabilizing grasp\"\"\"\n        # Grasp the bottom of the jar for stability\n        return R.from_euler('xyz', [0, np.pi, 0]).as_matrix()\n\n    def generate_twist_motion(self, jar_pose):\n        \"\"\"Generate twisting motion for jar opening\"\"\"\n        # Circular motion around jar axis\n        jar_pos = jar_pose['position']\n        radius = 0.03  # 3cm from center\n\n        trajectory = []\n        for angle in np.linspace(0, 4*np.pi, 100):  # 2 full rotations\n            x = jar_pos[0] + radius * np.cos(angle)\n            y = jar_pos[1] + radius * np.sin(angle)\n            z = jar_pos[2] + 0.02 * np.sin(4*angle)  # Small vertical oscillation\n\n            trajectory.append(np.array([x, y, z]))\n\n        return trajectory\n\n# Example usage\ndef example_dual_arm_manipulation():\n    coordinator = DualArmCoordinator()\n    bimanual_planner = BimanualManipulationPlanner()\n\n    # Example 1: Lifting a large object\n    large_object = {\n        'pose': {'position': np.array([0.6, 0.0, 0.1])},\n        'dimensions': [0.3, 0.2, 0.1]  # width, height, depth\n    }\n\n    success = coordinator.lift_large_object(large_object)\n    print(f\"Large object lift success: {success}\")\n\n    # Example 2: Planning a bimanual task\n    pouring_task = {\n        'type': 'pouring',\n        'container_pose': {'position': np.array([0.5, 0.1, 0.2])},\n        'target_pose': {'position': np.array([0.7, 0.1, 0.15])}\n    }\n\n    pouring_plan = bimanual_planner.plan_pouring_task(pouring_task)\n    print(f\"Pouring task planned: {pouring_plan['coordination']}\")\n\nif __name__ == \"__main__\":\n    example_dual_arm_manipulation()\n"})}),"\n",(0,i.jsx)(e.h2,{id:"knowledge-check",children:"Knowledge Check"}),"\n",(0,i.jsxs)(e.ol,{children:["\n",(0,i.jsx)(e.li,{children:"What are the key differences between power grasps and precision grasps?"}),"\n",(0,i.jsx)(e.li,{children:"How does force closure contribute to grasp stability?"}),"\n",(0,i.jsx)(e.li,{children:"What are the advantages of multi-modal interaction in manipulation tasks?"}),"\n",(0,i.jsx)(e.li,{children:"How do dual-arm coordination strategies improve manipulation capabilities?"}),"\n"]}),"\n",(0,i.jsx)(e.h2,{id:"summary",children:"Summary"}),"\n",(0,i.jsx)(e.p,{children:"This chapter covered advanced manipulation and grasping techniques for humanoid robots. We explored geometric and learning-based grasp planning algorithms, dexterous manipulation strategies for multi-finger hands, and human-robot interaction design for manipulation tasks. The chapter also covered multi-modal interaction approaches and dual-arm coordination strategies for complex manipulation scenarios."}),"\n",(0,i.jsx)(e.h2,{id:"next-steps",children:"Next Steps"}),"\n",(0,i.jsx)(e.p,{children:"In the next chapter, we'll explore natural human-robot interaction, covering communication paradigms, user experience design, and advanced interaction techniques for humanoid robotics applications."})]})}function _(n={}){const{wrapper:e}={...(0,r.R)(),...n.components};return e?(0,i.jsx)(e,{...n,children:(0,i.jsx)(l,{...n})}):l(n)}},8453(n,e,t){t.d(e,{R:()=>o,x:()=>s});var i=t(6540);const r={},a=i.createContext(r);function o(n){const e=i.useContext(a);return i.useMemo(function(){return"function"==typeof n?n(e):{...e,...n}},[e,n])}function s(n){let e;return e=n.disableParentContext?"function"==typeof n.components?n.components(r):n.components||r:o(n.components),i.createElement(a.Provider,{value:e},n.children)}}}]);