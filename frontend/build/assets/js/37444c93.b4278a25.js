"use strict";(globalThis.webpackChunkphysical_ai_humanoid_robotics_book=globalThis.webpackChunkphysical_ai_humanoid_robotics_book||[]).push([[454],{3308(e,n,t){t.r(n),t.d(n,{assets:()=>l,contentTitle:()=>o,default:()=>u,frontMatter:()=>r,metadata:()=>a,toc:()=>c});var i=t(4848),s=t(8453);const r={sidebar_position:18,title:"Chapter 18: Speech Recognition and Natural Language Understanding"},o="Chapter 18: Speech Recognition and Natural Language Understanding",a={id:"part6/chapter18",title:"Chapter 18: Speech Recognition and Natural Language Understanding",description:"Learning Objectives",source:"@site/docs/part6/chapter18.md",sourceDirName:"part6",slug:"/part6/chapter18",permalink:"/Physical-AI-Humanoid-Robotics/docs/part6/chapter18",draft:!1,unlisted:!1,editUrl:"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/part6/chapter18.md",tags:[],version:"current",sidebarPosition:18,frontMatter:{sidebar_position:18,title:"Chapter 18: Speech Recognition and Natural Language Understanding"},sidebar:"tutorialSidebar",previous:{title:"Chapter 17: Integrating LLMs for Conversational AI in Robots",permalink:"/Physical-AI-Humanoid-Robotics/docs/part6/chapter17"},next:{title:"Chapter 19: Cognitive Planning with LLMs",permalink:"/Physical-AI-Humanoid-Robotics/docs/part6/chapter19"}},l={},c=[{value:"Learning Objectives",id:"learning-objectives",level:2},{value:"Introduction to Speech Recognition in Robotics",id:"introduction-to-speech-recognition-in-robotics",level:2},{value:"Key Challenges in Robot Speech Recognition",id:"key-challenges-in-robot-speech-recognition",level:3},{value:"Speech Recognition Architecture",id:"speech-recognition-architecture",level:3},{value:"Voice Command Processing Systems",id:"voice-command-processing-systems",level:2},{value:"Real-time Audio Processing",id:"real-time-audio-processing",level:3},{value:"Speech-to-Text Integration",id:"speech-to-text-integration",level:3},{value:"Natural Language Understanding",id:"natural-language-understanding",level:2},{value:"Intent Recognition and Entity Extraction",id:"intent-recognition-and-entity-extraction",level:3},{value:"Multi-Modal Interaction",id:"multi-modal-interaction",level:2},{value:"Combining Speech with Other Modalities",id:"combining-speech-with-other-modalities",level:3},{value:"Dialogue Management",id:"dialogue-management",level:2},{value:"Conversational Flow Control",id:"conversational-flow-control",level:3},{value:"Knowledge Check",id:"knowledge-check",level:2},{value:"Summary",id:"summary",level:2},{value:"Next Steps",id:"next-steps",level:2}];function d(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,s.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(n.h1,{id:"chapter-18-speech-recognition-and-natural-language-understanding",children:"Chapter 18: Speech Recognition and Natural Language Understanding"}),"\n",(0,i.jsx)(n.h2,{id:"learning-objectives",children:"Learning Objectives"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Implement voice command processing for humanoid robots"}),"\n",(0,i.jsx)(n.li,{children:"Master natural language understanding in robotics"}),"\n",(0,i.jsx)(n.li,{children:"Design multi-modal interaction systems"}),"\n",(0,i.jsx)(n.li,{children:"Create robust dialogue management for robot communication"}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"introduction-to-speech-recognition-in-robotics",children:"Introduction to Speech Recognition in Robotics"}),"\n",(0,i.jsx)(n.p,{children:"Speech recognition is a critical component of natural human-robot interaction, enabling robots to understand and respond to voice commands. For humanoid robots, speech recognition systems must handle the challenges of real-world environments, including background noise, multiple speakers, and varying acoustic conditions."}),"\n",(0,i.jsx)(n.h3,{id:"key-challenges-in-robot-speech-recognition",children:"Key Challenges in Robot Speech Recognition"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Acoustic Environment"}),": Robots operate in noisy, dynamic environments"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Real-time Processing"}),": Commands must be processed with minimal latency"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Speaker Variability"}),": Systems must handle different voices and accents"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Domain Specificity"}),": Robot commands often use specific terminology"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Robustness"}),": Systems must work reliably in various conditions"]}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"speech-recognition-architecture",children:"Speech Recognition Architecture"}),"\n",(0,i.jsx)(n.p,{children:"The typical speech recognition pipeline for robotics includes:"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Audio Input"}),": Microphone array for sound capture"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Preprocessing"}),": Noise reduction and signal enhancement"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Feature Extraction"}),": Converting audio to recognizable features"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Recognition"}),": Converting features to text"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Natural Language Understanding"}),": Interpreting the meaning"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Action Mapping"}),": Connecting to robot actions"]}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"voice-command-processing-systems",children:"Voice Command Processing Systems"}),"\n",(0,i.jsx)(n.h3,{id:"real-time-audio-processing",children:"Real-time Audio Processing"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'# audio_processing.py\nimport pyaudio\nimport numpy as np\nimport webrtcvad\nimport collections\nimport threading\nimport queue\nimport time\nfrom scipy import signal\nimport librosa\n\nclass AudioProcessor:\n    def __init__(self, sample_rate=16000, frame_duration_ms=30, num_channels=1):\n        self.sample_rate = sample_rate\n        self.frame_duration_ms = frame_duration_ms\n        self.num_channels = num_channels\n        self.frame_size = int(sample_rate * frame_duration_ms / 1000)\n\n        # Voice Activity Detection\n        self.vad = webrtcvad.Vad(2)  # Aggressiveness level 2\n\n        # Audio buffer\n        self.audio_buffer = collections.deque(maxlen=30)  # 30 frames = 900ms\n        self.is_speaking = False\n        self.speech_start_time = None\n        self.speech_end_time = None\n\n        # Noise reduction parameters\n        self.noise_threshold = 0.01\n        self.speech_threshold = 0.05\n\n        # Audio processing parameters\n        self.pre_emphasis = 0.97\n        self.frame_stride = 0.01  # 10ms stride\n\n    def record_audio_stream(self, callback, duration=None):\n        """Record audio stream with real-time processing"""\n        p = pyaudio.PyAudio()\n\n        stream = p.open(\n            format=pyaudio.paInt16,\n            channels=self.num_channels,\n            rate=self.sample_rate,\n            input=True,\n            frames_per_buffer=self.frame_size\n        )\n\n        print("Starting audio recording...")\n\n        start_time = time.time()\n        while True:\n            if duration and time.time() - start_time > duration:\n                break\n\n            # Read audio data\n            audio_data = stream.read(self.frame_size, exception_on_overflow=False)\n            audio_array = np.frombuffer(audio_data, dtype=np.int16).astype(np.float32) / 32768.0\n\n            # Add to buffer\n            self.audio_buffer.append(audio_array)\n\n            # Check for voice activity\n            is_voice_active = self.is_voice_active(audio_array)\n\n            if is_voice_active and not self.is_speaking:\n                # Speech started\n                self.is_speaking = True\n                self.speech_start_time = time.time()\n                print("Speech detected - started")\n\n            elif not is_voice_active and self.is_speaking:\n                # Speech ended\n                self.is_speaking = False\n                self.speech_end_time = time.time()\n                print("Speech ended")\n\n                # Process the collected speech segment\n                speech_segment = self.get_speech_segment()\n                if len(speech_segment) > 0:\n                    callback(speech_segment, self.speech_start_time, self.speech_end_time)\n\n            # Process with callback if continuously speaking\n            if self.is_speaking:\n                callback(audio_array, time.time(), None, partial=True)\n\n        stream.stop_stream()\n        stream.close()\n        p.terminate()\n\n    def is_voice_active(self, audio_frame):\n        """Check if voice is active in the audio frame using WebRTC VAD"""\n        # Convert to bytes for WebRTC VAD\n        audio_bytes = (audio_frame * 32767).astype(np.int16).tobytes()\n\n        # Check voice activity\n        try:\n            return self.vad.is_speech(audio_bytes, self.sample_rate)\n        except:\n            # Fallback: simple energy-based detection\n            energy = np.mean(np.abs(audio_frame))\n            return energy > self.speech_threshold\n\n    def get_speech_segment(self):\n        """Get the complete speech segment from the buffer"""\n        if not self.audio_buffer:\n            return np.array([])\n\n        # Concatenate buffered audio\n        speech_data = np.concatenate(list(self.audio_buffer))\n\n        # Apply preprocessing\n        processed_speech = self.preprocess_audio(speech_data)\n\n        return processed_speech\n\n    def preprocess_audio(self, audio_data):\n        """Apply preprocessing to audio data"""\n        # Apply pre-emphasis filter\n        emphasized_audio = self.pre_emphasis_filter(audio_data)\n\n        # Noise reduction\n        denoised_audio = self.reduce_noise(emphasized_audio)\n\n        # Normalize\n        normalized_audio = self.normalize_audio(denoised_audio)\n\n        return normalized_audio\n\n    def pre_emphasis_filter(self, audio_data, pre_emphasis=0.97):\n        """Apply pre-emphasis filter to enhance high frequencies"""\n        return np.append(audio_data[0], audio_data[1:] - pre_emphasis * audio_data[:-1])\n\n    def reduce_noise(self, audio_data):\n        """Simple noise reduction using spectral subtraction"""\n        # For simplicity, using a basic noise reduction approach\n        # In practice, more sophisticated methods would be used\n\n        # Estimate noise profile from beginning of audio\n        noise_profile = np.mean(np.abs(audio_data[:int(self.sample_rate * 0.1)]))  # First 100ms\n\n        # Apply soft thresholding\n        threshold = max(self.noise_threshold, noise_profile * 0.5)\n\n        # Soft thresholding\n        denoised = np.sign(audio_data) * np.maximum(np.abs(audio_data) - threshold, 0)\n\n        return denoised\n\n    def normalize_audio(self, audio_data):\n        """Normalize audio to consistent level"""\n        if len(audio_data) == 0:\n            return audio_data\n\n        # RMS normalization\n        rms = np.sqrt(np.mean(audio_data ** 2))\n        target_rms = 0.1  # Target RMS level\n\n        if rms > 0:\n            gain = target_rms / rms\n            normalized = audio_data * gain\n            # Clip to prevent overflow\n            return np.clip(normalized, -1.0, 1.0)\n        else:\n            return audio_data\n\n    def detect_wake_word(self, audio_data, wake_word_model=None):\n        """Detect wake word in audio stream"""\n        # Simple energy-based wake word detection\n        # In practice, this would use a trained wake word model\n\n        energy = np.mean(np.abs(audio_data))\n\n        # For demonstration, assume wake word is detected if energy is high\n        # and has a specific pattern\n        if energy > self.speech_threshold * 2:  # Higher threshold for wake word\n            # Additional checks could include:\n            # - Spectral analysis for specific word patterns\n            # - ML model inference\n            # - Keyword spotting algorithms\n            return True\n\n        return False\n\nclass WakeWordDetector:\n    """Advanced wake word detection system"""\n    def __init__(self):\n        self.wake_words = ["robot", "hey robot", "assistant", "listen"]\n        self.detected_wake_word = None\n        self.wake_word_confidence = 0.0\n\n    def detect_wake_word(self, audio_data, sample_rate=16000):\n        """Detect wake word in audio data"""\n        # This would typically use a trained model\n        # For this example, we\'ll use a simple approach\n\n        # In practice, you might use:\n        # - Porcupine wake word engine\n        # - Custom trained wake word model\n        # - Audio keyword spotting with ML\n\n        # Simple approach: look for specific audio patterns\n        # This is a placeholder - real implementation would be more sophisticated\n\n        # Calculate audio features\n        rms_energy = np.sqrt(np.mean(audio_data ** 2))\n\n        # Check for specific patterns (simplified)\n        if rms_energy > 0.05:  # Energy threshold\n            # In a real system, this would analyze spectral features\n            # and use ML models to identify wake words\n            self.detected_wake_word = "robot"\n            self.wake_word_confidence = 0.8\n            return True\n\n        return False\n\n    def reset_detection(self):\n        """Reset wake word detection state"""\n        self.detected_wake_word = None\n        self.wake_word_confidence = 0.0\n\n# Example usage\ndef example_audio_processing():\n    processor = AudioProcessor()\n    wake_detector = WakeWordDetector()\n\n    def audio_callback(audio_segment, start_time, end_time, partial=False):\n        if partial:\n            # Check for wake word in partial stream\n            if wake_detector.detect_wake_word(audio_segment):\n                print(f"Wake word detected at {start_time:.2f}s!")\n        else:\n            # Process complete speech segment\n            print(f"Complete speech segment processed: {len(audio_segment)} samples")\n\n    print("Starting audio processing example...")\n    print("Speak to test voice activity detection")\n\n    # Note: This would run indefinitely in a real system\n    # For this example, we\'ll just show the structure\n\nif __name__ == "__main__":\n    example_audio_processing()\n'})}),"\n",(0,i.jsx)(n.h3,{id:"speech-to-text-integration",children:"Speech-to-Text Integration"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"# speech_to_text.py\nimport speech_recognition as sr\nimport asyncio\nimport threading\nfrom concurrent.futures import ThreadPoolExecutor\nimport json\nimport time\nfrom typing import Dict, List, Optional, Callable\n\nclass SpeechToTextEngine:\n    def __init__(self, language=\"en-US\", model_type=\"default\"):\n        self.recognizer = sr.Recognizer()\n        self.microphone = sr.Microphone()\n        self.language = language\n        self.model_type = model_type\n\n        # Configuration\n        self.recognizer.energy_threshold = 300  # Adjust for environment\n        self.recognizer.dynamic_energy_threshold = True\n        self.recognizer.pause_threshold = 0.8  # Pause duration to consider phrase complete\n\n        # Initialize microphone settings\n        with self.microphone as source:\n            self.recognizer.adjust_for_ambient_noise(source, duration=1.0)\n\n        # Threading for async operations\n        self.executor = ThreadPoolExecutor(max_workers=2)\n\n        # Recognition history\n        self.recognition_history = []\n        self.max_history = 50\n\n    def recognize_speech(self, audio_data, use_api=True):\n        \"\"\"Recognize speech from audio data\"\"\"\n        try:\n            if use_api:\n                # Use Google Web Speech API (requires internet)\n                text = self.recognizer.recognize_google(audio_data, language=self.language)\n            else:\n                # Use offline recognition (if available)\n                text = self.recognizer.recognize_sphinx(audio_data, language=self.language)\n\n            # Add to history\n            self.add_to_history(text, confidence=0.9)  # Default confidence\n\n            return text\n        except sr.UnknownValueError:\n            return None\n        except sr.RequestError as e:\n            print(f\"Speech recognition error: {e}\")\n            return None\n\n    def add_to_history(self, text, confidence=0.0):\n        \"\"\"Add recognition result to history\"\"\"\n        entry = {\n            'text': text,\n            'confidence': confidence,\n            'timestamp': time.time()\n        }\n\n        self.recognition_history.append(entry)\n\n        # Keep history size manageable\n        if len(self.recognition_history) > self.max_history:\n            self.recognition_history = self.recognition_history[-self.max_history:]\n\n    def get_recognition_context(self):\n        \"\"\"Get recent recognition context\"\"\"\n        recent_entries = self.recognition_history[-5:]  # Last 5 recognitions\n        return [entry['text'] for entry in recent_entries if entry['text']]\n\n    def continuous_listening(self, callback: Callable[[str], None], timeout=5.0):\n        \"\"\"Continuously listen for speech and process results\"\"\"\n        with self.microphone as source:\n            print(\"Listening for speech...\")\n\n            while True:\n                try:\n                    # Listen for audio with timeout\n                    audio = self.recognizer.listen(source, timeout=timeout, phrase_time_limit=5.0)\n\n                    # Recognize speech\n                    text = self.recognize_speech(audio)\n\n                    if text:\n                        print(f\"Recognized: {text}\")\n                        callback(text)\n                    else:\n                        print(\"Could not understand audio\")\n\n                except sr.WaitTimeoutError:\n                    # No speech detected within timeout, continue listening\n                    continue\n                except sr.UnknownValueError:\n                    print(\"Could not understand audio\")\n                except sr.RequestError as e:\n                    print(f\"Speech recognition error: {e}\")\n                    time.sleep(1)  # Brief pause before retrying\n\nclass AdvancedSpeechRecognizer:\n    \"\"\"Advanced speech recognition with multiple engine support\"\"\"\n    def __init__(self, language=\"en-US\"):\n        self.language = language\n\n        # Multiple recognition engines\n        self.engines = {\n            'google': self.google_recognize,\n            'wit': self.wit_recognize,\n            'houndify': self.houndify_recognize,\n            'bing': self.bing_recognize\n        }\n\n        self.active_engine = 'google'\n        self.fallback_engines = ['google', 'bing']  # Priority order for fallback\n\n        # Recognition confidence thresholds\n        self.confidence_threshold = 0.7\n        self.low_confidence_threshold = 0.5\n\n    def set_credentials(self, engine, **credentials):\n        \"\"\"Set API credentials for recognition engines\"\"\"\n        if engine == 'wit':\n            self.wit_key = credentials.get('key')\n        elif engine == 'houndify':\n            self.houndify_client_id = credentials.get('client_id')\n            self.houndify_client_key = credentials.get('client_key')\n        elif engine == 'bing':\n            self.bing_key = credentials.get('key')\n\n    def recognize_with_fallback(self, audio_data, confidence_boost=False):\n        \"\"\"Recognize speech with fallback engines\"\"\"\n        results = []\n\n        for engine_name in self.fallback_engines:\n            try:\n                result = self.engines[engine_name](audio_data)\n                if result and result['confidence'] >= self.low_confidence_threshold:\n                    results.append(result)\n\n                    # If high confidence result found, return immediately\n                    if result['confidence'] >= self.confidence_threshold:\n                        return result\n            except Exception as e:\n                print(f\"Engine {engine_name} failed: {e}\")\n                continue\n\n        # If no results with acceptable confidence, return the best one\n        if results:\n            return max(results, key=lambda x: x['confidence'])\n\n        return None\n\n    def google_recognize(self, audio_data):\n        \"\"\"Google Web Speech API recognition\"\"\"\n        try:\n            import speech_recognition as sr\n            recognizer = sr.Recognizer()\n\n            # Get confidence using alternative method\n            text = recognizer.recognize_google(audio_data, language=self.language)\n\n            # Estimate confidence (Google API doesn't provide confidence directly)\n            # This is a simplified estimation\n            word_count = len(text.split())\n            confidence = min(0.9, 0.6 + (word_count * 0.05))  # Basic confidence estimation\n\n            return {\n                'text': text,\n                'confidence': confidence,\n                'engine': 'google'\n            }\n        except Exception as e:\n            raise e\n\n    def wit_recognize(self, audio_data):\n        \"\"\"Wit.ai recognition (requires API key)\"\"\"\n        try:\n            import speech_recognition as sr\n            recognizer = sr.Recognizer()\n\n            text = recognizer.recognize_wit(audio_data, key=self.wit_key, language=self.language)\n\n            # Wit.ai may provide confidence information\n            return {\n                'text': text,\n                'confidence': 0.8,  # Placeholder\n                'engine': 'wit'\n            }\n        except Exception as e:\n            raise e\n\n    def houndify_recognize(self, audio_data):\n        \"\"\"Houndify recognition (requires credentials)\"\"\"\n        try:\n            import speech_recognition as sr\n            recognizer = sr.Recognizer()\n\n            text = recognizer.recognize_houndify(\n                audio_data,\n                client_id=self.houndify_client_id,\n                client_key=self.houndify_client_key\n            )\n\n            return {\n                'text': text,\n                'confidence': 0.8,  # Placeholder\n                'engine': 'houndify'\n            }\n        except Exception as e:\n            raise e\n\n    def bing_recognize(self, audio_data):\n        \"\"\"Microsoft Bing recognition\"\"\"\n        try:\n            import speech_recognition as sr\n            recognizer = sr.Recognizer()\n\n            text = recognizer.recognize_bing(audio_data, key=self.bing_key, language=self.language)\n\n            return {\n                'text': text,\n                'confidence': 0.8,  # Placeholder\n                'engine': 'bing'\n            }\n        except Exception as e:\n            raise e\n\nclass VoiceCommandProcessor:\n    \"\"\"Process voice commands and map to robot actions\"\"\"\n    def __init__(self, speech_recognizer):\n        self.speech_recognizer = speech_recognizer\n        self.command_mapping = self.initialize_command_mapping()\n        self.context = {}\n        self.user_preferences = {}\n\n    def initialize_command_mapping(self):\n        \"\"\"Initialize mapping from voice commands to robot actions\"\"\"\n        return {\n            # Navigation commands\n            'navigate_to': {\n                'patterns': ['go to', 'navigate to', 'move to', 'walk to', 'head to'],\n                'action': 'navigation',\n                'required_params': ['destination']\n            },\n            'move_forward': {\n                'patterns': ['move forward', 'go forward', 'step forward', 'forward'],\n                'action': 'move',\n                'required_params': ['direction', 'distance']\n            },\n            'turn': {\n                'patterns': ['turn left', 'turn right', 'turn around', 'rotate'],\n                'action': 'rotate',\n                'required_params': ['direction']\n            },\n\n            # Manipulation commands\n            'grasp_object': {\n                'patterns': ['grasp', 'grab', 'pick up', 'take', 'hold'],\n                'action': 'grasp',\n                'required_params': ['object']\n            },\n            'release_object': {\n                'patterns': ['release', 'let go', 'drop', 'put down'],\n                'action': 'release',\n                'required_params': []\n            },\n\n            # Information commands\n            'get_time': {\n                'patterns': ['what time is it', 'time', 'current time', 'tell me the time'],\n                'action': 'time_query',\n                'required_params': []\n            },\n            'get_date': {\n                'patterns': ['what date is it', 'date', 'current date', 'tell me the date'],\n                'action': 'date_query',\n                'required_params': []\n            },\n\n            # Social interaction\n            'greeting': {\n                'patterns': ['hello', 'hi', 'good morning', 'good afternoon', 'good evening'],\n                'action': 'greet',\n                'required_params': []\n            },\n            'farewell': {\n                'patterns': ['goodbye', 'bye', 'see you', 'farewell'],\n                'action': 'farewell',\n                'required_params': []\n            }\n        }\n\n    def process_voice_command(self, text):\n        \"\"\"Process voice command and extract action\"\"\"\n        text_lower = text.lower().strip()\n\n        # Check for command patterns\n        for command_type, config in self.command_mapping.items():\n            for pattern in config['patterns']:\n                if pattern in text_lower:\n                    # Extract parameters\n                    params = self.extract_parameters(text_lower, command_type, config)\n\n                    # Validate required parameters\n                    missing_params = self.validate_parameters(params, config['required_params'])\n\n                    if not missing_params:\n                        return {\n                            'action': config['action'],\n                            'command_type': command_type,\n                            'parameters': params,\n                            'raw_text': text\n                        }\n                    else:\n                        # Ask for missing parameters\n                        return {\n                            'action': 'request_info',\n                            'missing_params': missing_params,\n                            'original_command': command_type\n                        }\n\n        # If no specific command found, treat as general query\n        return {\n            'action': 'general_query',\n            'query': text,\n            'command_type': 'unknown'\n        }\n\n    def extract_parameters(self, text, command_type, config):\n        \"\"\"Extract parameters from command text\"\"\"\n        params = {}\n\n        if command_type == 'navigate_to':\n            # Extract destination from text like \"go to kitchen\"\n            for pattern in config['patterns']:\n                if pattern in text:\n                    remaining_text = text.replace(pattern, '').strip()\n                    if remaining_text:\n                        params['destination'] = remaining_text\n                    break\n\n        elif command_type == 'grasp_object':\n            # Extract object to grasp\n            for pattern in config['patterns']:\n                if pattern in text:\n                    remaining_text = text.replace(pattern, '').strip()\n                    if remaining_text:\n                        # Remove common words like \"the\", \"a\", \"an\"\n                        remaining_text = remaining_text.replace('the ', '').replace('a ', '').replace('an ', '').strip()\n                        params['object'] = remaining_text\n                    break\n\n        elif command_type == 'move_forward':\n            # Extract distance if specified\n            import re\n            distance_match = re.search(r'(\\d+(?:\\.\\d+)?)\\s*(meters?|m|feet|ft)', text)\n            if distance_match:\n                params['distance'] = float(distance_match.group(1))\n                params['unit'] = distance_match.group(2)\n            else:\n                params['distance'] = 1.0  # Default distance\n                params['unit'] = 'meters'\n\n        elif command_type == 'turn':\n            if 'left' in text:\n                params['direction'] = 'left'\n            elif 'right' in text:\n                params['direction'] = 'right'\n            elif 'around' in text:\n                params['direction'] = 'around'\n            else:\n                params['direction'] = 'left'  # Default\n\n        return params\n\n    def validate_parameters(self, params, required_params):\n        \"\"\"Validate that required parameters are present\"\"\"\n        missing = []\n        for param in required_params:\n            if param not in params or params[param] is None:\n                missing.append(param)\n        return missing\n\n    def handle_command(self, command_result):\n        \"\"\"Handle the processed command result\"\"\"\n        action = command_result['action']\n\n        if action == 'request_info':\n            # Request missing information\n            missing = command_result['missing_params']\n            original_cmd = command_result['original_command']\n\n            return f\"I need more information: {', '.join(missing)} for {original_cmd.replace('_', ' ')} command.\"\n\n        elif action == 'general_query':\n            # Handle as general query\n            query = command_result['query']\n            return f\"I received your query: '{query}'. How can I help?\"\n\n        else:\n            # Execute specific action\n            command_type = command_result['command_type']\n            params = command_result.get('parameters', {})\n\n            return f\"Executing {command_type.replace('_', ' ')} with parameters: {params}\"\n\n# Example usage\ndef example_speech_recognition():\n    print(\"Speech Recognition Example\")\n\n    # Initialize speech recognizer\n    stt_engine = SpeechToTextEngine()\n\n    # Initialize command processor\n    command_processor = VoiceCommandProcessor(stt_engine)\n\n    # Test commands\n    test_commands = [\n        \"Navigate to the kitchen\",\n        \"Grasp the red cup\",\n        \"What time is it?\",\n        \"Hello robot\",\n        \"Turn left\"\n    ]\n\n    print(\"Processing test commands:\")\n    for command in test_commands:\n        print(f\"\\nInput: {command}\")\n        result = command_processor.process_voice_command(command)\n        response = command_processor.handle_command(result)\n        print(f\"Output: {response}\")\n\nif __name__ == \"__main__\":\n    try:\n        example_speech_recognition()\n    except ImportError as e:\n        print(f\"Missing dependency: {e}\")\n        print(\"Install: pip install SpeechRecognition pyaudio\")\n"})}),"\n",(0,i.jsx)(n.h2,{id:"natural-language-understanding",children:"Natural Language Understanding"}),"\n",(0,i.jsx)(n.h3,{id:"intent-recognition-and-entity-extraction",children:"Intent Recognition and Entity Extraction"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"# nlu_engine.py\nimport re\nimport spacy\nimport numpy as np\nfrom typing import Dict, List, Tuple, Optional\nfrom dataclasses import dataclass\nfrom enum import Enum\nimport json\n\nclass IntentType(Enum):\n    NAVIGATION = \"navigation\"\n    MANIPULATION = \"manipulation\"\n    INFORMATION = \"information\"\n    SOCIAL = \"social\"\n    SYSTEM = \"system\"\n\n@dataclass\nclass Entity:\n    \"\"\"Represents an extracted entity\"\"\"\n    text: str\n    label: str\n    start: int\n    end: int\n    confidence: float = 1.0\n\n@dataclass\nclass Intent:\n    \"\"\"Represents a recognized intent\"\"\"\n    name: str\n    confidence: float\n    entities: List[Entity]\n    parameters: Dict[str, str]\n\nclass RuleBasedNLUEngine:\n    \"\"\"Rule-based Natural Language Understanding engine\"\"\"\n    def __init__(self):\n        # Load spaCy model (small English model)\n        try:\n            self.nlp = spacy.load(\"en_core_web_sm\")\n        except OSError:\n            print(\"SpaCy model not found. Install with: python -m spacy download en_core_web_sm\")\n            self.nlp = None\n\n        self.intent_patterns = self.initialize_intent_patterns()\n        self.entity_patterns = self.initialize_entity_patterns()\n\n    def initialize_intent_patterns(self):\n        \"\"\"Initialize patterns for intent recognition\"\"\"\n        return {\n            IntentType.NAVIGATION: [\n                (r'go to (\\w+)', 0.9),\n                (r'move to (\\w+)', 0.9),\n                (r'navigate to (\\w+)', 0.9),\n                (r'walk to (\\w+)', 0.9),\n                (r'head to (\\w+)', 0.8),\n                (r'bring me to (\\w+)', 0.8),\n            ],\n            IntentType.MANIPULATION: [\n                (r'grasp (\\w+)', 0.9),\n                (r'grab (\\w+)', 0.9),\n                (r'pick up (\\w+)', 0.9),\n                (r'take (\\w+)', 0.9),\n                (r'hold (\\w+)', 0.8),\n                (r'lift (\\w+)', 0.8),\n                (r'release (\\w+)', 0.9),\n                (r'put down (\\w+)', 0.9),\n                (r'drop (\\w+)', 0.8),\n            ],\n            IntentType.INFORMATION: [\n                (r'what time', 0.9),\n                (r'what date', 0.9),\n                (r'tell me about', 0.8),\n                (r'how (?:is|are|do)', 0.7),\n                (r'what (?:is|are|can)', 0.7),\n                (r'when', 0.7),\n                (r'where', 0.7),\n                (r'who', 0.7),\n            ],\n            IntentType.SOCIAL: [\n                (r'hello', 0.9),\n                (r'hi', 0.9),\n                (r'good morning', 0.9),\n                (r'good afternoon', 0.9),\n                (r'good evening', 0.9),\n                (r'goodbye', 0.9),\n                (r'bye', 0.9),\n                (r'thank you', 0.8),\n                (r'thanks', 0.8),\n            ],\n            IntentType.SYSTEM: [\n                (r'stop', 0.9),\n                (r'cancel', 0.9),\n                (r'abort', 0.8),\n                (r'help', 0.8),\n                (r'reset', 0.7),\n                (r'restart', 0.7),\n            ]\n        }\n\n    def initialize_entity_patterns(self):\n        \"\"\"Initialize patterns for entity extraction\"\"\"\n        return {\n            'LOCATION': [\n                (r'\\b(kitchen|living room|bedroom|office|bathroom|dining room|hallway|garage|garden|outside)\\b', 0.9),\n            ],\n            'OBJECT': [\n                (r'\\b(cup|bottle|book|phone|keys|ball|box|toy|food|water|coffee)\\b', 0.8),\n            ],\n            'PERSON': [\n                (r'\\b(mom|dad|mother|father|brother|sister|friend|person|man|woman|child)\\b', 0.7),\n            ],\n            'TIME': [\n                (r'\\b(\\d{1,2}:\\d{2})\\b', 0.9),  # Time like 10:30\n                (r'\\b(today|tomorrow|yesterday|now|later|morning|afternoon|evening|night)\\b', 0.8),\n            ],\n            'NUMBER': [\n                (r'\\b(\\d+(?:\\.\\d+)?)\\b', 0.9),\n            ]\n        }\n\n    def process_text(self, text: str) -> Intent:\n        \"\"\"Process text and extract intent and entities\"\"\"\n        # First, try to recognize intent\n        intent = self.recognize_intent(text)\n\n        # Then, extract entities\n        entities = self.extract_entities(text)\n\n        # Update intent with entities\n        intent.entities = entities\n\n        # Extract parameters from entities\n        intent.parameters = self.entities_to_parameters(entities)\n\n        return intent\n\n    def recognize_intent(self, text: str) -> Intent:\n        \"\"\"Recognize intent from text using pattern matching\"\"\"\n        best_intent = None\n        best_confidence = 0.0\n\n        for intent_type, patterns in self.intent_patterns.items():\n            for pattern, base_confidence in patterns:\n                match = re.search(pattern, text.lower())\n                if match:\n                    # Calculate confidence based on match quality\n                    confidence = base_confidence\n\n                    # Boost confidence if it's an exact match\n                    if match.group(0).lower() == text.lower().strip():\n                        confidence = min(1.0, confidence + 0.1)\n\n                    if confidence > best_confidence:\n                        best_confidence = confidence\n                        best_intent = Intent(\n                            name=intent_type.value,\n                            confidence=confidence,\n                            entities=[],\n                            parameters={}\n                        )\n\n        if best_intent is None:\n            # Default to general query intent\n            best_intent = Intent(\n                name='general_query',\n                confidence=0.3,\n                entities=[],\n                parameters={}\n            )\n\n        return best_intent\n\n    def extract_entities(self, text: str) -> List[Entity]:\n        \"\"\"Extract entities from text using pattern matching\"\"\"\n        entities = []\n\n        for entity_type, patterns in self.entity_patterns.items():\n            for pattern, base_confidence in patterns:\n                matches = re.finditer(pattern, text.lower())\n                for match in matches:\n                    entity = Entity(\n                        text=match.group(1) if len(match.groups()) > 0 else match.group(0),\n                        label=entity_type,\n                        start=match.start(),\n                        end=match.end(),\n                        confidence=base_confidence\n                    )\n                    entities.append(entity)\n\n        # Sort entities by position in text\n        entities.sort(key=lambda x: x.start)\n\n        return entities\n\n    def entities_to_parameters(self, entities: List[Entity]) -> Dict[str, str]:\n        \"\"\"Convert entities to action parameters\"\"\"\n        params = {}\n\n        for entity in entities:\n            if entity.label == 'LOCATION':\n                params['destination'] = entity.text\n            elif entity.label == 'OBJECT':\n                params['object'] = entity.text\n            elif entity.label == 'PERSON':\n                params['person'] = entity.text\n            elif entity.label == 'TIME':\n                params['time'] = entity.text\n            elif entity.label == 'NUMBER':\n                params['number'] = entity.text\n\n        return params\n\nclass MLBasedNLUEngine:\n    \"\"\"Machine Learning-based NLU engine (simplified example)\"\"\"\n    def __init__(self):\n        # In practice, this would load a trained model\n        # For this example, we'll simulate with rule-based approach\n        self.rule_engine = RuleBasedNLUEngine()\n\n        # Pre-trained intent classification (simulated)\n        self.intent_classifier = self.train_intent_classifier()\n\n        # Named Entity Recognition (simulated)\n        self.ner_model = self.train_ner_model()\n\n    def train_intent_classifier(self):\n        \"\"\"Train intent classification model (simulated)\"\"\"\n        # This would typically involve:\n        # 1. Training data with labeled intents\n        # 2. Feature extraction (TF-IDF, embeddings, etc.)\n        # 3. Training classifier (SVM, neural network, etc.)\n\n        # For simulation, return a function that mimics classification\n        def classify_intent(text):\n            # Simulate ML classification with confidence scores\n            text_lower = text.lower()\n\n            scores = {\n                'navigation': 0.1,\n                'manipulation': 0.1,\n                'information': 0.1,\n                'social': 0.1,\n                'system': 0.1\n            }\n\n            # Boost scores based on keywords\n            if any(word in text_lower for word in ['go', 'move', 'navigate', 'walk', 'head']):\n                scores['navigation'] = 0.9\n            if any(word in text_lower for word in ['grasp', 'grab', 'pick', 'take', 'hold', 'release']):\n                scores['manipulation'] = 0.9\n            if any(word in text_lower for word in ['what', 'when', 'where', 'how', 'time', 'date']):\n                scores['information'] = 0.8\n            if any(word in text_lower for word in ['hello', 'hi', 'good', 'bye', 'thank']):\n                scores['social'] = 0.9\n            if any(word in text_lower for word in ['stop', 'cancel', 'help']):\n                scores['system'] = 0.8\n\n            # Return best scoring intent\n            best_intent = max(scores.items(), key=lambda x: x[1])\n            return best_intent[0], best_intent[1]\n\n        return classify_intent\n\n    def train_ner_model(self):\n        \"\"\"Train Named Entity Recognition model (simulated)\"\"\"\n        # This would typically use models like BERT, SpaCy, etc.\n        # For simulation, return a function that mimics NER\n\n        def recognize_entities(text):\n            # Simulate entity recognition\n            entities = []\n\n            # Location entities\n            location_pattern = r'\\b(kitchen|living room|bedroom|office|bathroom)\\b'\n            for match in re.finditer(location_pattern, text.lower()):\n                entities.append(Entity(\n                    text=match.group(0),\n                    label='LOCATION',\n                    start=match.start(),\n                    end=match.end(),\n                    confidence=0.8\n                ))\n\n            # Object entities\n            object_pattern = r'\\b(cup|bottle|book|phone|keys)\\b'\n            for match in re.finditer(object_pattern, text.lower()):\n                entities.append(Entity(\n                    text=match.group(0),\n                    label='OBJECT',\n                    start=match.start(),\n                    end=match.end(),\n                    confidence=0.7\n                ))\n\n            return entities\n\n        return recognize_entities\n\n    def process_text(self, text: str) -> Intent:\n        \"\"\"Process text using ML-based NLU\"\"\"\n        # Classify intent\n        intent_name, intent_confidence = self.intent_classifier(text)\n\n        # Recognize entities\n        entities = self.ner_model(text)\n\n        # Create intent object\n        intent = Intent(\n            name=intent_name,\n            confidence=intent_confidence,\n            entities=entities,\n            parameters=self.entities_to_parameters(entities)\n        )\n\n        return intent\n\n    def entities_to_parameters(self, entities: List[Entity]) -> Dict[str, str]:\n        \"\"\"Convert entities to action parameters\"\"\"\n        params = {}\n\n        for entity in entities:\n            if entity.label == 'LOCATION':\n                params['destination'] = entity.text\n            elif entity.label == 'OBJECT':\n                params['object'] = entity.text\n\n        return params\n\nclass ContextualNLUEngine:\n    \"\"\"Context-aware NLU engine that considers conversation history\"\"\"\n    def __init__(self):\n        self.ml_engine = MLBasedNLUEngine()\n        self.conversation_context = []\n        self.max_context_length = 10\n\n    def process_text_with_context(self, text: str, user_id: str = \"default\") -> Intent:\n        \"\"\"Process text considering conversation context\"\"\"\n        # Get recent context\n        recent_context = self.get_recent_context(user_id)\n\n        # Process with context\n        intent = self.ml_engine.process_text(text)\n\n        # Enhance with context\n        enhanced_intent = self.enhance_with_context(intent, recent_context, text)\n\n        # Add to context\n        self.add_to_context(user_id, text, enhanced_intent)\n\n        return enhanced_intent\n\n    def get_recent_context(self, user_id: str) -> List[Dict]:\n        \"\"\"Get recent conversation context for user\"\"\"\n        # In practice, this would access a database or memory system\n        user_context = []\n\n        # Filter context for specific user\n        for entry in self.conversation_context[-self.max_context_length:]:\n            if entry.get('user_id') == user_id:\n                user_context.append(entry)\n\n        return user_context\n\n    def enhance_with_context(self, intent: Intent, context: List[Dict], current_text: str) -> Intent:\n        \"\"\"Enhance intent recognition with context information\"\"\"\n        # If intent confidence is low, use context to improve\n        if intent.confidence < 0.6 and context:\n            # Look for related entities in context\n            for entry in reversed(context[-3:]):  # Check last 3 exchanges\n                prev_intent = entry.get('intent', {})\n                prev_entities = prev_intent.get('entities', [])\n\n                # If previous interaction was about navigation,\n                # and current text is ambiguous, assume navigation context\n                if (prev_intent.get('name') == 'navigation' and\n                    any(word in current_text.lower() for word in ['there', 'it', 'that', 'the'])):\n\n                    # Try to resolve ambiguous references\n                    for entity in prev_entities:\n                        if entity['label'] == 'LOCATION':\n                            # Current text might refer to this location\n                            if 'go' in current_text.lower() or 'move' in current_text.lower():\n                                intent.parameters['destination'] = entity['text']\n                                intent.confidence = max(intent.confidence, 0.7)\n                                intent.name = 'navigation'\n\n        return intent\n\n    def add_to_context(self, user_id: str, text: str, intent: Intent):\n        \"\"\"Add interaction to conversation context\"\"\"\n        context_entry = {\n            'user_id': user_id,\n            'text': text,\n            'intent': {\n                'name': intent.name,\n                'confidence': intent.confidence,\n                'entities': [{'text': e.text, 'label': e.label, 'confidence': e.confidence}\n                           for e in intent.entities],\n                'parameters': intent.parameters.copy()\n            },\n            'timestamp': time.time()\n        }\n\n        self.conversation_context.append(context_entry)\n\n        # Keep context size manageable\n        if len(self.conversation_context) > 100:  # Maximum 100 entries\n            self.conversation_context = self.conversation_context[-50:]\n\n# Example usage\ndef example_nlu_processing():\n    print(\"Natural Language Understanding Example\")\n\n    # Initialize NLU engines\n    rule_engine = RuleBasedNLUEngine()\n    ml_engine = MLBasedNLUEngine()\n    contextual_engine = ContextualNLUEngine()\n\n    # Test sentences\n    test_sentences = [\n        \"Navigate to the kitchen\",\n        \"Grasp the red cup\",\n        \"What time is it?\",\n        \"Hello robot\",\n        \"Go to the place we talked about\",\n        \"Pick up that object\"\n    ]\n\n    print(\"Testing Rule-Based NLU:\")\n    for sentence in test_sentences:\n        intent = rule_engine.process_text(sentence)\n        print(f\"Input: '{sentence}' -> Intent: {intent.name} (conf: {intent.confidence:.2f})\")\n        if intent.entities:\n            print(f\"  Entities: {[(e.text, e.label) for e in intent.entities]}\")\n        if intent.parameters:\n            print(f\"  Parameters: {intent.parameters}\")\n\n    print(\"\\nTesting ML-Based NLU:\")\n    for sentence in test_sentences:\n        intent = ml_engine.process_text(sentence)\n        print(f\"Input: '{sentence}' -> Intent: {intent.name} (conf: {intent.confidence:.2f})\")\n        if intent.entities:\n            print(f\"  Entities: {[(e.text, e.label) for e in intent.entities]}\")\n\nif __name__ == \"__main__\":\n    import time\n    example_nlu_processing()\n"})}),"\n",(0,i.jsx)(n.h2,{id:"multi-modal-interaction",children:"Multi-Modal Interaction"}),"\n",(0,i.jsx)(n.h3,{id:"combining-speech-with-other-modalities",children:"Combining Speech with Other Modalities"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"# multi_modal_interaction.py\nimport numpy as np\nimport cv2\nimport asyncio\nfrom typing import Dict, List, Any, Optional, Tuple\nfrom dataclasses import dataclass\nimport threading\nimport queue\n\n@dataclass\nclass MultiModalInput:\n    \"\"\"Represents input from multiple modalities\"\"\"\n    speech: Optional[str] = None\n    vision: Optional[np.ndarray] = None\n    gesture: Optional[str] = None\n    touch: Optional[Dict[str, Any]] = None\n    timestamp: float = 0.0\n\n@dataclass\nclass MultiModalOutput:\n    \"\"\"Represents output across multiple modalities\"\"\"\n    speech: Optional[str] = None\n    action: Optional[str] = None\n    visual_feedback: Optional[np.ndarray] = None\n    haptic_feedback: Optional[str] = None\n\nclass MultiModalFusionEngine:\n    \"\"\"Fuses information from multiple modalities\"\"\"\n    def __init__(self):\n        self.speech_processor = self.initialize_speech_processor()\n        self.vision_processor = self.initialize_vision_processor()\n        self.gesture_processor = self.initialize_gesture_processor()\n\n        # Confidence weights for different modalities\n        self.modality_weights = {\n            'speech': 0.6,\n            'vision': 0.3,\n            'gesture': 0.1\n        }\n\n        # Fusion strategies\n        self.fusion_strategies = {\n            'early': self.early_fusion,\n            'late': self.late_fusion,\n            'intermediate': self.intermediate_fusion\n        }\n\n        self.current_fusion_strategy = 'late'\n\n    def initialize_speech_processor(self):\n        \"\"\"Initialize speech processing components\"\"\"\n        # This would integrate with the speech recognition system\n        return {\n            'recognizer': None,  # Would be connected to speech recognizer\n            'nlu': None          # Would be connected to NLU engine\n        }\n\n    def initialize_vision_processor(self):\n        \"\"\"Initialize vision processing components\"\"\"\n        # This would connect to computer vision systems\n        return {\n            'object_detector': None,\n            'pose_estimator': None,\n            'scene_analyzer': None\n        }\n\n    def initialize_gesture_processor(self):\n        \"\"\"Initialize gesture processing components\"\"\"\n        # This would connect to gesture recognition systems\n        return {\n            'hand_tracker': None,\n            'gesture_classifier': None\n        }\n\n    def early_fusion(self, inputs: MultiModalInput) -> Dict[str, Any]:\n        \"\"\"Combine raw inputs from different modalities before processing\"\"\"\n        fused_features = {}\n\n        # For early fusion, we would combine raw features\n        # This is complex and often not practical for different data types\n        # So we'll simulate by creating a combined representation\n\n        if inputs.speech:\n            # Convert speech to features\n            speech_features = self.extract_speech_features(inputs.speech)\n            fused_features['speech'] = speech_features\n\n        if inputs.vision is not None:\n            # Extract visual features\n            vision_features = self.extract_vision_features(inputs.vision)\n            fused_features['vision'] = vision_features\n\n        if inputs.gesture:\n            # Convert gesture to features\n            gesture_features = self.extract_gesture_features(inputs.gesture)\n            fused_features['gesture'] = gesture_features\n\n        return fused_features\n\n    def late_fusion(self, inputs: MultiModalInput) -> Dict[str, Any]:\n        \"\"\"Process each modality separately and combine results\"\"\"\n        results = {}\n\n        if inputs.speech:\n            speech_result = self.process_speech(inputs.speech)\n            results['speech'] = speech_result\n\n        if inputs.vision is not None:\n            vision_result = self.process_vision(inputs.vision)\n            results['vision'] = vision_result\n\n        if inputs.gesture:\n            gesture_result = self.process_gesture(inputs.gesture)\n            results['gesture'] = gesture_result\n\n        # Combine results based on confidence and weights\n        combined_result = self.combine_results(results)\n\n        return combined_result\n\n    def intermediate_fusion(self, inputs: MultiModalInput) -> Dict[str, Any]:\n        \"\"\"Combine modalities at intermediate processing levels\"\"\"\n        # This would involve sharing representations between modalities\n        # during processing, which is more complex\n\n        # For this example, we'll use a weighted combination\n        # of partially processed information\n        results = self.late_fusion(inputs)\n\n        # Apply cross-modal influence\n        refined_result = self.apply_cross_modal_influence(results, inputs)\n\n        return refined_result\n\n    def extract_speech_features(self, speech: str) -> Dict[str, Any]:\n        \"\"\"Extract features from speech input\"\"\"\n        # In practice, this would use acoustic and linguistic features\n        return {\n            'text': speech,\n            'word_count': len(speech.split()),\n            'key_phrases': self.extract_key_phrases(speech),\n            'sentiment': self.estimate_sentiment(speech)\n        }\n\n    def extract_vision_features(self, vision: np.ndarray) -> Dict[str, Any]:\n        \"\"\"Extract features from visual input\"\"\"\n        # In practice, this would use CNN features, object detection, etc.\n        return {\n            'objects_present': [],  # Would come from object detection\n            'scene_type': 'indoor',  # Would be classified\n            'people_present': 0,     # Would come from person detection\n            'prominent_colors': []   # Would come from color analysis\n        }\n\n    def extract_gesture_features(self, gesture: str) -> Dict[str, Any]:\n        \"\"\"Extract features from gesture input\"\"\"\n        return {\n            'gesture_type': gesture,\n            'confidence': 0.9,\n            'temporal_pattern': 'single'\n        }\n\n    def process_speech(self, speech: str) -> Dict[str, Any]:\n        \"\"\"Process speech input\"\"\"\n        # Use NLU to extract meaning\n        # This would connect to the NLU system\n        return {\n            'intent': 'unknown',\n            'entities': [],\n            'confidence': 0.8,\n            'raw_text': speech\n        }\n\n    def process_vision(self, vision: np.ndarray) -> Dict[str, Any]:\n        \"\"\"Process visual input\"\"\"\n        # This would use computer vision algorithms\n        return {\n            'detected_objects': [],\n            'scene_description': 'unknown',\n            'confidence': 0.7\n        }\n\n    def process_gesture(self, gesture: str) -> Dict[str, Any]:\n        \"\"\"Process gesture input\"\"\"\n        return {\n            'gesture_meaning': gesture,\n            'confidence': 0.9\n        }\n\n    def combine_results(self, results: Dict[str, Dict]) -> Dict[str, Any]:\n        \"\"\"Combine results from different modalities\"\"\"\n        # Calculate weighted confidence\n        total_confidence = 0\n        weighted_intent = None\n        max_confidence = 0\n\n        for modality, result in results.items():\n            weight = self.modality_weights.get(modality, 0.1)\n            confidence = result.get('confidence', 0.5) * weight\n\n            total_confidence += confidence\n\n            if confidence > max_confidence:\n                max_confidence = confidence\n                weighted_intent = result.get('intent', 'unknown')\n\n        # Normalize confidence\n        normalized_confidence = min(1.0, total_confidence)\n\n        return {\n            'intent': weighted_intent or 'unknown',\n            'confidence': normalized_confidence,\n            'modality_contributions': {\n                mod: results[mod].get('confidence', 0.5) if mod in results else 0\n                for mod in self.modality_weights.keys()\n            },\n            'individual_results': results\n        }\n\n    def apply_cross_modal_influence(self, results: Dict, inputs: MultiModalInput) -> Dict[str, Any]:\n        \"\"\"Apply cross-modal influence to refine results\"\"\"\n        # Example: If speech says \"that object\" and vision shows an object,\n        # link them together\n        refined = results.copy()\n\n        speech_result = results['individual_results'].get('speech', {})\n        vision_result = results['individual_results'].get('vision', {})\n\n        # If speech contains reference to visual object\n        if (speech_result.get('raw_text') and\n            any(word in speech_result['raw_text'].lower() for word in ['that', 'there', 'it']) and\n            vision_result.get('detected_objects')):\n\n            # Resolve the reference\n            refined['resolved_reference'] = vision_result['detected_objects'][0] if vision_result['detected_objects'] else None\n            refined['intent'] = 'referenced_object_interaction'\n\n        return refined\n\n    def extract_key_phrases(self, text: str) -> List[str]:\n        \"\"\"Extract key phrases from text\"\"\"\n        # Simple keyword extraction\n        keywords = ['navigate', 'grasp', 'move', 'go to', 'pick up', 'hello', 'help']\n        found_keywords = [kw for kw in keywords if kw.lower() in text.lower()]\n        return found_keywords\n\n    def estimate_sentiment(self, text: str) -> str:\n        \"\"\"Estimate sentiment from text\"\"\"\n        # Simple sentiment analysis\n        positive_words = ['good', 'great', 'excellent', 'please', 'thank', 'nice']\n        negative_words = ['bad', 'terrible', 'stop', 'don\\'t', 'not']\n\n        pos_count = sum(1 for word in positive_words if word in text.lower())\n        neg_count = sum(1 for word in negative_words if word in text.lower())\n\n        if pos_count > neg_count:\n            return 'positive'\n        elif neg_count > pos_count:\n            return 'negative'\n        else:\n            return 'neutral'\n\nclass MultiModalInteractionManager:\n    \"\"\"Manages multi-modal interaction flow\"\"\"\n    def __init__(self):\n        self.fusion_engine = MultiModalFusionEngine()\n        self.input_queue = queue.Queue()\n        self.output_queue = queue.Queue()\n\n        # Active interaction state\n        self.current_interaction = None\n        self.interaction_history = []\n\n        # Modalities\n        self.speech_recognizer = None  # Would be connected to speech system\n        self.vision_system = None      # Would be connected to vision system\n        self.gesture_system = None     # Would be connected to gesture system\n\n    def process_multi_modal_input(self, inputs: MultiModalInput) -> MultiModalOutput:\n        \"\"\"Process multi-modal input and generate response\"\"\"\n        # Fuse the inputs\n        fused_result = self.fusion_engine.late_fusion(inputs)\n\n        # Generate appropriate response based on fused result\n        response = self.generate_response(fused_result, inputs)\n\n        return response\n\n    def generate_response(self, fused_result: Dict[str, Any], inputs: MultiModalInput) -> MultiModalOutput:\n        \"\"\"Generate multi-modal response\"\"\"\n        output = MultiModalOutput()\n\n        intent = fused_result.get('intent', 'unknown')\n        confidence = fused_result.get('confidence', 0.0)\n\n        if confidence < 0.3:\n            output.speech = \"I'm not sure I understood that correctly. Could you please repeat?\"\n        elif intent == 'navigation':\n            output.speech = \"I'll navigate to the location for you.\"\n            output.action = \"navigate_to_location\"\n        elif intent == 'manipulation':\n            output.speech = \"I'll grasp that object for you.\"\n            output.action = \"grasp_object\"\n        elif intent == 'referenced_object_interaction':\n            output.speech = \"I see what you mean. I'll interact with that object.\"\n            output.action = \"interact_with_object\"\n        else:\n            output.speech = f\"I understand you want me to {intent.replace('_', ' ')}.\"\n\n        # Add visual feedback if vision input was used\n        if inputs.vision is not None:\n            output.visual_feedback = self.generate_visual_feedback(inputs.vision, fused_result)\n\n        return output\n\n    def generate_visual_feedback(self, vision_input: np.ndarray, analysis: Dict[str, Any]) -> np.ndarray:\n        \"\"\"Generate visual feedback based on input and analysis\"\"\"\n        # Create a simple visual feedback (in practice, this would be more sophisticated)\n        feedback = vision_input.copy()\n\n        # Draw confidence indicator\n        height, width = feedback.shape[:2]\n        conf_text = f\"Confidence: {analysis.get('confidence', 0):.2f}\"\n\n        cv2.putText(feedback, conf_text, (10, 30),\n                   cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n\n        return feedback\n\n    def start_interaction_loop(self):\n        \"\"\"Start the multi-modal interaction loop\"\"\"\n        def interaction_worker():\n            while True:\n                try:\n                    # Get input from queue\n                    inputs = self.input_queue.get(timeout=1.0)\n\n                    # Process the multi-modal input\n                    output = self.process_multi_modal_input(inputs)\n\n                    # Put output in queue\n                    self.output_queue.put(output)\n\n                    # Add to interaction history\n                    self.interaction_history.append({\n                        'input': inputs,\n                        'output': output,\n                        'timestamp': time.time()\n                    })\n\n                    # Keep history size manageable\n                    if len(self.interaction_history) > 100:\n                        self.interaction_history = self.interaction_history[-50:]\n\n                except queue.Empty:\n                    continue\n                except Exception as e:\n                    print(f\"Error in interaction loop: {e}\")\n\n        # Start worker thread\n        worker_thread = threading.Thread(target=interaction_worker, daemon=True)\n        worker_thread.start()\n\n    def add_input(self, inputs: MultiModalInput):\n        \"\"\"Add multi-modal input to processing queue\"\"\"\n        self.input_queue.put(inputs)\n\n    def get_output(self) -> Optional[MultiModalOutput]:\n        \"\"\"Get processed output from queue\"\"\"\n        try:\n            return self.output_queue.get_nowait()\n        except queue.Empty:\n            return None\n\n# Example usage\ndef example_multi_modal_interaction():\n    print(\"Multi-Modal Interaction Example\")\n\n    # Initialize the multi-modal system\n    manager = MultiModalInteractionManager()\n\n    # Start the interaction loop\n    manager.start_interaction_loop()\n\n    # Simulate multi-modal inputs\n    test_inputs = [\n        MultiModalInput(\n            speech=\"Navigate to the kitchen\",\n            vision=np.random.rand(480, 640, 3),  # Simulated image\n            timestamp=time.time()\n        ),\n        MultiModalInput(\n            speech=\"Grasp the red cup\",\n            vision=np.random.rand(480, 640, 3),\n            gesture=\"pointing\",\n            timestamp=time.time()\n        ),\n        MultiModalInput(\n            speech=\"Go to that location\",\n            vision=np.random.rand(480, 640, 3),\n            gesture=\"pointing\",\n            timestamp=time.time()\n        )\n    ]\n\n    print(\"Processing multi-modal inputs:\")\n    for i, inputs in enumerate(test_inputs):\n        print(f\"\\nTest {i+1}:\")\n        print(f\"Speech: {inputs.speech}\")\n        print(f\"Vision: {'Present' if inputs.vision is not None else 'Absent'}\")\n        print(f\"Gesture: {inputs.gesture}\")\n\n        # Process the input\n        output = manager.process_multi_modal_input(inputs)\n\n        print(f\"Response: {output.speech}\")\n        print(f\"Action: {output.action}\")\n\nif __name__ == \"__main__\":\n    import time\n    example_multi_modal_interaction()\n"})}),"\n",(0,i.jsx)(n.h2,{id:"dialogue-management",children:"Dialogue Management"}),"\n",(0,i.jsx)(n.h3,{id:"conversational-flow-control",children:"Conversational Flow Control"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"# dialogue_management.py\nimport re\nimport json\nfrom typing import Dict, List, Any, Optional, Callable\nfrom dataclasses import dataclass\nfrom enum import Enum\nimport time\n\nclass DialogueState(Enum):\n    GREETING = \"greeting\"\n    LISTENING = \"listening\"\n    PROCESSING = \"processing\"\n    RESPONDING = \"responding\"\n    CONFIRMING = \"confirming\"\n    HANDLING_ERROR = \"handling_error\"\n    IDLE = \"idle\"\n\nclass ConversationType(Enum):\n    TASK_ORIENTED = \"task_oriented\"\n    INFORMATION_SEEKING = \"information_seeking\"\n    SOCIAL_CHITCHAT = \"social_chitchat\"\n    COMMAND_EXECUTION = \"command_execution\"\n\n@dataclass\nclass DialogueContext:\n    \"\"\"Context information for the current dialogue\"\"\"\n    user_id: str\n    conversation_type: ConversationType\n    current_topic: Optional[str] = None\n    previous_utterances: List[Dict[str, str]] = None\n    task_stack: List[Dict[str, Any]] = None\n    user_preferences: Dict[str, Any] = None\n    session_start_time: float = 0.0\n    last_activity_time: float = 0.0\n\nclass DialogueManager:\n    \"\"\"Manages conversational flow and dialogue state\"\"\"\n    def __init__(self):\n        self.current_state = DialogueState.IDLE\n        self.context = None\n        self.state_handlers = self.initialize_state_handlers()\n        self.response_templates = self.initialize_response_templates()\n        self.conversation_history = []\n        self.max_history_length = 50\n\n    def initialize_state_handlers(self):\n        \"\"\"Initialize handlers for different dialogue states\"\"\"\n        return {\n            DialogueState.GREETING: self.handle_greeting_state,\n            DialogueState.LISTENING: self.handle_listening_state,\n            DialogueState.PROCESSING: self.handle_processing_state,\n            DialogueState.RESPONDING: self.handle_responding_state,\n            DialogueState.CONFIRMING: self.handle_confirming_state,\n            DialogueState.HANDLING_ERROR: self.handle_error_state\n        }\n\n    def initialize_response_templates(self):\n        \"\"\"Initialize response templates for different situations\"\"\"\n        return {\n            'greeting': [\n                \"Hello! How can I assist you today?\",\n                \"Hi there! What can I do for you?\",\n                \"Good day! How may I help you?\"\n            ],\n            'confirmation': [\n                \"I'll {action} for you. Is that correct?\",\n                \"So you want me to {action}. Should I proceed?\",\n                \"I understand you want {action}. Is this right?\"\n            ],\n            'error': [\n                \"I'm sorry, I didn't understand that.\",\n                \"Could you please rephrase that?\",\n                \"I'm having trouble understanding. Could you try again?\"\n            ],\n            'task_complete': [\n                \"I've completed {task}.\",\n                \"Task {task} is done.\",\n                \"{task} has been completed successfully.\"\n            ]\n        }\n\n    def start_conversation(self, user_id: str, conversation_type: ConversationType = ConversationType.TASK_ORIENTED):\n        \"\"\"Start a new conversation with a user\"\"\"\n        self.context = DialogueContext(\n            user_id=user_id,\n            conversation_type=conversation_type,\n            previous_utterances=[],\n            task_stack=[],\n            user_preferences={},\n            session_start_time=time.time(),\n            last_activity_time=time.time()\n        )\n\n        self.current_state = DialogueState.GREETING\n        return self.generate_greeting()\n\n    def process_user_input(self, user_input: str) -> Dict[str, Any]:\n        \"\"\"Process user input and generate response\"\"\"\n        if not self.context:\n            # Start default conversation\n            self.start_conversation(\"default_user\")\n\n        self.context.last_activity_time = time.time()\n\n        # Add user input to history\n        self.add_to_conversation_history(\"user\", user_input)\n\n        # Process based on current state\n        response = self.state_handlers[self.current_state](user_input)\n\n        # Add system response to history\n        self.add_to_conversation_history(\"system\", response.get('text', ''))\n\n        return response\n\n    def handle_greeting_state(self, user_input: str) -> Dict[str, Any]:\n        \"\"\"Handle the greeting state\"\"\"\n        response = {\n            'text': self.generate_greeting(),\n            'state': DialogueState.LISTENING,\n            'action': 'greeting'\n        }\n        self.current_state = DialogueState.LISTENING\n        return response\n\n    def handle_listening_state(self, user_input: str) -> Dict[str, Any]:\n        \"\"\"Handle the listening state - analyze user input\"\"\"\n        # Analyze the user input to determine next state\n        analysis = self.analyze_user_input(user_input)\n\n        if analysis['intent'] == 'greeting':\n            response = {\n                'text': \"Hello! How can I help you?\",\n                'state': DialogueState.LISTENING,\n                'action': 'greeting_response'\n            }\n        elif analysis['intent'] == 'task_request':\n            # Push task onto stack\n            task = {\n                'type': 'task',\n                'description': user_input,\n                'parameters': analysis.get('parameters', {}),\n                'status': 'pending'\n            }\n            self.context.task_stack.append(task)\n\n            response = {\n                'text': f\"I can help with that. You want me to {user_input.lower()}. Is that correct?\",\n                'state': DialogueState.CONFIRMING,\n                'action': 'task_confirmation',\n                'task_id': len(self.context.task_stack) - 1\n            }\n            self.current_state = DialogueState.CONFIRMING\n        else:\n            # Default to processing\n            response = {\n                'text': f\"I'll process your request: {user_input}\",\n                'state': DialogueState.PROCESSING,\n                'action': 'processing_request'\n            }\n            self.current_state = DialogueState.PROCESSING\n\n        return response\n\n    def handle_processing_state(self, user_input: str) -> Dict[str, Any]:\n        \"\"\"Handle the processing state\"\"\"\n        # In a real system, this would process the request\n        # For this example, we'll just acknowledge and return to listening\n\n        response = {\n            'text': \"I'm processing your request. This may take a moment.\",\n            'state': DialogueState.RESPONDING,\n            'action': 'processing_acknowledgment'\n        }\n        self.current_state = DialogueState.RESPONDING\n        return response\n\n    def handle_responding_state(self, user_input: str) -> Dict[str, Any]:\n        \"\"\"Handle the responding state\"\"\"\n        # Generate response based on context\n        if self.context.task_stack:\n            current_task = self.context.task_stack[-1]\n            if current_task['status'] == 'completed':\n                response_text = self.response_templates['task_complete'][0].format(\n                    task=current_task['description']\n                )\n            else:\n                response_text = \"I've processed your request.\"\n        else:\n            response_text = \"I've handled your request.\"\n\n        response = {\n            'text': response_text,\n            'state': DialogueState.LISTENING,\n            'action': 'task_completion'\n        }\n        self.current_state = DialogueState.LISTENING\n        return response\n\n    def handle_confirming_state(self, user_input: str) -> Dict[str, Any]:\n        \"\"\"Handle the confirming state\"\"\"\n        user_input_lower = user_input.lower()\n\n        if any(word in user_input_lower for word in ['yes', 'yep', 'sure', 'okay', 'correct', 'right']):\n            # User confirmed, proceed with task\n            if self.context.task_stack:\n                task = self.context.task_stack[-1]\n                task['status'] = 'confirmed'\n\n                response = {\n                    'text': f\"Great! I'll work on {task['description']} now.\",\n                    'state': DialogueState.PROCESSING,\n                    'action': 'task_execution',\n                    'execute_task': True\n                }\n                self.current_state = DialogueState.PROCESSING\n            else:\n                response = {\n                    'text': \"I'm ready to help. What would you like me to do?\",\n                    'state': DialogueState.LISTENING,\n                    'action': 'request_task'\n                }\n                self.current_state = DialogueState.LISTENING\n        elif any(word in user_input_lower for word in ['no', 'nope', 'wrong', 'cancel', 'stop']):\n            # User rejected, cancel task\n            if self.context.task_stack:\n                cancelled_task = self.context.task_stack.pop()\n\n                response = {\n                    'text': f\"I've cancelled the task: {cancelled_task['description']}. What else can I help with?\",\n                    'state': DialogueState.LISTENING,\n                    'action': 'task_cancelled'\n                }\n            else:\n                response = {\n                    'text': \"Okay, what would you like to do instead?\",\n                    'state': DialogueState.LISTENING,\n                    'action': 'request_alternative'\n                }\n            self.current_state = DialogueState.LISTENING\n        else:\n            # Unclear response, ask for clarification\n            response = {\n                'text': \"I'm not sure if you confirmed or rejected the task. Please say yes or no.\",\n                'state': DialogueState.CONFIRMING,\n                'action': 'request_clarification'\n            }\n            # Stay in confirming state\n\n        return response\n\n    def handle_error_state(self, user_input: str) -> Dict[str, Any]:\n        \"\"\"Handle the error state\"\"\"\n        response = {\n            'text': self.response_templates['error'][0],\n            'state': DialogueState.LISTENING,\n            'action': 'error_recovery'\n        }\n        self.current_state = DialogueState.LISTENING\n        return response\n\n    def analyze_user_input(self, user_input: str) -> Dict[str, Any]:\n        \"\"\"Analyze user input to determine intent and parameters\"\"\"\n        user_lower = user_input.lower()\n\n        analysis = {\n            'intent': 'unknown',\n            'confidence': 0.0,\n            'parameters': {}\n        }\n\n        # Intent classification\n        if any(greeting in user_lower for greeting in ['hello', 'hi', 'hey', 'good morning', 'good afternoon', 'good evening']):\n            analysis['intent'] = 'greeting'\n            analysis['confidence'] = 0.9\n        elif any(task_word in user_lower for task_word in ['go', 'navigate', 'move', 'walk', 'head', 'take', 'grasp', 'pick up', 'get']):\n            analysis['intent'] = 'task_request'\n            analysis['confidence'] = 0.8\n\n            # Extract parameters\n            # Simple parameter extraction\n            import re\n            destination_match = re.search(r'to (\\w+)', user_lower)\n            if destination_match:\n                analysis['parameters']['destination'] = destination_match.group(1)\n\n            object_match = re.search(r'(?:grasp|pick up|take) (\\w+)', user_lower)\n            if object_match:\n                analysis['parameters']['object'] = object_match.group(1)\n        else:\n            analysis['intent'] = 'general_request'\n            analysis['confidence'] = 0.6\n\n        return analysis\n\n    def generate_greeting(self) -> str:\n        \"\"\"Generate an appropriate greeting\"\"\"\n        import random\n        return random.choice(self.response_templates['greeting'])\n\n    def add_to_conversation_history(self, speaker: str, text: str):\n        \"\"\"Add an utterance to the conversation history\"\"\"\n        utterance = {\n            'speaker': speaker,\n            'text': text,\n            'timestamp': time.time()\n        }\n\n        self.conversation_history.append(utterance)\n\n        # Keep history size manageable\n        if len(self.conversation_history) > self.max_history_length:\n            self.conversation_history = self.conversation_history[-self.max_history_length:]\n\n    def get_conversation_context(self, num_utterances: int = 3) -> List[Dict[str, str]]:\n        \"\"\"Get recent conversation context\"\"\"\n        return self.conversation_history[-num_utterances:]\n\n    def reset_conversation(self):\n        \"\"\"Reset the conversation state\"\"\"\n        self.current_state = DialogueState.IDLE\n        self.context = None\n        self.conversation_history = []\n\nclass ContextualDialogueManager(DialogueManager):\n    \"\"\"Enhanced dialogue manager with context awareness\"\"\"\n    def __init__(self):\n        super().__init__()\n        self.user_models = {}  # Store user-specific models\n        self.topic_tracker = TopicTracker()\n        self.follow_up_manager = FollowUpManager()\n\n    def process_user_input(self, user_input: str, user_id: str = \"default\") -> Dict[str, Any]:\n        \"\"\"Enhanced input processing with user context\"\"\"\n        # Load or create user model\n        if user_id not in self.user_models:\n            self.user_models[user_id] = UserModel(user_id)\n\n        user_model = self.user_models[user_id]\n\n        # Update user model with new input\n        user_model.update_with_input(user_input)\n\n        # Track topics\n        new_topics = self.topic_tracker.extract_topics(user_input)\n        self.topic_tracker.update_context(new_topics)\n\n        # Handle follow-ups\n        follow_up_response = self.follow_up_manager.check_for_follow_up(user_input, self.conversation_history)\n        if follow_up_response:\n            return follow_up_response\n\n        # Process with enhanced context\n        if not self.context or self.context.user_id != user_id:\n            self.start_conversation(user_id, ConversationType.TASK_ORIENTED)\n\n        self.context.last_activity_time = time.time()\n\n        # Add user input to history\n        self.add_to_conversation_history(\"user\", user_input)\n\n        # Process based on current state with user context\n        response = self.state_handlers[self.current_state](user_input)\n\n        # Personalize response based on user model\n        response['text'] = self.personalize_response(response['text'], user_model)\n\n        # Add system response to history\n        self.add_to_conversation_history(\"system\", response.get('text', ''))\n\n        return response\n\n    def personalize_response(self, response: str, user_model: 'UserModel') -> str:\n        \"\"\"Personalize response based on user model\"\"\"\n        # Simple personalization\n        if user_model.preferred_greeting:\n            response = response.replace(\"Hello\", user_model.preferred_greeting)\n\n        # Add user-specific information\n        if user_model.last_interaction_time:\n            time_diff = time.time() - user_model.last_interaction_time\n            if time_diff > 3600:  # More than an hour\n                response = f\"It's good to see you again! {response}\"\n\n        return response\n\nclass TopicTracker:\n    \"\"\"Tracks and manages conversation topics\"\"\"\n    def __init__(self):\n        self.current_topics = []\n        self.topic_history = []\n        self.topic_keywords = {\n            'navigation': ['go', 'move', 'navigate', 'walk', 'to', 'toward', 'kitchen', 'bedroom', 'office'],\n            'manipulation': ['grasp', 'pick', 'take', 'hold', 'cup', 'object', 'grab', 'lift'],\n            'time': ['time', 'date', 'when', 'now', 'schedule', 'calendar'],\n            'weather': ['weather', 'temperature', 'rain', 'sunny', 'hot', 'cold'],\n            'greeting': ['hello', 'hi', 'good', 'morning', 'afternoon', 'evening', 'hey']\n        }\n\n    def extract_topics(self, text: str) -> List[str]:\n        \"\"\"Extract topics from text\"\"\"\n        text_lower = text.lower()\n        detected_topics = []\n\n        for topic, keywords in self.topic_keywords.items():\n            if any(keyword in text_lower for keyword in keywords):\n                detected_topics.append(topic)\n\n        return detected_topics\n\n    def update_context(self, new_topics: List[str]):\n        \"\"\"Update topic context\"\"\"\n        self.current_topics = new_topics\n        self.topic_history.extend(new_topics)\n\nclass FollowUpManager:\n    \"\"\"Manages follow-up questions and responses\"\"\"\n    def __init__(self):\n        self.last_utterance = \"\"\n        self.follow_up_patterns = {\n            r'do (?:you|I) need to.*': 'request_clarification',\n            r'what.*next': 'request_next_step',\n            r'how.*do.*that': 'request_explanation',\n            r'can you.*': 'request_capability',\n        }\n\n    def check_for_follow_up(self, current_input: str, history: List[Dict]) -> Optional[Dict[str, Any]]:\n        \"\"\"Check if current input is a follow-up to previous utterances\"\"\"\n        if not history:\n            return None\n\n        # Check for follow-up patterns\n        for pattern, intent in self.follow_up_patterns.items():\n            if re.search(pattern, current_input.lower()):\n                # This is a follow-up question\n                return {\n                    'text': self.generate_follow_up_response(intent, history[-1]),\n                    'state': DialogueState.LISTENING,\n                    'action': 'follow_up_response'\n                }\n\n        return None\n\n    def generate_follow_up_response(self, intent: str, previous_utterance: Dict) -> str:\n        \"\"\"Generate appropriate response for follow-up\"\"\"\n        responses = {\n            'request_clarification': \"Let me clarify: I can help with navigation, object manipulation, and information retrieval.\",\n            'request_next_step': \"The next step would be to execute the task I described.\",\n            'request_explanation': \"I can explain how I plan to complete this task.\",\n            'request_capability': \"Yes, I can perform various tasks including navigation and manipulation.\"\n        }\n        return responses.get(intent, \"I can help you with that.\")\n\nclass UserModel:\n    \"\"\"Model of a specific user for personalization\"\"\"\n    def __init__(self, user_id: str):\n        self.user_id = user_id\n        self.preferred_greeting = None\n        self.last_interaction_time = None\n        self.conversation_style = 'formal'  # or 'casual'\n        self.topic_preferences = []\n        self.response_preferences = {\n            'verbosity': 'medium',  # 'brief', 'medium', 'detailed'\n            'tone': 'helpful'      # 'formal', 'friendly', 'helpful'\n        }\n\n    def update_with_input(self, user_input: str):\n        \"\"\"Update user model based on new input\"\"\"\n        self.last_interaction_time = time.time()\n\n        # Learn preferences from input\n        if user_input.lower().startswith('hey') or user_input.lower().startswith('hi'):\n            self.conversation_style = 'casual'\n        elif user_input.lower().startswith('hello') or user_input.lower().startswith('good'):\n            self.conversation_style = 'formal'\n\n# Example usage\ndef example_dialogue_management():\n    print(\"Dialogue Management Example\")\n\n    # Initialize dialogue manager\n    dialogue_manager = ContextualDialogueManager()\n\n    # Simulate a conversation\n    conversation = [\n        \"Hello robot\",\n        \"Navigate to the kitchen\",\n        \"Yes, that's correct\",\n        \"Grasp the red cup\",\n        \"No, I meant the blue one\",\n        \"What time is it?\",\n        \"Thank you\"\n    ]\n\n    print(\"Simulating conversation:\")\n    for i, user_input in enumerate(conversation):\n        print(f\"\\n{i+1}. User: {user_input}\")\n\n        response = dialogue_manager.process_user_input(user_input, f\"user_{i%3}\")\n        print(f\"   Robot: {response['text']}\")\n        print(f\"   State: {response['state'].value}\")\n        print(f\"   Action: {response['action']}\")\n\nif __name__ == \"__main__\":\n    example_dialogue_management()\n"})}),"\n",(0,i.jsx)(n.h2,{id:"knowledge-check",children:"Knowledge Check"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsx)(n.li,{children:"What are the key components of a speech recognition system for robotics?"}),"\n",(0,i.jsx)(n.li,{children:"How does natural language understanding differ from speech recognition?"}),"\n",(0,i.jsx)(n.li,{children:"What are the benefits of multi-modal interaction in robotics?"}),"\n",(0,i.jsx)(n.li,{children:"How does dialogue management contribute to natural conversation flow?"}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"summary",children:"Summary"}),"\n",(0,i.jsx)(n.p,{children:"This chapter covered speech recognition and natural language understanding for humanoid robots. We explored voice command processing systems, natural language understanding engines, multi-modal interaction techniques, and dialogue management systems. The chapter provided practical implementations for processing speech input, understanding user intent, combining multiple modalities, and managing conversational flow for natural human-robot interaction."}),"\n",(0,i.jsx)(n.h2,{id:"next-steps",children:"Next Steps"}),"\n",(0,i.jsx)(n.p,{children:"In the next chapter, we'll explore cognitive planning with LLMs, covering how to use Large Language Models to translate natural language commands into executable robotic actions and create sophisticated task planning systems for humanoid robots."})]})}function u(e={}){const{wrapper:n}={...(0,s.R)(),...e.components};return n?(0,i.jsx)(n,{...e,children:(0,i.jsx)(d,{...e})}):d(e)}},8453(e,n,t){t.d(n,{R:()=>o,x:()=>a});var i=t(6540);const s={},r=i.createContext(s);function o(e){const n=i.useContext(r);return i.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:o(e.components),i.createElement(r.Provider,{value:n},e.children)}}}]);