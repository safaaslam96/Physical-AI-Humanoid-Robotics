"use strict";(globalThis.webpackChunkphysical_ai_humanoid_robotics_book=globalThis.webpackChunkphysical_ai_humanoid_robotics_book||[]).push([[785],{8453(e,n,t){t.d(n,{R:()=>o,x:()=>s});var a=t(6540);const i={},r=a.createContext(i);function o(e){const n=a.useContext(r);return a.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function s(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:o(e.components),a.createElement(r.Provider,{value:n},e.children)}},8920(e,n,t){t.r(n),t.d(n,{assets:()=>l,contentTitle:()=>o,default:()=>m,frontMatter:()=>r,metadata:()=>s,toc:()=>c});var a=t(4848),i=t(8453);const r={sidebar_position:9,title:"Chapter 9: NVIDIA Isaac SDK and Isaac Sim"},o="Chapter 9: NVIDIA Isaac SDK and Isaac Sim",s={id:"part4/chapter9",title:"Chapter 9: NVIDIA Isaac SDK and Isaac Sim",description:"Learning Objectives",source:"@site/docs/part4/chapter9.md",sourceDirName:"part4",slug:"/part4/chapter9",permalink:"/Physical-AI-Humanoid-Robotics/docs/part4/chapter9",draft:!1,unlisted:!1,editUrl:"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/part4/chapter9.md",tags:[],version:"current",sidebarPosition:9,frontMatter:{sidebar_position:9,title:"Chapter 9: NVIDIA Isaac SDK and Isaac Sim"},sidebar:"tutorialSidebar",previous:{title:"Chapter 8: Unity Visualization and Sensor Simulation",permalink:"/Physical-AI-Humanoid-Robotics/docs/part3/chapter8"},next:{title:"Chapter 10: Isaac ROS and Hardware-Accelerated Perception",permalink:"/Physical-AI-Humanoid-Robotics/docs/part4/chapter10"}},l={},c=[{value:"Learning Objectives",id:"learning-objectives",level:2},{value:"Introduction to NVIDIA Isaac Platform",id:"introduction-to-nvidia-isaac-platform",level:2},{value:"Isaac Platform Components",id:"isaac-platform-components",level:3},{value:"Hardware Requirements",id:"hardware-requirements",level:3},{value:"Isaac Sim Overview",id:"isaac-sim-overview",level:2},{value:"Installing Isaac Sim",id:"installing-isaac-sim",level:3},{value:"Basic Isaac Sim Concepts",id:"basic-isaac-sim-concepts",level:3},{value:"Photorealistic Simulation",id:"photorealistic-simulation",level:2},{value:"Lighting and Materials",id:"lighting-and-materials",level:3},{value:"Camera Configuration for Synthetic Data",id:"camera-configuration-for-synthetic-data",level:3},{value:"Synthetic Data Generation",id:"synthetic-data-generation",level:2},{value:"Creating Diverse Training Datasets",id:"creating-diverse-training-datasets",level:3},{value:"Domain Randomization",id:"domain-randomization",level:3},{value:"AI-Powered Perception Systems",id:"ai-powered-perception-systems",level:2},{value:"Visual Perception Pipeline",id:"visual-perception-pipeline",level:3},{value:"Manipulation Perception",id:"manipulation-perception",level:3},{value:"Isaac ROS Integration",id:"isaac-ros-integration",level:2},{value:"Hardware-Accelerated Perception",id:"hardware-accelerated-perception",level:3},{value:"Isaac Gym for Reinforcement Learning",id:"isaac-gym-for-reinforcement-learning",level:2},{value:"GPU-Accelerated Training Environment",id:"gpu-accelerated-training-environment",level:3},{value:"Performance Optimization",id:"performance-optimization",level:2},{value:"GPU Acceleration Best Practices",id:"gpu-acceleration-best-practices",level:3},{value:"Troubleshooting and Best Practices",id:"troubleshooting-and-best-practices",level:2},{value:"Common Issues and Solutions",id:"common-issues-and-solutions",level:3},{value:"Knowledge Check",id:"knowledge-check",level:2},{value:"Summary",id:"summary",level:2},{value:"Next Steps",id:"next-steps",level:2}];function d(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,i.R)(),...e.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(n.h1,{id:"chapter-9-nvidia-isaac-sdk-and-isaac-sim",children:"Chapter 9: NVIDIA Isaac SDK and Isaac Sim"}),"\n",(0,a.jsx)(n.h2,{id:"learning-objectives",children:"Learning Objectives"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"Understand the NVIDIA Isaac platform ecosystem"}),"\n",(0,a.jsx)(n.li,{children:"Set up Isaac Sim for photorealistic simulation"}),"\n",(0,a.jsx)(n.li,{children:"Generate synthetic data for AI training"}),"\n",(0,a.jsx)(n.li,{children:"Implement AI-powered perception and manipulation systems"}),"\n"]}),"\n",(0,a.jsx)(n.h2,{id:"introduction-to-nvidia-isaac-platform",children:"Introduction to NVIDIA Isaac Platform"}),"\n",(0,a.jsx)(n.p,{children:"The NVIDIA Isaac platform is a comprehensive solution for developing, simulating, and deploying AI-powered robots. It combines hardware acceleration with software tools to create end-to-end robotics solutions, particularly excelling in perception, navigation, and manipulation tasks for humanoid robots."}),"\n",(0,a.jsx)(n.h3,{id:"isaac-platform-components",children:"Isaac Platform Components"}),"\n",(0,a.jsx)(n.p,{children:"The Isaac platform consists of several interconnected components:"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Isaac Sim"}),": High-fidelity physics and rendering simulation environment"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Isaac ROS"}),": Hardware-accelerated ROS 2 packages for perception and navigation"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Isaac SDK"}),": Software development kit with perception and manipulation libraries"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Isaac Apps"}),": Pre-built applications for common robotics tasks"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Isaac Gym"}),": GPU-accelerated reinforcement learning environment"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Omniverse"}),": 3D design collaboration and simulation platform"]}),"\n"]}),"\n",(0,a.jsx)(n.h3,{id:"hardware-requirements",children:"Hardware Requirements"}),"\n",(0,a.jsx)(n.p,{children:"To fully utilize the Isaac platform:"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"GPU"}),": NVIDIA RTX 4090, A6000, or similar professional GPU"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"VRAM"}),": 24GB+ recommended for high-fidelity simulation"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"CPU"}),": Multi-core processor (8+ cores recommended)"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"RAM"}),": 32GB+ for complex scenes"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Storage"}),": SSD with 100GB+ free space"]}),"\n"]}),"\n",(0,a.jsx)(n.h2,{id:"isaac-sim-overview",children:"Isaac Sim Overview"}),"\n",(0,a.jsx)(n.p,{children:"Isaac Sim is built on NVIDIA's Omniverse platform and provides:"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Photorealistic Rendering"}),": RTX-accelerated ray tracing"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"High-Fidelity Physics"}),": PhysX 5.0 physics engine"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Synthetic Data Generation"}),": Large-scale dataset creation"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Multi-Robot Simulation"}),": Support for complex multi-robot scenarios"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"ROS 2 Integration"}),": Native ROS 2 bridge for seamless integration"]}),"\n"]}),"\n",(0,a.jsx)(n.h3,{id:"installing-isaac-sim",children:"Installing Isaac Sim"}),"\n",(0,a.jsxs)(n.ol,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Prerequisites"}),":"]}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"NVIDIA GPU with CUDA support"}),"\n",(0,a.jsx)(n.li,{children:"NVIDIA Omniverse Launcher"}),"\n",(0,a.jsx)(n.li,{children:"Compatible graphics drivers"}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Installation Steps"}),":"]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-bash",children:"# Download and install Omniverse Launcher\n# Launch Isaac Sim through Omniverse\n# Or install via pip for development:\npip install omni.isaac.sim\n"})}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Verification"}),":"]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-bash",children:'# Launch Isaac Sim\npython -m omni.isaac.kit --exec "standalone_examples/api/omni_isaac_core/hello_world.py"\n'})}),"\n"]}),"\n"]}),"\n",(0,a.jsx)(n.h3,{id:"basic-isaac-sim-concepts",children:"Basic Isaac Sim Concepts"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'# Basic Isaac Sim example\nimport omni\nfrom omni.isaac.core import World\nfrom omni.isaac.core.utils.stage import add_reference_to_stage\nfrom omni.isaac.core.utils.prims import get_prim_at_path\nfrom omni.isaac.core.robots import Robot\nfrom omni.isaac.core.utils.nucleus import get_assets_root_path\n\n# Create the world\nworld = World(stage_units_in_meters=1.0)\n\n# Add a robot to the stage\nassets_root_path = get_assets_root_path()\nif assets_root_path is None:\n    print("Could not find Isaac Sim assets. Please enable Isaac Sim Nucleus on Omniverse Launcher.")\nelse:\n    # Add a Franka robot (example - replace with humanoid robot)\n    add_reference_to_stage(\n        usd_path=assets_root_path + "/Isaac/Robots/Franka/franka.usd",\n        prim_path="/World/Franka"\n    )\n\n    # Create robot object\n    robot = world.scene.add(\n        Robot(\n            prim_path="/World/Franka",\n            name="franka_robot",\n            position=[0, 0, 0],\n            orientation=[0, 0, 0, 1]\n        )\n    )\n\n# Reset and step the world\nworld.reset()\nfor i in range(100):\n    world.step(render=True)\n\n# Cleanup\nworld.clear()\n'})}),"\n",(0,a.jsx)(n.h2,{id:"photorealistic-simulation",children:"Photorealistic Simulation"}),"\n",(0,a.jsx)(n.h3,{id:"lighting-and-materials",children:"Lighting and Materials"}),"\n",(0,a.jsx)(n.p,{children:"Isaac Sim provides advanced lighting and material systems for photorealistic rendering:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'import omni\nfrom pxr import UsdLux, UsdGeom, Gf, Sdf\nfrom omni.isaac.core.utils.prims import define_prim\nfrom omni.isaac.core.utils.stage import get_current_stage\n\ndef setup_photorealistic_scene():\n    stage = get_current_stage()\n\n    # Add dome light for environment lighting\n    dome_light = UsdLux.DomeLight.Define(stage, Sdf.Path("/World/DomeLight"))\n    dome_light.CreateIntensityAttr(1000)\n    dome_light.CreateColorAttr(Gf.Vec3f(1.0, 1.0, 1.0))\n\n    # Add directional light (sun)\n    directional_light = UsdLux.DistantLight.Define(stage, Sdf.Path("/World/KeyLight"))\n    directional_light.CreateIntensityAttr(3000)\n    directional_light.CreateColorAttr(Gf.Vec3f(1.0, 0.95, 0.9))\n    directional_light.AddRotateYOp().Set(45)\n    directional_light.AddRotateXOp().Set(-30)\n\n    # Add environment texture (HDRI)\n    dome_light.CreateTextureFileAttr().Set("path/to/hdri_texture.exr")\n\n    # Configure materials\n    setup_materials()\n\ndef setup_materials():\n    # Create physically-based materials\n    stage = get_current_stage()\n\n    # Plastic material\n    plastic_material_path = Sdf.Path("/World/Looks/PlasticMaterial")\n    plastic_material = define_prim(plastic_material_path, "Material")\n\n    # Add USD Preview Surface shader\n    shader_path = plastic_material_path.AppendChild("Shader")\n    shader = define_prim(shader_path, "Shader")\n    shader.GetPrim().GetAttribute("info:id").Set("UsdPreviewSurface")\n\n    # Set material properties\n    shader.GetPrim().GetAttribute("inputs:diffuseColor").Set(Gf.Vec3f(0.8, 0.1, 0.1))  # Red plastic\n    shader.GetPrim().GetAttribute("inputs:metallic").Set(0.0)\n    shader.GetPrim().GetAttribute("inputs:roughness").Set(0.2)\n    shader.GetPrim().GetAttribute("inputs:clearcoat").Set(0.8)\n    shader.GetPrim().GetAttribute("inputs:clearcoatRoughness").Set(0.1)\n'})}),"\n",(0,a.jsx)(n.h3,{id:"camera-configuration-for-synthetic-data",children:"Camera Configuration for Synthetic Data"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'from omni.isaac.sensor import Camera\nfrom omni.isaac.core.utils.prims import get_prim_at_path\nimport numpy as np\n\ndef setup_synthetic_camera(robot_prim_path, camera_name="rgb_camera"):\n    # Create camera attached to robot\n    camera = Camera(\n        prim_path=f"{robot_prim_path}/head/{camera_name}",\n        frequency=30,  # Hz\n        resolution=(640, 480),\n        position=np.array([0.1, 0.0, 0.1]),  # Offset from head\n        orientation=np.array([0, 0, 0, 1])\n    )\n\n    # Configure camera properties for photorealism\n    camera_config = {\n        "focal_length": 24.0,  # mm\n        "horizontal_aperture": 36.0,  # mm\n        "f_stop": 2.8,\n        "focus_distance": 10.0,\n        "iso": 100,\n        "shutter_speed": 1.0/60.0\n    }\n\n    # Apply configuration\n    for param, value in camera_config.items():\n        camera.param.set(param, value)\n\n    return camera\n\ndef capture_synthetic_data(camera, frame_count=1000):\n    """Generate synthetic dataset"""\n    import cv2\n    import os\n\n    os.makedirs("synthetic_dataset", exist_ok=True)\n\n    for i in range(frame_count):\n        # Render frame\n        rgb_data = camera.get_rgb()\n\n        # Save image\n        img_path = f"synthetic_dataset/frame_{i:06d}.png"\n        cv2.imwrite(img_path, cv2.cvtColor(rgb_data, cv2.COLOR_RGB2BGR))\n\n        # Capture additional data (depth, segmentation, etc.)\n        depth_data = camera.get_depth()\n        seg_data = camera.get_semantic_segmentation()\n\n        # Save additional data\n        np.save(f"synthetic_dataset/depth_{i:06d}.npy", depth_data)\n        np.save(f"synthetic_dataset/seg_{i:06d}.npy", seg_data)\n\n        print(f"Captured frame {i+1}/{frame_count}")\n'})}),"\n",(0,a.jsx)(n.h2,{id:"synthetic-data-generation",children:"Synthetic Data Generation"}),"\n",(0,a.jsx)(n.h3,{id:"creating-diverse-training-datasets",children:"Creating Diverse Training Datasets"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'import random\nimport numpy as np\nfrom omni.isaac.core import World\nfrom omni.isaac.core.utils.stage import add_reference_to_stage\nfrom omni.isaac.core.utils.prims import get_prim_at_path\nfrom omni.isaac.core.utils.nucleus import get_assets_root_path\n\nclass SyntheticDataGenerator:\n    def __init__(self, world):\n        self.world = world\n        self.assets_root = get_assets_root_path()\n        self.scene_objects = []\n\n    def setup_diverse_scenes(self):\n        """Create varied environments for synthetic data"""\n        # Different lighting conditions\n        lighting_conditions = [\n            {"intensity": 1000, "color": [1.0, 1.0, 1.0], "type": "dome"},\n            {"intensity": 3000, "color": [1.0, 0.95, 0.9], "type": "directional"},\n            {"intensity": 2000, "color": [0.8, 0.9, 1.0], "type": "dome"}\n        ]\n\n        # Different weather conditions (simulated through materials)\n        weather_conditions = ["sunny", "cloudy", "overcast"]\n\n        # Different times of day\n        times_of_day = ["morning", "noon", "afternoon", "evening"]\n\n        for i, light_config in enumerate(lighting_conditions):\n            self.create_environment(f"env_{i}", light_config)\n\n    def create_environment(self, env_name, light_config):\n        """Create a specific environment with given conditions"""\n        # Create environment prim\n        env_prim_path = f"/World/{env_name}"\n        env_prim = define_prim(env_prim_path, "Xform")\n\n        # Add lighting\n        self.add_lighting(env_prim_path, light_config)\n\n        # Add objects with random positions\n        self.add_random_objects(env_prim_path)\n\n        # Add floor/ground plane\n        self.add_ground_plane(env_prim_path)\n\n    def add_lighting(self, env_path, config):\n        """Add lighting to environment"""\n        stage = get_current_stage()\n\n        if config["type"] == "dome":\n            light_path = f"{env_path}/DomeLight"\n            dome_light = UsdLux.DomeLight.Define(stage, Sdf.Path(light_path))\n            dome_light.CreateIntensityAttr(config["intensity"])\n            dome_light.CreateColorAttr(Gf.Vec3f(*config["color"]))\n        else:\n            light_path = f"{env_path}/DirectionalLight"\n            directional_light = UsdLux.DistantLight.Define(stage, Sdf.Path(light_path))\n            directional_light.CreateIntensityAttr(config["intensity"])\n            directional_light.CreateColorAttr(Gf.Vec3f(*config["color"]))\n\n    def add_random_objects(self, env_path, count=10):\n        """Add random objects to environment"""\n        object_types = [\n            "Isaac/Props/Blocks/block_01_20cm.usd",\n            "Isaac/Props/Kiva/kiva_shelf.usd",\n            "Isaac/Props/TrafficCone/traffic_cone.usd"\n        ]\n\n        for i in range(count):\n            obj_type = random.choice(object_types)\n            obj_path = f"{env_path}/Object_{i}"\n\n            add_reference_to_stage(\n                usd_path=f"{self.assets_root}/{obj_type}",\n                prim_path=obj_path\n            )\n\n            # Random position\n            x = random.uniform(-5, 5)\n            y = random.uniform(-5, 5)\n            z = random.uniform(0, 2)\n\n            # Apply random position\n            prim = get_prim_at_path(obj_path)\n            prim.GetAttribute("xformOp:translate").Set(Gf.Vec3d(x, y, z))\n\n    def add_ground_plane(self, env_path):\n        """Add ground plane to environment"""\n        from pxr import UsdGeom\n        stage = get_current_stage()\n\n        plane_path = f"{env_path}/GroundPlane"\n        plane = UsdGeom.Mesh.Define(stage, plane_path)\n\n        # Configure plane geometry\n        plane.CreatePointsAttr([(-10, -10, 0), (10, -10, 0), (10, 10, 0), (-10, 10, 0)])\n        plane.CreateFaceVertexIndicesAttr([0, 1, 2, 3])\n        plane.CreateFaceVertexCountsAttr([4])\n\n        # Add material\n        self.add_ground_material(plane_path)\n\n    def add_ground_material(self, prim_path):\n        """Add realistic ground material"""\n        stage = get_current_stage()\n        material_path = f"{prim_path}/Material"\n\n        # Create material\n        material = UsdShade.Material.Define(stage, material_path)\n\n        # Create shader\n        shader = UsdShade.Shader.Define(stage, material_path.AppendChild("PreviewSurface"))\n        shader.CreateIdAttr("UsdPreviewSurface")\n\n        # Configure for realistic ground\n        shader.CreateInput("diffuseColor", Sdf.ValueTypeNames.Color3f).Set(\n            Gf.Vec3f(0.5, 0.5, 0.5)\n        )\n        shader.CreateInput("roughness", Sdf.ValueTypeNames.Float).Set(0.8)\n        shader.CreateInput("metallic", Sdf.ValueTypeNames.Float).Set(0.0)\n\n        # Bind material to geometry\n        UsdShade.MaterialBindingAPI(prim_path).Bind(material)\n\n    def generate_dataset(self, robot, camera, output_dir="synthetic_data", num_frames=1000):\n        """Generate synthetic dataset with robot actions"""\n        import os\n        os.makedirs(output_dir, exist_ok=True)\n\n        # Move robot to different positions\n        for frame_idx in range(num_frames):\n            # Random robot movement\n            new_position = [\n                random.uniform(-2, 2),\n                random.uniform(-2, 2),\n                0.5  # Fixed height for humanoid\n            ]\n\n            robot.set_world_pose(position=new_position)\n\n            # Capture data\n            rgb_data = camera.get_rgb()\n            depth_data = camera.get_depth()\n            seg_data = camera.get_semantic_segmentation()\n\n            # Save frame data\n            np.save(f"{output_dir}/rgb_{frame_idx:06d}.npy", rgb_data)\n            np.save(f"{output_dir}/depth_{frame_idx:06d}.npy", depth_data)\n            np.save(f"{output_dir}/seg_{frame_idx:06d}.npy", seg_data)\n\n            # Save robot state\n            pos, quat = robot.get_world_pose()\n            robot_state = {\n                "position": pos,\n                "orientation": quat,\n                "timestamp": frame_idx\n            }\n            np.save(f"{output_dir}/robot_state_{frame_idx:06d}.npy", robot_state)\n\n            print(f"Generated frame {frame_idx+1}/{num_frames}")\n'})}),"\n",(0,a.jsx)(n.h3,{id:"domain-randomization",children:"Domain Randomization"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'class DomainRandomizer:\n    def __init__(self):\n        self.parameters = {\n            "lighting": {\n                "intensity_range": (500, 5000),\n                "color_temperature_range": (3000, 8000),\n                "position_range": ((-10, -10, 5), (10, 10, 15))\n            },\n            "materials": {\n                "roughness_range": (0.1, 0.9),\n                "metallic_range": (0.0, 0.1),\n                "albedo_range": ((0.1, 0.1, 0.1), (1.0, 1.0, 1.0))\n            },\n            "camera": {\n                "position_noise": 0.01,\n                "orientation_noise": 0.01,\n                "focus_noise": 0.1\n            }\n        }\n\n    def randomize_lighting(self):\n        """Randomize lighting conditions"""\n        intensity = random.uniform(*self.parameters["lighting"]["intensity_range"])\n        color_temp = random.uniform(*self.parameters["lighting"]["color_temperature_range"])\n\n        # Convert color temperature to RGB (simplified)\n        rgb = self.color_temperature_to_rgb(color_temp)\n\n        position = [\n            random.uniform(*[r[i] for i in range(3)])\n            for r in self.parameters["lighting"]["position_range"]\n        ]\n\n        return {\n            "intensity": intensity,\n            "color": rgb,\n            "position": position\n        }\n\n    def randomize_materials(self, prim_path):\n        """Randomize material properties"""\n        # Get material prim\n        material_prim = get_prim_at_path(prim_path)\n\n        # Randomize properties\n        roughness = random.uniform(*self.parameters["materials"]["roughness_range"])\n        metallic = random.uniform(*self.parameters["materials"]["metallic_range"])\n\n        min_albedo = self.parameters["materials"]["albedo_range"][0]\n        max_albedo = self.parameters["materials"]["albedo_range"][1]\n        albedo = [\n            random.uniform(min_albedo[i], max_albedo[i])\n            for i in range(3)\n        ]\n\n        # Apply changes\n        material_prim.GetAttribute("inputs:roughness").Set(roughness)\n        material_prim.GetAttribute("inputs:metallic").Set(metallic)\n        material_prim.GetAttribute("inputs:diffuseColor").Set(Gf.Vec3f(*albedo))\n\n    def color_temperature_to_rgb(self, temperature):\n        """Convert color temperature to RGB (approximate)"""\n        temperature = temperature / 100\n        if temperature <= 66:\n            red = 255\n            green = temperature\n            green = 99.4708025861 * math.log(green) - 161.1195681661\n        else:\n            red = temperature - 60\n            red = 329.698727446 * (red ** -0.1332047592)\n            green = temperature - 60\n            green = 288.1221695283 * (green ** -0.0755148492)\n\n        blue = temperature - 10\n        if temperature >= 66:\n            blue = 138.5177312231 * math.log(blue) - 305.0447927307\n        else:\n            blue = 0\n\n        return [max(0, min(255, x)) / 255.0 for x in [red, green, blue]]\n\n    def randomize_camera(self, camera):\n        """Add noise to camera parameters"""\n        # Add small random offsets\n        pos_noise = np.random.normal(0, self.parameters["camera"]["position_noise"], 3)\n        rot_noise = np.random.normal(0, self.parameters["camera"]["orientation_noise"], 4)\n\n        # Apply noise (simplified)\n        current_pos = camera.get_position()\n        camera.set_position(current_pos + pos_noise)\n'})}),"\n",(0,a.jsx)(n.h2,{id:"ai-powered-perception-systems",children:"AI-Powered Perception Systems"}),"\n",(0,a.jsx)(n.h3,{id:"visual-perception-pipeline",children:"Visual Perception Pipeline"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"import torch\nimport torchvision.transforms as transforms\nfrom omni.isaac.core import World\nfrom omni.isaac.sensor import Camera\nimport cv2\n\nclass IsaacPerceptionPipeline:\n    def __init__(self, robot, camera):\n        self.robot = robot\n        self.camera = camera\n\n        # Initialize perception models\n        self.object_detector = self.load_object_detector()\n        self.pose_estimator = self.load_pose_estimator()\n        self.depth_estimator = self.load_depth_estimator()\n\n        # Image preprocessing\n        self.transform = transforms.Compose([\n            transforms.ToTensor(),\n            transforms.Normalize(mean=[0.485, 0.456, 0.406],\n                               std=[0.229, 0.224, 0.225])\n        ])\n\n    def load_object_detector(self):\n        \"\"\"Load pre-trained object detection model\"\"\"\n        # Using a pre-trained model like YOLO or Detectron2\n        import torchvision\n        model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n        model.eval()\n        return model\n\n    def load_pose_estimator(self):\n        \"\"\"Load 6D pose estimation model\"\"\"\n        # Placeholder for pose estimation model\n        # Could be DeepIM, PVNet, etc.\n        class PoseEstimator:\n            def estimate(self, image, object_class):\n                # Return rotation matrix and translation vector\n                return np.eye(3), np.zeros(3)\n        return PoseEstimator()\n\n    def load_depth_estimator(self):\n        \"\"\"Load monocular depth estimation model\"\"\"\n        # Using MiDaS or similar\n        import torch\n        model = torch.hub.load(\"intel-isl/MiDaS\", \"DPT_Large\", pretrained=True)\n        model.eval()\n        return model\n\n    def process_camera_data(self):\n        \"\"\"Process camera data through perception pipeline\"\"\"\n        # Get RGB image from Isaac Sim\n        rgb_image = self.camera.get_rgb()\n\n        # Convert to tensor\n        input_tensor = self.transform(rgb_image).unsqueeze(0)\n\n        # Object detection\n        with torch.no_grad():\n            detections = self.object_detector(input_tensor)\n\n        # Process detections\n        processed_detections = self.process_detections(detections, rgb_image.shape)\n\n        # Depth estimation\n        depth_map = self.estimate_depth(rgb_image)\n\n        # Pose estimation for detected objects\n        for detection in processed_detections:\n            pose = self.estimate_pose(rgb_image, detection)\n            detection[\"pose\"] = pose\n\n        return {\n            \"detections\": processed_detections,\n            \"depth_map\": depth_map,\n            \"camera_pose\": self.get_camera_world_pose()\n        }\n\n    def process_detections(self, detections, image_shape):\n        \"\"\"Process raw detections into usable format\"\"\"\n        processed = []\n\n        for i in range(len(detections[0][\"boxes\"])):\n            box = detections[0][\"boxes\"][i].cpu().numpy()\n            score = detections[0][\"scores\"][i].cpu().item()\n            label = detections[0][\"labels\"][i].cpu().item()\n\n            # Convert to image coordinates\n            h, w = image_shape[:2]\n            x1, y1, x2, y2 = box\n            x1, y1, x2, y2 = int(x1), int(y1), int(x2), int(y2)\n\n            # Filter by confidence\n            if score > 0.5:\n                processed.append({\n                    \"bbox\": [x1, y1, x2, y2],\n                    \"confidence\": score,\n                    \"class_id\": label,\n                    \"class_name\": self.coco_id_to_name(label)\n                })\n\n        return processed\n\n    def estimate_depth(self, rgb_image):\n        \"\"\"Estimate depth from RGB image\"\"\"\n        # Preprocess image for depth model\n        depth_input = cv2.resize(rgb_image, (384, 384))\n        depth_input = torch.tensor(depth_input).permute(2, 0, 1).unsqueeze(0).float() / 255.0\n\n        with torch.no_grad():\n            depth_output = self.depth_estimator(depth_input)\n\n        # Resize back to original\n        depth_map = cv2.resize(depth_output.squeeze().cpu().numpy(),\n                              (rgb_image.shape[1], rgb_image.shape[0]))\n\n        return depth_map\n\n    def estimate_pose(self, image, detection):\n        \"\"\"Estimate 6D pose of detected object\"\"\"\n        bbox = detection[\"bbox\"]\n        cropped_image = image[bbox[1]:bbox[3], bbox[0]:bbox[2]]\n\n        pose = self.pose_estimator.estimate(cropped_image, detection[\"class_name\"])\n        return pose\n\n    def get_camera_world_pose(self):\n        \"\"\"Get camera pose in world coordinates\"\"\"\n        # Get robot pose and camera offset\n        robot_pos, robot_quat = self.robot.get_world_pose()\n        camera_offset = self.camera.get_position()  # Relative to robot\n\n        # Calculate world pose\n        world_pos = robot_pos + camera_offset\n        world_quat = robot_quat  # Simplified - actual calculation depends on robot orientation\n\n        return {\"position\": world_pos, \"orientation\": world_quat}\n\n    def coco_id_to_name(self, class_id):\n        \"\"\"Convert COCO class ID to name\"\"\"\n        coco_names = {\n            1: 'person', 2: 'bicycle', 3: 'car', 4: 'motorcycle', 5: 'airplane',\n            6: 'bus', 7: 'train', 8: 'truck', 9: 'boat', 10: 'traffic light',\n            11: 'fire hydrant', 13: 'stop sign', 14: 'parking meter', 15: 'bench',\n            16: 'bird', 17: 'cat', 18: 'dog', 19: 'horse', 20: 'sheep',\n            21: 'cow', 22: 'elephant', 23: 'bear', 24: 'zebra', 25: 'giraffe',\n            27: 'backpack', 28: 'umbrella', 31: 'handbag', 32: 'tie', 33: 'suitcase',\n            34: 'frisbee', 35: 'skis', 36: 'snowboard', 37: 'sports ball', 38: 'kite',\n            39: 'baseball bat', 40: 'baseball glove', 41: 'skateboard', 42: 'surfboard',\n            43: 'tennis racket', 44: 'bottle', 46: 'wine glass', 47: 'cup', 48: 'fork',\n            49: 'knife', 50: 'spoon', 51: 'bowl', 52: 'banana', 53: 'apple',\n            54: 'sandwich', 55: 'orange', 56: 'broccoli', 57: 'carrot', 58: 'hot dog',\n            59: 'pizza', 60: 'donut', 61: 'cake', 62: 'chair', 63: 'couch',\n            64: 'potted plant', 65: 'bed', 67: 'dining table', 70: 'toilet', 72: 'tv',\n            73: 'laptop', 74: 'mouse', 75: 'remote', 76: 'keyboard', 77: 'cell phone',\n            78: 'microwave', 79: 'oven', 80: 'toaster', 81: 'sink', 82: 'refrigerator',\n            84: 'book', 85: 'clock', 86: 'vase', 87: 'scissors', 88: 'teddy bear',\n            89: 'hair drier', 90: 'toothbrush'\n        }\n        return coco_names.get(class_id, f\"unknown_{class_id}\")\n"})}),"\n",(0,a.jsx)(n.h3,{id:"manipulation-perception",children:"Manipulation Perception"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'class ManipulationPerception:\n    def __init__(self, robot, camera):\n        self.robot = robot\n        self.camera = camera\n\n        # Grasp detection model\n        self.grasp_detector = self.load_grasp_detector()\n\n        # Surface normal estimation\n        self.surface_estimator = self.load_surface_estimator()\n\n    def load_grasp_detector(self):\n        """Load grasp detection model"""\n        # Placeholder for grasp detection model\n        # Could be based on Dex-Net, FC-GQ-CNN, etc.\n        class GraspDetector:\n            def detect_grasps(self, depth_image, rgb_image):\n                # Return list of grasp candidates\n                # Each grasp: [x, y, angle, width, score]\n                return []\n        return GraspDetector()\n\n    def detect_grasp_points(self, depth_map, rgb_image):\n        """Detect potential grasp points"""\n        grasps = self.grasp_detector.detect_grasps(depth_map, rgb_image)\n\n        # Filter grasps based on robot reachability\n        reachable_grasps = []\n        robot_pos = self.robot.get_world_pose()[0]\n\n        for grasp in grasps:\n            grasp_world_pos = self.camera_pixel_to_world(grasp[:2], depth_map[grasp[1], grasp[0]])\n            distance = np.linalg.norm(grasp_world_pos - robot_pos)\n\n            # Check if within reach (simplified)\n            if distance < 1.0:  # 1 meter reach\n                reachable_grasps.append({\n                    "position": grasp_world_pos,\n                    "angle": grasp[2],\n                    "width": grasp[3],\n                    "score": grasp[4],\n                    "type": self.classify_grasp_type(grasp)\n                })\n\n        return sorted(reachable_grasps, key=lambda x: x["score"], reverse=True)\n\n    def camera_pixel_to_world(self, pixel_coords, depth_value):\n        """Convert camera pixel + depth to world coordinates"""\n        # Get camera intrinsic parameters\n        intrinsics = self.camera.get_intrinsics()\n\n        # Convert pixel to normalized coordinates\n        x_norm = (pixel_coords[0] - intrinsics[0][2]) / intrinsics[0][0]\n        y_norm = (pixel_coords[1] - intrinsics[1][2]) / intrinsics[1][1]\n\n        # Convert to world coordinates (simplified)\n        world_x = x_norm * depth_value\n        world_y = y_norm * depth_value\n        world_z = depth_value\n\n        return np.array([world_x, world_y, world_z])\n\n    def classify_grasp_type(self, grasp):\n        """Classify grasp type based on angle and context"""\n        angle = grasp[2]\n\n        if -0.25 < angle < 0.25:\n            return "parallel"\n        elif 1.35 < abs(angle) < 1.75:\n            return "perpendicular"\n        else:\n            return "angled"\n\n    def estimate_surface_normals(self, depth_map):\n        """Estimate surface normals from depth map"""\n        # Compute gradients\n        grad_y, grad_x = np.gradient(depth_map)\n\n        # Compute normal vectors\n        normals = np.zeros((depth_map.shape[0], depth_map.shape[1], 3))\n        normals[:, :, 0] = -grad_x\n        normals[:, :, 1] = -grad_y\n        normals[:, :, 2] = 1\n\n        # Normalize\n        norm = np.linalg.norm(normals, axis=2, keepdims=True)\n        normals = normals / norm\n\n        return normals\n'})}),"\n",(0,a.jsx)(n.h2,{id:"isaac-ros-integration",children:"Isaac ROS Integration"}),"\n",(0,a.jsx)(n.h3,{id:"hardware-accelerated-perception",children:"Hardware-Accelerated Perception"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'import rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import Image, CameraInfo\nfrom geometry_msgs.msg import PointStamped\nfrom vision_msgs.msg import Detection2DArray, ObjectHypothesisWithPose\nimport numpy as np\nimport cv2\nfrom cv_bridge import CvBridge\n\nclass IsaacROSPerceptionNode(Node):\n    def __init__(self):\n        super().__init__(\'isaac_ros_perception_node\')\n\n        # Publishers\n        self.detection_pub = self.create_publisher(Detection2DArray, \'detections\', 10)\n        self.point_pub = self.create_publisher(PointStamped, \'object_point\', 10)\n\n        # Subscribers\n        self.image_sub = self.create_subscription(\n            Image, \'camera/image_raw\', self.image_callback, 10)\n        self.camera_info_sub = self.create_subscription(\n            CameraInfo, \'camera/camera_info\', self.camera_info_callback, 10)\n\n        # Initialize perception components\n        self.bridge = CvBridge()\n        self.camera_intrinsics = None\n        self.perception_pipeline = IsaacPerceptionPipeline(None, None)\n\n        # Isaac Sim integration\n        self.setup_isaac_integration()\n\n    def setup_isaac_integration(self):\n        """Setup connection to Isaac Sim"""\n        # This would typically involve setting up USD stage communication\n        # or using Isaac ROS bridge packages\n        pass\n\n    def image_callback(self, msg):\n        """Process incoming image from Isaac Sim"""\n        try:\n            # Convert ROS image to OpenCV\n            cv_image = self.bridge.imgmsg_to_cv2(msg, desired_encoding=\'rgb8\')\n\n            # Process through perception pipeline\n            results = self.process_perception(cv_image)\n\n            # Publish results\n            self.publish_detections(results)\n\n        except Exception as e:\n            self.get_logger().error(f\'Error processing image: {e}\')\n\n    def camera_info_callback(self, msg):\n        """Update camera intrinsics"""\n        self.camera_intrinsics = np.array(msg.k).reshape(3, 3)\n\n    def process_perception(self, image):\n        """Process image through perception pipeline"""\n        # This would integrate with Isaac Sim\'s rendering pipeline\n        # For now, using a simplified approach\n\n        # Convert to tensor and process\n        image_tensor = torch.tensor(image).permute(2, 0, 1).unsqueeze(0).float() / 255.0\n\n        with torch.no_grad():\n            # Run through perception models\n            detections = self.perception_pipeline.object_detector(image_tensor)\n\n        return self.process_detections_for_ros(detections, image.shape)\n\n    def process_detections_for_ros(self, detections, image_shape):\n        """Convert detections to ROS format"""\n        ros_detections = Detection2DArray()\n        ros_detections.header.stamp = self.get_clock().now().to_msg()\n        ros_detections.header.frame_id = "camera_frame"\n\n        for i in range(len(detections[0]["boxes"])):\n            box = detections[0]["boxes"][i].cpu().numpy()\n            score = detections[0]["scores"][i].cpu().item()\n            label = detections[0]["labels"][i].cpu().item()\n\n            if score > 0.5:  # Confidence threshold\n                detection = Detection2D()\n                detection.header.stamp = ros_detections.header.stamp\n                detection.header.frame_id = ros_detections.header.frame_id\n\n                # Bounding box\n                x1, y1, x2, y2 = box\n                detection.bbox.size_x = x2 - x1\n                detection.bbox.size_y = y2 - y1\n                detection.bbox.center.x = (x1 + x2) / 2\n                detection.bbox.center.y = (y1 + y2) / 2\n\n                # Hypothesis\n                hypothesis = ObjectHypothesisWithPose()\n                hypothesis.id = int(label)\n                hypothesis.score = float(score)\n\n                detection.results.append(hypothesis)\n                ros_detections.detections.append(detection)\n\n        return ros_detections\n\n    def publish_detections(self, detections):\n        """Publish detection results"""\n        self.detection_pub.publish(detections)\n\ndef main(args=None):\n    rclpy.init(args=args)\n    node = IsaacROSPerceptionNode()\n\n    try:\n        rclpy.spin(node)\n    except KeyboardInterrupt:\n        pass\n    finally:\n        node.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == \'__main__\':\n    main()\n'})}),"\n",(0,a.jsx)(n.h2,{id:"isaac-gym-for-reinforcement-learning",children:"Isaac Gym for Reinforcement Learning"}),"\n",(0,a.jsx)(n.h3,{id:"gpu-accelerated-training-environment",children:"GPU-Accelerated Training Environment"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'import torch\nimport numpy as np\nfrom omni.isaac.gym.vec_env import VecEnvBase\nfrom omni.isaac.gym.tasks.humanoid.humanoid_task import HumanoidTask\nfrom omni.isaac.core import World\nfrom omni.isaac.core.utils.stage import add_reference_to_stage\nfrom omni.isaac.core.utils.nucleus import get_assets_root_path\n\nclass IsaacHumanoidEnv(VecEnvBase):\n    def __init__(self, name="Isaac-Humanoid-v0", num_envs=64, headless=True):\n        super().__init__(name=name, num_envs=num_envs, headless=headless)\n\n        # Create world\n        self.world = World(stage_units_in_meters=1.0)\n\n        # Setup humanoid task\n        self.task = HumanoidTask(\n            name="humanoid_task",\n            num_envs=num_envs,\n            device="cuda" if torch.cuda.is_available() else "cpu"\n        )\n\n        # Add task to world\n        self.world.add_task(self.task)\n\n        # Reset world\n        self.world.reset()\n\n        # Get initial observations\n        self.obs = self.get_observations()\n\n    def get_observations(self):\n        """Get observations from all environments"""\n        # This would return state information for each environment\n        obs = self.task.get_observations()\n        return obs\n\n    def step(self, actions):\n        """Execute actions and return next state"""\n        # Apply actions to all environments\n        self.task.apply_actions(actions)\n\n        # Step the world\n        self.world.step(render=False)\n\n        # Get next observations, rewards, dones, info\n        next_obs = self.get_observations()\n        rewards = self.task.get_rewards()\n        dones = self.task.get_dones()\n        info = self.task.get_extras()\n\n        return next_obs, rewards, dones, info\n\n    def reset(self):\n        """Reset all environments"""\n        self.task.reset()\n        self.world.reset()\n        return self.get_observations()\n\n    def close(self):\n        """Close environment"""\n        self.world.clear()\n'})}),"\n",(0,a.jsx)(n.h2,{id:"performance-optimization",children:"Performance Optimization"}),"\n",(0,a.jsx)(n.h3,{id:"gpu-acceleration-best-practices",children:"GPU Acceleration Best Practices"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'class IsaacPerformanceOptimizer:\n    def __init__(self):\n        self.gpu_config = {\n            "max_batch_size": 64,\n            "precision": "mixed",  # fp16 or mixed\n            "memory_fraction": 0.9,\n            "async_execution": True\n        }\n\n    def optimize_rendering(self):\n        """Optimize rendering performance"""\n        # Reduce rendering quality for training\n        settings = {\n            "render_resolution": [640, 480],  # Lower resolution\n            "enable_msaa": False,  # Disable anti-aliasing\n            "enable_denoising": False,  # Disable denoising for training\n            "max_lights_per_view": 8,  # Limit lights\n            "max_distortion": 0.1  # Limit distortion effects\n        }\n        return settings\n\n    def optimize_physics(self):\n        """Optimize physics simulation"""\n        settings = {\n            "substeps": 1,  # Reduce substeps for speed\n            "solver_position_iteration_count": 4,  # Reduce solver iterations\n            "solver_velocity_iteration_count": 1,  # Reduce velocity iterations\n            "enable_ccd": False,  # Disable continuous collision detection\n            "max_depenetration_velocity": 10.0  # Limit velocity\n        }\n        return settings\n\n    def memory_management(self):\n        """Manage GPU memory efficiently"""\n        if torch.cuda.is_available():\n            # Set memory fraction\n            torch.cuda.set_per_process_memory_fraction(\n                self.gpu_config["memory_fraction"]\n            )\n\n            # Enable memory efficient attention if available\n            torch.backends.cuda.matmul.allow_tf32 = True\n            torch.backends.cudnn.allow_tf32 = True\n\n# Example usage for training\ndef setup_training_environment():\n    optimizer = IsaacPerformanceOptimizer()\n\n    # Apply optimizations\n    render_settings = optimizer.optimize_rendering()\n    physics_settings = optimizer.optimize_physics()\n    optimizer.memory_management()\n\n    # Create optimized environment\n    env = IsaacHumanoidEnv(\n        num_envs=128,  # Larger batch for training\n        headless=True  # No rendering for training\n    )\n\n    return env\n'})}),"\n",(0,a.jsx)(n.h2,{id:"troubleshooting-and-best-practices",children:"Troubleshooting and Best Practices"}),"\n",(0,a.jsx)(n.h3,{id:"common-issues-and-solutions",children:"Common Issues and Solutions"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'class IsaacTroubleshooter:\n    def __init__(self):\n        pass\n\n    def check_gpu_memory(self):\n        """Check GPU memory usage and suggest solutions"""\n        if torch.cuda.is_available():\n            memory_allocated = torch.cuda.memory_allocated()\n            memory_reserved = torch.cuda.memory_reserved()\n            memory_total = torch.cuda.get_device_properties(0).total_memory\n\n            print(f"GPU Memory - Allocated: {memory_allocated/1e9:.2f}GB, "\n                  f"Reserved: {memory_reserved/1e9:.2f}GB, "\n                  f"Total: {memory_total/1e9:.2f}GB")\n\n            if memory_allocated > 0.8 * memory_total:\n                print("Warning: GPU memory usage is high. Consider:")\n                print("- Reducing batch size")\n                print("- Using mixed precision training")\n                print("- Clearing unused tensors")\n\n    def optimize_scene_complexity(self):\n        """Suggest scene optimization strategies"""\n        suggestions = [\n            "Reduce polygon count of static objects",\n            "Use Level of Detail (LOD) for distant objects",\n            "Limit dynamic lighting calculations",\n            "Use occlusion culling for hidden objects",\n            "Implement frustum culling for camera visibility"\n        ]\n        return suggestions\n\n    def performance_monitoring(self):\n        """Monitor performance metrics"""\n        import time\n\n        class PerformanceMonitor:\n            def __init__(self):\n                self.frame_times = []\n                self.max_samples = 100\n\n            def start_frame(self):\n                self.frame_start = time.time()\n\n            def end_frame(self):\n                frame_time = time.time() - self.frame_start\n                self.frame_times.append(frame_time)\n\n                if len(self.frame_times) > self.max_samples:\n                    self.frame_times.pop(0)\n\n                avg_frame_time = sum(self.frame_times) / len(self.frame_times)\n                fps = 1.0 / avg_frame_time if avg_frame_time > 0 else 0\n\n                return fps, avg_frame_time\n\n        return PerformanceMonitor()\n'})}),"\n",(0,a.jsx)(n.h2,{id:"knowledge-check",children:"Knowledge Check"}),"\n",(0,a.jsxs)(n.ol,{children:["\n",(0,a.jsx)(n.li,{children:"What are the key components of the NVIDIA Isaac platform ecosystem?"}),"\n",(0,a.jsx)(n.li,{children:"How does Isaac Sim enable photorealistic simulation for robotics?"}),"\n",(0,a.jsx)(n.li,{children:"What is domain randomization and why is it important for synthetic data generation?"}),"\n",(0,a.jsx)(n.li,{children:"How do you integrate Isaac with ROS 2 for perception and navigation tasks?"}),"\n"]}),"\n",(0,a.jsx)(n.h2,{id:"summary",children:"Summary"}),"\n",(0,a.jsx)(n.p,{children:"This chapter covered the NVIDIA Isaac platform, focusing on Isaac Sim for photorealistic simulation and synthetic data generation. We explored how to create diverse training datasets, implement AI-powered perception systems, and integrate with ROS 2 for hardware-accelerated robotics applications. The chapter also provided best practices for performance optimization and GPU acceleration."}),"\n",(0,a.jsx)(n.h2,{id:"next-steps",children:"Next Steps"}),"\n",(0,a.jsx)(n.p,{children:"In the next chapter, we'll dive into Isaac ROS and hardware-accelerated perception, exploring VSLAM (Visual SLAM), navigation systems, and computer vision techniques specifically designed for humanoid robotics applications."})]})}function m(e={}){const{wrapper:n}={...(0,i.R)(),...e.components};return n?(0,a.jsx)(n,{...e,children:(0,a.jsx)(d,{...e})}):d(e)}}}]);