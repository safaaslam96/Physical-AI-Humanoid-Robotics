"use strict";(globalThis.webpackChunkphysical_ai_humanoid_robotics_book=globalThis.webpackChunkphysical_ai_humanoid_robotics_book||[]).push([[394],{8453(e,n,t){t.d(n,{R:()=>r,x:()=>o});var a=t(6540);const s={},i=a.createContext(s);function r(e){const n=a.useContext(i);return a.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:r(e.components),a.createElement(i.Provider,{value:n},e.children)}},8755(e,n,t){t.r(n),t.d(n,{assets:()=>c,contentTitle:()=>r,default:()=>_,frontMatter:()=>i,metadata:()=>o,toc:()=>l});var a=t(4848),s=t(8453);const i={sidebar_position:16,title:"Chapter 16: Natural Human-Robot Interaction"},r="Chapter 16: Natural Human-Robot Interaction",o={id:"part5/chapter16",title:"Chapter 16: Natural Human-Robot Interaction",description:"Learning Objectives",source:"@site/docs/part5/chapter16.md",sourceDirName:"part5",slug:"/part5/chapter16",permalink:"/Physical-AI-Humanoid-Robotics/docs/part5/chapter16",draft:!1,unlisted:!1,editUrl:"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/part5/chapter16.md",tags:[],version:"current",sidebarPosition:16,frontMatter:{sidebar_position:16,title:"Chapter 16: Natural Human-Robot Interaction"},sidebar:"tutorialSidebar",previous:{title:"Chapter 15: Manipulation and Grasping",permalink:"/Physical-AI-Humanoid-Robotics/docs/part5/chapter15"},next:{title:"Chapter 17: Integrating LLMs for Conversational AI in Robots",permalink:"/Physical-AI-Humanoid-Robotics/docs/part6/chapter17"}},c={},l=[{value:"Learning Objectives",id:"learning-objectives",level:2},{value:"Introduction to Natural Human-Robot Interaction",id:"introduction-to-natural-human-robot-interaction",level:2},{value:"Key Principles of Natural Interaction",id:"key-principles-of-natural-interaction",level:3},{value:"Communication Modalities",id:"communication-modalities",level:3},{value:"Communication Paradigms",id:"communication-paradigms",level:2},{value:"Voice-Based Interaction",id:"voice-based-interaction",level:3},{value:"Gesture-Based Interaction",id:"gesture-based-interaction",level:3},{value:"Visual Interaction and Social Cues",id:"visual-interaction-and-social-cues",level:3},{value:"User Experience Design",id:"user-experience-design",level:2},{value:"Interaction Flow Design",id:"interaction-flow-design",level:3},{value:"Context-Aware Interaction",id:"context-aware-interaction",level:3},{value:"Knowledge Check",id:"knowledge-check",level:2},{value:"Summary",id:"summary",level:2},{value:"Next Steps",id:"next-steps",level:2}];function d(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,s.R)(),...e.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(n.h1,{id:"chapter-16-natural-human-robot-interaction",children:"Chapter 16: Natural Human-Robot Interaction"}),"\n",(0,a.jsx)(n.h2,{id:"learning-objectives",children:"Learning Objectives"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"Design natural communication paradigms for human-robot interaction"}),"\n",(0,a.jsx)(n.li,{children:"Implement user experience optimization for humanoid robots"}),"\n",(0,a.jsx)(n.li,{children:"Master advanced interaction techniques for humanoid robotics"}),"\n",(0,a.jsx)(n.li,{children:"Understand social robotics principles and best practices"}),"\n"]}),"\n",(0,a.jsx)(n.h2,{id:"introduction-to-natural-human-robot-interaction",children:"Introduction to Natural Human-Robot Interaction"}),"\n",(0,a.jsx)(n.p,{children:"Natural Human-Robot Interaction (NHRI) is the study and design of interfaces and behaviors that allow humans to interact with robots in intuitive, familiar ways. For humanoid robots, this involves creating interactions that feel natural to humans by leveraging our understanding of human communication patterns, social cues, and expectations."}),"\n",(0,a.jsx)(n.h3,{id:"key-principles-of-natural-interaction",children:"Key Principles of Natural Interaction"}),"\n",(0,a.jsxs)(n.ol,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Intuitive Communication"}),": Using familiar modalities like speech, gestures, and facial expressions"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Context Awareness"}),": Understanding the environment and situation"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Social Norms"}),": Following human social conventions and expectations"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Predictability"}),": Behaving in ways that humans can anticipate"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Feedback"}),": Providing clear, timely responses to human actions"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Safety"}),": Ensuring interactions are safe and comfortable"]}),"\n"]}),"\n",(0,a.jsx)(n.h3,{id:"communication-modalities",children:"Communication Modalities"}),"\n",(0,a.jsx)(n.p,{children:"Human communication is multi-modal, involving:"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Verbal"}),": Speech and language"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Non-verbal"}),": Gestures, facial expressions, posture"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Paralinguistic"}),": Tone, pitch, volume"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Spatial"}),": Proxemics and personal space"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Temporal"}),": Timing and rhythm of interactions"]}),"\n"]}),"\n",(0,a.jsx)(n.h2,{id:"communication-paradigms",children:"Communication Paradigms"}),"\n",(0,a.jsx)(n.h3,{id:"voice-based-interaction",children:"Voice-Based Interaction"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"# voice_interaction.py\nimport speech_recognition as sr\nimport pyttsx3\nimport asyncio\nimport threading\nfrom queue import Queue\nimport time\n\nclass VoiceInteractionManager:\n    def __init__(self):\n        self.recognizer = sr.Recognizer()\n        self.microphone = sr.Microphone()\n        self.tts_engine = pyttsx3.init()\n\n        # Configuration\n        self.tts_engine.setProperty('rate', 150)  # Speed of speech\n        self.tts_engine.setProperty('volume', 0.9)  # Volume level\n\n        # Voice interaction state\n        self.is_listening = False\n        self.conversation_context = {}\n        self.response_queue = Queue()\n        self.command_handlers = {}\n\n        # Initialize speech recognition\n        with self.microphone as source:\n            self.recognizer.adjust_for_ambient_noise(source)\n\n        self.setup_command_handlers()\n\n    def setup_command_handlers(self):\n        \"\"\"Setup command handlers for different robot capabilities\"\"\"\n        self.command_handlers.update({\n            'move': self.handle_move_command,\n            'grasp': self.handle_grasp_command,\n            'navigation': self.handle_navigation_command,\n            'information': self.handle_information_request,\n            'social': self.handle_social_interaction\n        })\n\n    def start_voice_system(self):\n        \"\"\"Start the voice interaction system\"\"\"\n        self.is_listening = True\n        self.listen_thread = threading.Thread(target=self.voice_loop, daemon=True)\n        self.listen_thread.start()\n\n    def voice_loop(self):\n        \"\"\"Main voice interaction loop\"\"\"\n        while self.is_listening:\n            try:\n                with self.microphone as source:\n                    # Listen for speech with timeout\n                    audio = self.recognizer.listen(source, timeout=1.0, phrase_time_limit=5.0)\n\n                # Recognize speech\n                text = self.recognizer.recognize_google(audio)\n                print(f\"Heard: {text}\")\n\n                # Process the command\n                self.process_voice_command(text)\n\n            except sr.WaitTimeoutError:\n                # No speech detected, continue listening\n                continue\n            except sr.UnknownValueError:\n                self.speak_response(\"Sorry, I didn't understand that.\")\n            except sr.RequestError as e:\n                print(f\"Speech recognition error: {e}\")\n                self.speak_response(\"Sorry, I'm having trouble understanding.\")\n\n    def process_voice_command(self, text):\n        \"\"\"Process a voice command using NLP techniques\"\"\"\n        # Simple command parsing (in practice, use more sophisticated NLP)\n        text_lower = text.lower()\n\n        # Check for wake word (optional)\n        if 'robot' in text_lower or 'hey' in text_lower:\n            # Extract the actual command\n            command_start = max(text_lower.find('robot') + 6, text_lower.find('hey') + 4)\n            command = text_lower[command_start:].strip()\n        else:\n            command = text_lower\n\n        # Classify command type\n        command_type = self.classify_command(command)\n\n        if command_type in self.command_handlers:\n            response = self.command_handlers[command_type](command)\n            if response:\n                self.speak_response(response)\n        else:\n            self.speak_response(f\"I can help with movement, grasping, navigation, information, or social interaction. What would you like to do?\")\n\n    def classify_command(self, command):\n        \"\"\"Classify command type based on keywords\"\"\"\n        if any(word in command for word in ['move', 'go', 'walk', 'navigate', 'to', 'toward']):\n            return 'navigation'\n        elif any(word in command for word in ['grasp', 'pick', 'take', 'grab', 'hold', 'lift']):\n            return 'grasp'\n        elif any(word in command for word in ['move', 'step', 'forward', 'backward', 'left', 'right']):\n            return 'move'\n        elif any(word in command for word in ['what', 'how', 'when', 'where', 'who', 'tell me', 'information']):\n            return 'information'\n        elif any(word in command for word in ['hello', 'hi', 'good', 'morning', 'afternoon', 'evening', 'bye', 'goodbye', 'nice', 'meet']):\n            return 'social'\n        else:\n            return 'unknown'\n\n    def handle_navigation_command(self, command):\n        \"\"\"Handle navigation-related commands\"\"\"\n        # Extract destination from command\n        destinations = ['kitchen', 'living room', 'bedroom', 'office', 'dining room']\n\n        for dest in destinations:\n            if dest in command:\n                # Simulate navigation\n                self.speak_response(f\"Okay, I'm navigating to the {dest}. Please follow me.\")\n                # In practice, this would trigger navigation system\n                return f\"Moving toward {dest}\"\n\n        # If no specific destination, ask for clarification\n        return \"Where would you like me to navigate to?\"\n\n    def handle_grasp_command(self, command):\n        \"\"\"Handle grasping-related commands\"\"\"\n        # Extract object reference\n        objects = ['cup', 'bottle', 'book', 'phone', 'keys', 'box']\n\n        for obj in objects:\n            if obj in command:\n                # Simulate grasping action\n                self.speak_response(f\"Okay, I'll grasp the {obj} for you.\")\n                # In practice, this would trigger grasping system\n                return f\"Attempting to grasp {obj}\"\n\n        return \"What object would you like me to grasp?\"\n\n    def handle_move_command(self, command):\n        \"\"\"Handle movement commands\"\"\"\n        if 'forward' in command or 'ahead' in command:\n            self.speak_response(\"Moving forward.\")\n            return \"Moving forward\"\n        elif 'backward' in command or 'back' in command:\n            self.speak_response(\"Moving backward.\")\n            return \"Moving backward\"\n        elif 'left' in command:\n            self.speak_response(\"Turning left.\")\n            return \"Turning left\"\n        elif 'right' in command:\n            self.speak_response(\"Turning right.\")\n            return \"Turning right\"\n        else:\n            return \"In what direction would you like me to move?\"\n\n    def handle_information_request(self, command):\n        \"\"\"Handle information requests\"\"\"\n        if 'time' in command:\n            import datetime\n            current_time = datetime.datetime.now().strftime(\"%H:%M\")\n            return f\"The current time is {current_time}.\"\n        elif 'date' in command:\n            import datetime\n            current_date = datetime.datetime.now().strftime(\"%B %d, %Y\")\n            return f\"Today's date is {current_date}.\"\n        elif 'weather' in command:\n            return \"I don't have access to weather information right now, but I can help you find it if you'd like.\"\n        else:\n            return \"I can tell you the time, date, or help with other information. What would you like to know?\"\n\n    def handle_social_interaction(self, command):\n        \"\"\"Handle social interaction commands\"\"\"\n        if 'hello' in command or 'hi' in command:\n            return \"Hello! It's nice to meet you. How can I help you today?\"\n        elif 'bye' in command or 'goodbye' in command:\n            return \"Goodbye! It was nice interacting with you.\"\n        elif 'thank' in command:\n            return \"You're welcome! I'm happy to help.\"\n        elif 'name' in command:\n            return \"I'm your humanoid robot assistant. You can call me ARIA - Autonomous Robot Interaction Assistant.\"\n        else:\n            return \"Hello! How can I assist you today?\"\n\n    def speak_response(self, text):\n        \"\"\"Generate speech response\"\"\"\n        print(f\"Robot says: {text}\")\n        self.tts_engine.say(text)\n        self.tts_engine.runAndWait()\n\n    def get_response(self, text):\n        \"\"\"Get response without speaking (for internal use)\"\"\"\n        command_type = self.classify_command(text.lower())\n        if command_type in self.command_handlers:\n            return self.command_handlers[command_type](text.lower())\n        return \"I'm not sure how to respond to that.\"\n\n    def stop_voice_system(self):\n        \"\"\"Stop the voice interaction system\"\"\"\n        self.is_listening = False\n\nclass ConversationalContextManager:\n    \"\"\"Manage conversation context and state\"\"\"\n    def __init__(self):\n        self.context = {\n            'current_topic': None,\n            'previous_utterances': [],\n            'user_preferences': {},\n            'task_state': {},\n            'time_started': time.time()\n        }\n\n    def update_context(self, user_input, robot_response):\n        \"\"\"Update conversation context\"\"\"\n        self.context['previous_utterances'].append({\n            'user': user_input,\n            'robot': robot_response,\n            'timestamp': time.time()\n        })\n\n        # Keep only recent utterances\n        if len(self.context['previous_utterances']) > 10:\n            self.context['previous_utterances'] = self.context['previous_utterances'][-10:]\n\n    def get_context_summary(self):\n        \"\"\"Get summary of current context\"\"\"\n        return {\n            'topic': self.context['current_topic'],\n            'conversation_length': len(self.context['previous_utterances']),\n            'duration': time.time() - self.context['time_started'],\n            'recent_utterances': self.context['previous_utterances'][-3:] if self.context['previous_utterances'] else []\n        }\n\n    def infer_user_intent(self, current_input):\n        \"\"\"Infer user intent based on context\"\"\"\n        # Simple intent inference based on keywords and context\n        recent_context = ' '.join([u['user'] for u in self.context['previous_utterances'][-3:]])\n\n        combined_input = f\"{recent_context} {current_input}\".lower()\n\n        if any(word in combined_input for word in ['stop', 'cancel', 'abort']):\n            return 'cancel_task'\n        elif any(word in combined_input for word in ['repeat', 'again', 'more']):\n            return 'repeat_action'\n        elif any(word in combined_input for word in ['different', 'change', 'other']):\n            return 'change_approach'\n        else:\n            return 'new_request'\n\n# Example usage\ndef example_voice_interaction():\n    voice_manager = VoiceInteractionManager()\n    context_manager = ConversationalContextManager()\n\n    # Simulate some voice commands\n    test_commands = [\n        \"Hello robot\",\n        \"What time is it?\",\n        \"Navigate to the kitchen\",\n        \"Grasp the cup\",\n        \"Thank you\"\n    ]\n\n    for command in test_commands:\n        print(f\"\\nUser says: {command}\")\n        response = voice_manager.get_response(command)\n        print(f\"Robot response: {response}\")\n\n        # Update context\n        context_manager.update_context(command, response)\n\n        # Show context summary\n        summary = context_manager.get_context_summary()\n        print(f\"Context: Topic={summary['topic']}, Length={summary['conversation_length']}\")\n\nif __name__ == \"__main__\":\n    try:\n        example_voice_interaction()\n    except ImportError as e:\n        print(f\"Missing dependency: {e}\")\n        print(\"Install: pip install SpeechRecognition pyttsx3\")\n"})}),"\n",(0,a.jsx)(n.h3,{id:"gesture-based-interaction",children:"Gesture-Based Interaction"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'# gesture_interaction.py\nimport cv2\nimport mediapipe as mp\nimport numpy as np\nfrom enum import Enum\n\nclass GestureType(Enum):\n    WAVE = "wave"\n    THUMBS_UP = "thumbs_up"\n    THUMBS_DOWN = "thumbs_down"\n    POINTING = "pointing"\n    STOP = "stop"\n    COME_HERE = "come_here"\n    FOLLOW_ME = "follow_me"\n    GRASP = "grasp"\n    RELEASE = "release"\n\nclass GestureRecognitionManager:\n    def __init__(self):\n        # Initialize MediaPipe for hand tracking\n        self.mp_hands = mp.solutions.hands\n        self.hands = self.mp_hands.Hands(\n            static_image_mode=False,\n            max_num_hands=2,\n            min_detection_confidence=0.7,\n            min_tracking_confidence=0.7\n        )\n        self.mp_drawing = mp.solutions.drawing_utils\n\n        # Gesture recognition state\n        self.previous_gestures = []\n        self.gesture_buffer_size = 5\n        self.confidence_threshold = 0.8\n\n    def process_frame(self, frame):\n        """Process a video frame for gesture recognition"""\n        # Convert BGR to RGB\n        rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n\n        # Process the frame\n        results = self.hands.process(rgb_frame)\n\n        gestures = []\n\n        if results.multi_hand_landmarks:\n            for hand_landmarks in results.multi_hand_landmarks:\n                # Recognize gesture from hand landmarks\n                gesture = self.recognize_gesture(hand_landmarks, results.multi_handedness)\n                if gesture:\n                    gestures.append(gesture)\n\n                # Draw landmarks on frame\n                self.mp_drawing.draw_landmarks(\n                    frame, hand_landmarks, self.mp_hands.HAND_CONNECTIONS\n                )\n\n        # Update gesture history\n        self.previous_gestures.extend(gestures)\n        if len(self.previous_gestures) > self.gesture_buffer_size:\n            self.previous_gestures = self.previous_gestures[-self.gesture_buffer_size:]\n\n        return frame, gestures\n\n    def recognize_gesture(self, hand_landmarks, handedness):\n        """Recognize specific gesture from hand landmarks"""\n        # Get landmark coordinates\n        landmarks = hand_landmarks.landmark\n\n        # Determine if this is left or right hand\n        is_right = handedness[0].classification[0].label == \'Right\'\n\n        # Calculate distances between key points\n        thumb_tip = np.array([landmarks[self.mp_hands.HandLandmark.THUMB_TIP].x,\n                             landmarks[self.mp_hands.HandLandmark.THUMB_TIP].y])\n        index_tip = np.array([landmarks[self.mp_hands.HandLandmark.INDEX_FINGER_TIP].x,\n                             landmarks[self.mp_hands.HandLandmark.INDEX_FINGER_TIP].y])\n        middle_tip = np.array([landmarks[self.mp_hands.HandLandmark.MIDDLE_FINGER_TIP].x,\n                              landmarks[self.mp_hands.HandLandmark.MIDDLE_FINGER_TIP].y])\n        ring_tip = np.array([landmarks[self.mp_hands.HandLandmark.RING_FINGER_TIP].x,\n                            landmarks[self.mp_hands.HandLandmark.RING_FINGER_TIP].y])\n        pinky_tip = np.array([landmarks[self.mp_hands.HandLandmark.PINKY_TIP].x,\n                             landmarks[self.mp_hands.HandLandmark.PINKY_TIP].y])\n\n        wrist = np.array([landmarks[self.mp_hands.HandLandmark.WRIST].x,\n                         landmarks[self.mp_hands.HandLandmark.WRIST].y])\n\n        # Calculate distances\n        thumb_index_dist = np.linalg.norm(thumb_tip - index_tip)\n        index_middle_dist = np.linalg.norm(index_tip - middle_tip)\n        middle_ring_dist = np.linalg.norm(middle_tip - ring_tip)\n        ring_pinky_dist = np.linalg.norm(ring_tip - pinky_tip)\n\n        # Calculate angles and positions\n        index_direction = index_tip - wrist\n        middle_direction = middle_tip - wrist\n        thumb_direction = thumb_tip - wrist\n\n        # Recognize specific gestures\n        gesture = self.classify_gesture(\n            thumb_tip, index_tip, middle_tip, ring_tip, pinky_tip, wrist,\n            thumb_index_dist, index_middle_dist, middle_ring_dist, ring_pinky_dist,\n            index_direction, middle_direction, thumb_direction, is_right\n        )\n\n        return gesture\n\n    def classify_gesture(self, thumb, index, middle, ring, pinky, wrist,\n                        d_thumb_index, d_index_middle, d_middle_ring, d_ring_pinky,\n                        index_dir, middle_dir, thumb_dir, is_right):\n        """Classify gesture based on finger positions and distances"""\n\n        # WAVE: Hand moving side to side\n        # This would be detected by tracking movement over frames\n        # For static detection, we\'ll focus on pose-based gestures\n\n        # THUMBS_UP: Thumb up, other fingers closed\n        if (thumb[1] < index[1] and  # Thumb higher than index finger\n            d_index_middle < 0.1 and  # Index and middle fingers close together\n            d_middle_ring < 0.1 and   # Middle and ring fingers close together\n            d_ring_pinky < 0.1):      # Ring and pinky fingers close together\n            return GestureType.THUMBS_UP\n\n        # THUMBS_DOWN: Thumb down, other fingers closed\n        elif (thumb[1] > index[1] and  # Thumb lower than index finger\n              d_index_middle < 0.1 and\n              d_middle_ring < 0.1 and\n              d_ring_pinky < 0.1):\n            return GestureType.THUMBS_DOWN\n\n        # STOP: Open palm facing robot\n        elif (abs(index[0] - wrist[0]) > 0.1 and  # Fingers extended\n              abs(middle[0] - wrist[0]) > 0.1 and\n              abs(ring[0] - wrist[0]) > 0.1 and\n              abs(pinky[0] - wrist[0]) > 0.1 and\n              index[1] < wrist[1] and  # Hand upright\n              middle[1] < wrist[1] and\n              ring[1] < wrist[1] and\n              pinky[1] < wrist[1]):\n            return GestureType.STOP\n\n        # POINTING: Index finger extended, others closed\n        elif (np.linalg.norm(index - wrist) > np.linalg.norm(thumb - wrist) and  # Index finger extended\n              d_index_middle > 0.15 and  # Index and middle fingers apart\n              d_middle_ring < 0.1 and    # Other fingers together\n              d_ring_pinky < 0.1):\n            return GestureType.POINTING\n\n        # GRASP: All fingers curled as if grasping\n        elif (d_thumb_index < 0.05 and  # Thumb and index close\n              d_index_middle < 0.05 and  # All fingers close together\n              d_middle_ring < 0.05 and\n              d_ring_pinky < 0.05):\n            return GestureType.GRASP\n\n        # RELEASE: Hand open as if releasing\n        elif (d_thumb_index > 0.15 and  # Fingers spread\n              d_index_middle > 0.15 and\n              d_middle_ring > 0.15 and\n              d_ring_pinky > 0.15):\n            return GestureType.RELEASE\n\n        # COME HERE: Index finger pointing toward robot\n        elif (np.linalg.norm(index - wrist) > 0.1 and  # Index extended\n              d_index_middle > 0.1 and   # Index separate\n              d_middle_ring < 0.05 and   # Others together\n              d_ring_pinky < 0.05 and\n              index[0] < wrist[0] and    # Pointing toward robot (assuming right hand)\n              is_right):\n            return GestureType.COME_HERE\n\n        # FOLLOW ME: Hand moving in guiding motion\n        # This would require tracking movement over time\n        # For now, we\'ll use a specific hand shape\n        elif (abs(index[0] - middle[0]) < 0.05 and  # Index and middle together\n              abs(middle[0] - ring[0]) < 0.05 and   # All fingers together\n              abs(ring[0] - pinky[0]) < 0.05 and\n              index[1] < wrist[1] and               # Hand upright\n              middle[1] < wrist[1] and\n              ring[1] < wrist[1] and\n              pinky[1] < wrist[1] and\n              thumb[0] > wrist[0]):                 # Thumb on outside\n            return GestureType.FOLLOW_ME\n\n        return None\n\n    def get_gesture_meaning(self, gesture_type):\n        """Get the meaning/action associated with a gesture"""\n        meanings = {\n            GestureType.WAVE: "Greeting/Acknowledgment",\n            GestureType.THUMBS_UP: "Approval/Confirmation",\n            GestureType.THUMBS_DOWN: "Disapproval/Denial",\n            GestureType.POINTING: "Directing attention/Indicating location",\n            GestureType.STOP: "Halt/Wait/Stop action",\n            GestureType.COME_HERE: "Approach user",\n            GestureType.FOLLOW_ME: "Follow user",\n            GestureType.GRASP: "Prepare to grasp object",\n            GestureType.RELEASE: "Release object/Stop grasping"\n        }\n        return meanings.get(gesture_type, "Unknown gesture")\n\nclass GestureInteractionController:\n    """Controller for gesture-based robot interaction"""\n    def __init__(self):\n        self.gesture_manager = GestureRecognitionManager()\n        self.robot_actions = {\n            GestureType.COME_HERE: self.move_towards_user,\n            GestureType.FOLLOW_ME: self.follow_user,\n            GestureType.STOP: self.stop_robot,\n            GestureType.GRASP: self.prepare_grasp,\n            GestureType.RELEASE: self.release_object,\n            GestureType.THUMBS_UP: self.confirm_action,\n            GestureType.THUMBS_DOWN: self.reject_action\n        }\n        self.current_task = None\n        self.user_position = None\n\n    def process_gesture(self, gesture_type):\n        """Process a recognized gesture"""\n        if gesture_type in self.robot_actions:\n            action = self.robot_actions[gesture_type]\n            result = action()\n            return result\n        else:\n            print(f"Gesture {gesture_type} not mapped to any action")\n            return None\n\n    def move_towards_user(self):\n        """Move robot towards user"""\n        print("Moving towards user...")\n        # In practice, this would use navigation system\n        return "Moving towards user"\n\n    def follow_user(self):\n        """Follow user to a destination"""\n        print("Following user...")\n        # In practice, this would start person-following behavior\n        return "Following user"\n\n    def stop_robot(self):\n        """Stop current robot action"""\n        print("Stopping robot...")\n        # In practice, this would stop any ongoing motion\n        return "Robot stopped"\n\n    def prepare_grasp(self):\n        """Prepare for grasping action"""\n        print("Preparing to grasp...")\n        # In practice, this would ready the manipulation system\n        return "Grasp preparation initiated"\n\n    def release_object(self):\n        """Release currently held object"""\n        print("Releasing object...")\n        # In practice, this would open grippers\n        return "Object released"\n\n    def confirm_action(self):\n        """Confirm current action"""\n        print("Action confirmed")\n        return "Action confirmed"\n\n    def reject_action(self):\n        """Reject current action"""\n        print("Action rejected")\n        return "Action rejected"\n\n    def run_gesture_interaction(self):\n        """Run gesture-based interaction loop"""\n        cap = cv2.VideoCapture(0)\n\n        print("Gesture interaction started. Show gestures to control the robot.")\n        print("Available gestures: Stop, Come Here, Follow Me, Grasp, Release, Thumbs Up/Down")\n\n        while True:\n            ret, frame = cap.read()\n            if not ret:\n                break\n\n            # Process frame for gestures\n            processed_frame, gestures = self.gesture_manager.process_frame(frame)\n\n            # Process recognized gestures\n            for gesture in gestures:\n                print(f"Recognized gesture: {gesture}")\n                meaning = self.gesture_manager.get_gesture_meaning(gesture)\n                print(f"Meaning: {meaning}")\n\n                # Execute corresponding action\n                result = self.process_gesture(gesture)\n                if result:\n                    print(f"Action result: {result}")\n\n            # Display frame\n            cv2.imshow(\'Gesture Recognition\', processed_frame)\n\n            # Break on \'q\' key press\n            if cv2.waitKey(1) & 0xFF == ord(\'q\'):\n                break\n\n        cap.release()\n        cv2.destroyAllWindows()\n\n# Example usage\ndef example_gesture_interaction():\n    controller = GestureInteractionController()\n\n    # Simulate gesture recognition and processing\n    print("Gesture interaction examples:")\n\n    # Test some gestures\n    test_gestures = [\n        GestureType.COME_HERE,\n        GestureType.STOP,\n        GestureType.GRASP,\n        GestureType.THUMBS_UP\n    ]\n\n    for gesture in test_gestures:\n        print(f"\\nProcessing gesture: {gesture}")\n        meaning = controller.gesture_manager.get_gesture_meaning(gesture)\n        print(f"Meaning: {meaning}")\n        result = controller.process_gesture(gesture)\n        print(f"Result: {result}")\n\nif __name__ == "__main__":\n    try:\n        example_gesture_interaction()\n    except ImportError as e:\n        print(f"Missing dependency: {e}")\n        print("Install: pip install opencv-python mediapipe")\n'})}),"\n",(0,a.jsx)(n.h3,{id:"visual-interaction-and-social-cues",children:"Visual Interaction and Social Cues"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'# visual_interaction.py\nimport cv2\nimport numpy as np\nimport math\nfrom enum import Enum\n\nclass SocialDistance(Enum):\n    INTIMATE = (0.0, 0.45)    # 0-1.5 feet - very close\n    PERSONAL = (0.45, 1.2)    # 1.5-4 feet - friends, family\n    SOCIAL = (1.2, 3.6)       # 4-12 feet - social interactions\n    PUBLIC = (3.6, 10.0)      # 12+ feet - public speaking\n\nclass VisualInteractionManager:\n    def __init__(self):\n        self.eye_contact_enabled = True\n        self.head_movement_enabled = True\n        self.social_distance_preference = SocialDistance.PERSONAL\n        self.user_tracking = True\n        self.face_detection_enabled = True\n\n        # Initialize face detection\n        self.face_cascade = cv2.CascadeClassifier(\n            cv2.data.haarcascades + \'haarcascade_frontalface_default.xml\'\n        )\n\n        # Robot head parameters\n        self.head_position = np.array([0.0, 0.0, 1.7])  # Robot head position (x, y, z)\n        self.current_gaze_target = None\n        self.head_orientation = np.array([0.0, 0.0, 0.0])  # Pitch, yaw, roll\n\n    def detect_faces(self, frame):\n        """Detect faces in the input frame"""\n        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n        faces = self.face_cascade.detectMultiScale(\n            gray,\n            scaleFactor=1.1,\n            minNeighbors=5,\n            minSize=(30, 30)\n        )\n        return faces\n\n    def track_user_attention(self, frame):\n        """Track user attention and engagement"""\n        faces = self.detect_faces(frame)\n\n        if len(faces) > 0:\n            # Get the largest face (closest to camera)\n            largest_face = max(faces, key=lambda f: f[2] * f[3])\n            x, y, w, h = largest_face\n\n            # Calculate face center\n            face_center = (x + w//2, y + h//2)\n\n            # Draw face rectangle and center\n            cv2.rectangle(frame, (x, y), (x+w, y+h), (255, 0, 0), 2)\n            cv2.circle(frame, face_center, 5, (0, 255, 0), -1)\n\n            # Calculate distance to face (rough estimate based on size)\n            face_size = max(w, h)\n            estimated_distance = self.estimate_distance_from_size(face_size, frame.shape[1])\n\n            # Check if user is in appropriate social distance\n            appropriate_distance = self.check_social_distance(estimated_distance)\n\n            return {\n                \'face_detected\': True,\n                \'face_center\': face_center,\n                \'distance\': estimated_distance,\n                \'appropriate_distance\': appropriate_distance,\n                \'engagement_level\': self.calculate_engagement_level(faces, frame)\n            }\n\n        return {\n            \'face_detected\': False,\n            \'engagement_level\': 0.0\n        }\n\n    def estimate_distance_from_size(self, face_width_pixels, frame_width_pixels):\n        """Estimate distance to face based on face size in frame"""\n        # This is a simplified estimation\n        # In practice, you\'d use calibrated camera parameters\n        # and known face size (average face width ~20cm)\n        known_face_width = 0.2  # meters\n        focal_length = 800  # pixels (typical for webcams)\n\n        estimated_distance = (known_face_width * focal_length) / face_width_pixels\n        return estimated_distance\n\n    def check_social_distance(self, distance):\n        """Check if distance is appropriate for current interaction"""\n        min_dist, max_dist = self.social_distance_preference.value\n        return min_dist <= distance <= max_dist\n\n    def calculate_engagement_level(self, faces, frame):\n        """Calculate user engagement level based on multiple factors"""\n        if len(faces) == 0:\n            return 0.0\n\n        # Engagement factors:\n        # 1. Number of people (more people = more engagement)\n        num_people = len(faces)\n\n        # 2. Average face size (closer = more engaged)\n        avg_face_size = np.mean([max(f[2], f[3]) for f in faces])\n        size_factor = min(avg_face_size / 200, 1.0)  # Normalize\n\n        # 3. Face positions (central faces = more engaged)\n        frame_center = np.array([frame.shape[1]/2, frame.shape[0]/2])\n        position_scores = []\n        for (x, y, w, h) in faces:\n            face_center = np.array([x + w/2, y + h/2])\n            distance_to_center = np.linalg.norm(frame_center - face_center)\n            max_distance = np.linalg.norm(frame_center)\n            position_score = 1 - (distance_to_center / max_distance)\n            position_scores.append(position_score)\n\n        avg_position_score = np.mean(position_scores) if position_scores else 0.0\n\n        # Combine factors\n        engagement = (0.3 * min(num_people / 3, 1.0) +\n                     0.4 * size_factor +\n                     0.3 * avg_position_score)\n\n        return min(engagement, 1.0)\n\n    def maintain_eye_contact(self, face_center, frame_shape):\n        """Adjust head orientation to maintain eye contact"""\n        if face_center is None:\n            return\n\n        frame_center = (frame_shape[1] // 2, frame_shape[0] // 2)\n\n        # Calculate the angle difference\n        dx = face_center[0] - frame_center[0]\n        dy = frame_center[1] - face_center[1]  # Inverted because y increases downward\n\n        # Convert to head movement (simplified)\n        max_head_movement = 30  # degrees\n        x_angle = max(-max_head_movement, min(max_head_movement, dx * max_head_movement / (frame_shape[1]/2)))\n        y_angle = max(-max_head_movement, min(max_head_movement, dy * max_head_movement / (frame_shape[0]/2)))\n\n        # Update head orientation\n        self.head_orientation[1] = math.radians(x_angle)  # Yaw\n        self.head_orientation[0] = math.radians(y_angle)  # Pitch\n\n    def generate_head_movement(self, engagement_level, base_movement=True):\n        """Generate natural head movements based on engagement"""\n        movements = []\n\n        if base_movement:\n            # Subtle head nods to show attention\n            if engagement_level > 0.3:\n                movements.append({\n                    \'type\': \'nod\',\n                    \'amplitude\': 0.1 * engagement_level,\n                    \'frequency\': 0.5,\n                    \'duration\': 0.5\n                })\n\n        # Head tilts when listening\n        if engagement_level > 0.6:\n            movements.append({\n                \'type\': \'tilt\',\n                \'amplitude\': 0.05,\n                \'frequency\': 0.3,\n                \'direction\': \'random\'  # Left or right\n            })\n\n        return movements\n\n    def display_social_feedback(self, frame, interaction_data):\n        """Display social feedback on the frame"""\n        if interaction_data[\'face_detected\']:\n            face_center = interaction_data[\'face_center\']\n\n            # Draw social distance indicator\n            color = (0, 255, 0) if interaction_data[\'appropriate_distance\'] else (0, 0, 255)\n            cv2.circle(frame, face_center, 50, color, 2)\n\n            # Display engagement level\n            cv2.putText(frame, f\'Engagement: {interaction_data["engagement_level"]:.2f}\',\n                       (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)\n\n            # Display distance\n            cv2.putText(frame, f\'Distance: {interaction_data["distance"]:.2f}m\',\n                       (10, 60), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)\n\n            # Display social distance zone\n            zone_text = self.get_social_distance_zone(interaction_data[\'distance\'])\n            cv2.putText(frame, f\'Zone: {zone_text}\',\n                       (10, 90), cv2.FONT_HERSHEY_SIMPLEX, 0.7, color, 2)\n\n        return frame\n\n    def get_social_distance_zone(self, distance):\n        """Get the social distance zone for a given distance"""\n        for zone in SocialDistance:\n            min_dist, max_dist = zone.value\n            if min_dist <= distance <= max_dist:\n                return zone.name\n        return \'UNKNOWN\'\n\nclass SocialBehaviorController:\n    """Controller for social behaviors and responses"""\n    def __init__(self):\n        self.visual_manager = VisualInteractionManager()\n        self.engagement_threshold = 0.5\n        self.response_delay = 1.0  # seconds\n        self.last_response_time = 0\n\n    def process_social_interaction(self, frame):\n        """Process social interaction in the current frame"""\n        # Track user attention\n        interaction_data = self.visual_manager.track_user_attention(frame)\n\n        # Update eye contact if enabled\n        if self.visual_manager.eye_contact_enabled and interaction_data[\'face_detected\']:\n            self.visual_manager.maintain_eye_contact(\n                interaction_data[\'face_center\'],\n                frame.shape\n            )\n\n        # Generate social feedback\n        frame_with_feedback = self.visual_manager.display_social_feedback(frame, interaction_data)\n\n        # Check for appropriate response timing\n        current_time = time.time()\n        if (current_time - self.last_response_time > self.response_delay and\n            interaction_data[\'engagement_level\'] > self.engagement_threshold):\n\n            # Generate appropriate social response\n            response = self.generate_social_response(interaction_data)\n            self.last_response_time = current_time\n\n            # In practice, this would trigger robot actions\n            print(f"Social response: {response}")\n\n        # Generate head movements based on engagement\n        if self.visual_manager.head_movement_enabled:\n            head_movements = self.visual_manager.generate_head_movement(\n                interaction_data[\'engagement_level\']\n            )\n            # Apply head movements to robot (simulated here)\n            self.apply_head_movements(head_movements)\n\n        return frame_with_feedback, interaction_data\n\n    def generate_social_response(self, interaction_data):\n        """Generate appropriate social response based on interaction data"""\n        if interaction_data[\'engagement_level\'] > 0.8:\n            responses = [\n                "Hello! It\'s great to see you!",\n                "I\'m happy you\'re here!",\n                "How can I help you today?"\n            ]\n        elif interaction_data[\'engagement_level\'] > 0.5:\n            responses = [\n                "Hello there!",\n                "Hi! How are you doing?",\n                "Good to see you!"\n            ]\n        else:\n            responses = [\n                "Hello!",\n                "Hi!",\n                "Welcome!"\n            ]\n\n        import random\n        return random.choice(responses)\n\n    def apply_head_movements(self, movements):\n        """Apply head movements to robot (simulated)"""\n        for movement in movements:\n            if movement[\'type\'] == \'nod\':\n                print(f"Head nod: amplitude={movement[\'amplitude\']}, frequency={movement[\'frequency\']}")\n            elif movement[\'type\'] == \'tilt\':\n                print(f"Head tilt: amplitude={movement[\'amplitude\']}, direction={movement[\'direction\']}")\n\n    def adjust_social_distance(self, preferred_zone):\n        """Adjust preferred social distance zone"""\n        if preferred_zone in SocialDistance.__members__:\n            self.visual_manager.social_distance_preference = SocialDistance[preferred_zone]\n            print(f"Social distance adjusted to {preferred_zone} zone")\n\n# Example usage\ndef example_visual_interaction():\n    social_controller = SocialBehaviorController()\n\n    # Simulate video capture\n    cap = cv2.VideoCapture(0)\n\n    print("Starting social interaction simulation...")\n    print("Show faces to the camera to test engagement detection and social responses")\n\n    while True:\n        ret, frame = cap.read()\n        if not ret:\n            break\n\n        # Process social interaction\n        processed_frame, interaction_data = social_controller.process_social_interaction(frame)\n\n        # Display the frame\n        cv2.imshow(\'Social Interaction\', processed_frame)\n\n        # Break on \'q\' key press\n        if cv2.waitKey(1) & 0xFF == ord(\'q\'):\n            break\n\n    cap.release()\n    cv2.destroyAllWindows()\n\nif __name__ == "__main__":\n    import time\n    try:\n        example_visual_interaction()\n    except ImportError as e:\n        print(f"Missing dependency: {e}")\n        print("Install: pip install opencv-python")\n'})}),"\n",(0,a.jsx)(n.h2,{id:"user-experience-design",children:"User Experience Design"}),"\n",(0,a.jsx)(n.h3,{id:"interaction-flow-design",children:"Interaction Flow Design"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'# interaction_flow.py\nimport time\nimport json\nfrom enum import Enum\nfrom dataclasses import dataclass\nfrom typing import List, Dict, Any, Callable\n\nclass InteractionState(Enum):\n    IDLE = "idle"\n    LISTENING = "listening"\n    PROCESSING = "processing"\n    RESPONDING = "responding"\n    AWAITING_CONFIRMATION = "awaiting_confirmation"\n    ERROR = "error"\n    SHUTDOWN = "shutdown"\n\nclass InteractionModality(Enum):\n    SPEECH = "speech"\n    GESTURE = "gesture"\n    TOUCH = "touch"\n    VISUAL = "visual"\n    MULTIMODAL = "multimodal"\n\n@dataclass\nclass InteractionEvent:\n    """Represents an interaction event"""\n    timestamp: float\n    modality: InteractionModality\n    content: Any\n    confidence: float = 1.0\n    user_id: str = "default_user"\n\nclass InteractionFlowManager:\n    """Manages the flow of human-robot interactions"""\n    def __init__(self):\n        self.current_state = InteractionState.IDLE\n        self.event_history = []\n        self.context_stack = []\n        self.response_callbacks = {}\n        self.timeout_settings = {\n            \'listening\': 10.0,  # seconds\n            \'processing\': 30.0,\n            \'response\': 5.0,\n            \'confirmation\': 15.0\n        }\n        self.last_event_time = time.time()\n        self.session_id = self.generate_session_id()\n\n    def generate_session_id(self):\n        """Generate a unique session ID"""\n        import uuid\n        return str(uuid.uuid4())\n\n    def add_event(self, event: InteractionEvent):\n        """Add an interaction event to the history"""\n        self.event_history.append(event)\n        self.last_event_time = event.timestamp\n\n        # Trigger state transition based on event\n        self.transition_state_based_on_event(event)\n\n    def transition_state_based_on_event(self, event: InteractionEvent):\n        """Transition interaction state based on event"""\n        if event.modality == InteractionModality.SPEECH and "hello" in str(event.content).lower():\n            self.set_state(InteractionState.LISTENING)\n        elif event.modality in [InteractionModality.SPEECH, InteractionModality.GESTURE]:\n            if self.current_state == InteractionState.IDLE:\n                self.set_state(InteractionState.LISTENING)\n            elif self.current_state == InteractionState.LISTENING:\n                self.set_state(InteractionState.PROCESSING)\n\n    def set_state(self, new_state: InteractionState):\n        """Set the interaction state"""\n        old_state = self.current_state\n        self.current_state = new_state\n\n        print(f"Interaction state: {old_state.value} -> {new_state.value}")\n\n        # Trigger state-specific actions\n        self.on_state_enter(new_state, old_state)\n\n    def on_state_enter(self, new_state: InteractionState, old_state: InteractionState):\n        """Actions to perform when entering a new state"""\n        if new_state == InteractionState.LISTENING:\n            self.on_enter_listening()\n        elif new_state == InteractionState.PROCESSING:\n            self.on_enter_processing()\n        elif new_state == InteractionState.RESPONDING:\n            self.on_enter_responding()\n        elif new_state == InteractionState.AWAITING_CONFIRMATION:\n            self.on_enter_awaiting_confirmation()\n\n    def on_enter_listening(self):\n        """Actions when entering listening state"""\n        print("Robot is now listening...")\n\n    def on_enter_processing(self):\n        """Actions when entering processing state"""\n        print("Processing user input...")\n\n    def on_enter_responding(self):\n        """Actions when entering responding state"""\n        print("Generating response...")\n\n    def on_enter_awaiting_confirmation(self):\n        """Actions when entering confirmation state"""\n        print("Awaiting user confirmation...")\n\n    def check_timeouts(self):\n        """Check for state timeouts"""\n        current_time = time.time()\n\n        if self.current_state == InteractionState.LISTENING:\n            if current_time - self.last_event_time > self.timeout_settings[\'listening\']:\n                self.handle_timeout(InteractionState.LISTENING)\n\n        elif self.current_state == InteractionState.PROCESSING:\n            if current_time - self.last_event_time > self.timeout_settings[\'processing\']:\n                self.handle_timeout(InteractionState.PROCESSING)\n\n    def handle_timeout(self, state: InteractionState):\n        """Handle timeout for a specific state"""\n        print(f"Timeout in {state.value} state")\n\n        if state == InteractionState.LISTENING:\n            self.set_state(InteractionState.IDLE)\n        elif state == InteractionState.PROCESSING:\n            self.set_state(InteractionState.ERROR)\n\n    def get_context(self) -> Dict[str, Any]:\n        """Get current interaction context"""\n        return {\n            \'session_id\': self.session_id,\n            \'current_state\': self.current_state.value,\n            \'event_count\': len(self.event_history),\n            \'last_event_time\': self.last_event_time,\n            \'context_stack\': self.context_stack.copy()\n        }\n\n    def push_context(self, context: Dict[str, Any]):\n        """Push context onto the stack"""\n        self.context_stack.append(context)\n\n    def pop_context(self) -> Dict[str, Any]:\n        """Pop context from the stack"""\n        if self.context_stack:\n            return self.context_stack.pop()\n        return {}\n\n    def clear_context(self):\n        """Clear all context"""\n        self.context_stack.clear()\n\nclass TaskOrientedInteraction:\n    """Handles task-oriented interactions with proper flow"""\n    def __init__(self, flow_manager: InteractionFlowManager):\n        self.flow_manager = flow_manager\n        self.active_tasks = {}\n        self.task_templates = self.load_task_templates()\n\n    def load_task_templates(self):\n        """Load predefined task templates"""\n        return {\n            \'navigation\': {\n                \'steps\': [\'destination_request\', \'route_confirmation\', \'navigation_start\', \'arrival_confirmation\'],\n                \'required_params\': [\'destination\'],\n                \'success_criteria\': [\'reached_destination\']\n            },\n            \'grasping\': {\n                \'steps\': [\'object_identification\', \'grasp_planning\', \'grasp_execution\', \'success_verification\'],\n                \'required_params\': [\'object_description\'],\n                \'success_criteria\': [\'object_grasped\']\n            },\n            \'information\': {\n                \'steps\': [\'query_understanding\', \'information_retrieval\', \'response_generation\'],\n                \'required_params\': [\'query\'],\n                \'success_criteria\': [\'information_provided\']\n            }\n        }\n\n    def start_task(self, task_type: str, params: Dict[str, Any]):\n        """Start a new task"""\n        if task_type not in self.task_templates:\n            raise ValueError(f"Unknown task type: {task_type}")\n\n        task_id = self.generate_task_id()\n        task_template = self.task_templates[task_type]\n\n        task = {\n            \'id\': task_id,\n            \'type\': task_type,\n            \'template\': task_template,\n            \'params\': params,\n            \'current_step\': 0,\n            \'status\': \'active\',\n            \'start_time\': time.time(),\n            \'history\': []\n        }\n\n        self.active_tasks[task_id] = task\n        self.execute_task_step(task_id)\n        return task_id\n\n    def generate_task_id(self):\n        """Generate a unique task ID"""\n        import uuid\n        return f"task_{str(uuid.uuid4())[:8]}"\n\n    def execute_task_step(self, task_id: str):\n        """Execute the current step of a task"""\n        task = self.active_tasks[task_id]\n        template = task[\'template\']\n        step_name = template[\'steps\'][task[\'current_step\']]\n\n        print(f"Executing task step: {step_name}")\n\n        # Execute step-specific logic\n        step_result = self.execute_step_logic(step_name, task[\'params\'])\n\n        # Update task history\n        task[\'history\'].append({\n            \'step\': step_name,\n            \'result\': step_result,\n            \'timestamp\': time.time()\n        })\n\n        # Check if step was successful\n        if step_result[\'success\']:\n            task[\'current_step\'] += 1\n\n            # Check if task is complete\n            if task[\'current_step\'] >= len(template[\'steps\']):\n                self.complete_task(task_id)\n            else:\n                # Continue to next step\n                self.execute_task_step(task_id)\n        else:\n            # Handle step failure\n            self.handle_step_failure(task_id, step_name, step_result)\n\n    def execute_step_logic(self, step_name: str, params: Dict[str, Any]):\n        """Execute the logic for a specific step"""\n        # This would contain task-specific step implementations\n        step_functions = {\n            \'destination_request\': self.step_destination_request,\n            \'route_confirmation\': self.step_route_confirmation,\n            \'navigation_start\': self.step_navigation_start,\n            \'arrival_confirmation\': self.step_arrival_confirmation,\n            \'object_identification\': self.step_object_identification,\n            \'grasp_planning\': self.step_grasp_planning,\n            \'grasp_execution\': self.step_grasp_execution,\n            \'success_verification\': self.step_success_verification,\n            \'query_understanding\': self.step_query_understanding,\n            \'information_retrieval\': self.step_information_retrieval,\n            \'response_generation\': self.step_response_generation\n        }\n\n        if step_name in step_functions:\n            return step_functions[step_name](params)\n        else:\n            return {\'success\': False, \'error\': f\'Unknown step: {step_name}\'}\n\n    def step_destination_request(self, params: Dict[str, Any]):\n        """Request destination from user"""\n        if \'destination\' in params:\n            return {\'success\': True, \'destination\': params[\'destination\']}\n        else:\n            # In practice, this would prompt user for destination\n            return {\'success\': False, \'error\': \'No destination provided\'}\n\n    def step_route_confirmation(self, params: Dict[str, Any]):\n        """Confirm route with user"""\n        return {\'success\': True, \'route_confirmed\': True}\n\n    def step_navigation_start(self, params: Dict[str, Any]):\n        """Start navigation"""\n        return {\'success\': True, \'navigation_started\': True}\n\n    def step_arrival_confirmation(self, params: Dict[str, Any]):\n        """Confirm arrival at destination"""\n        return {\'success\': True, \'arrived\': True}\n\n    def step_object_identification(self, params: Dict[str, Any]):\n        """Identify object to be grasped"""\n        if \'object_description\' in params:\n            return {\'success\': True, \'object\': params[\'object_description\']}\n        else:\n            return {\'success\': False, \'error\': \'No object description provided\'}\n\n    def step_grasp_planning(self, params: Dict[str, Any]):\n        """Plan the grasp"""\n        return {\'success\': True, \'grasp_plan\': \'calculated\'}\n\n    def step_grasp_execution(self, params: Dict[str, Any]):\n        """Execute the grasp"""\n        return {\'success\': True, \'grasp_executed\': True}\n\n    def step_success_verification(self, params: Dict[str, Any]):\n        """Verify grasp success"""\n        return {\'success\': True, \'success_verified\': True}\n\n    def step_query_understanding(self, params: Dict[str, Any]):\n        """Understand the information query"""\n        if \'query\' in params:\n            return {\'success\': True, \'understood_query\': params[\'query\']}\n        else:\n            return {\'success\': False, \'error\': \'No query provided\'}\n\n    def step_information_retrieval(self, params: Dict[str, Any]):\n        """Retrieve requested information"""\n        return {\'success\': True, \'information\': \'retrieved\'}\n\n    def step_response_generation(self, params: Dict[str, Any]):\n        """Generate response to user"""\n        return {\'success\': True, \'response_generated\': True}\n\n    def handle_step_failure(self, task_id: str, step_name: str, result: Dict[str, Any]):\n        """Handle failure of a task step"""\n        print(f"Task step failed: {step_name}, Error: {result.get(\'error\', \'Unknown error\')}")\n\n        # Implement recovery logic\n        task = self.active_tasks[task_id]\n        if task[\'current_step\'] < 3:  # Retry for first few steps\n            print("Retrying step...")\n            self.execute_task_step(task_id)\n        else:\n            # Mark task as failed\n            self.fail_task(task_id, result[\'error\'])\n\n    def complete_task(self, task_id: str):\n        """Complete a task successfully"""\n        task = self.active_tasks[task_id]\n        task[\'status\'] = \'completed\'\n        task[\'end_time\'] = time.time()\n        task[\'duration\'] = task[\'end_time\'] - task[\'start_time\']\n\n        print(f"Task {task_id} completed successfully in {task[\'duration\']:.2f}s")\n\n        # Clean up\n        del self.active_tasks[task_id]\n\n    def fail_task(self, task_id: str, error: str):\n        """Fail a task"""\n        task = self.active_tasks[task_id]\n        task[\'status\'] = \'failed\'\n        task[\'error\'] = error\n        task[\'end_time\'] = time.time()\n\n        print(f"Task {task_id} failed: {error}")\n\n        # Clean up\n        del self.active_tasks[task_id]\n\n    def get_active_tasks(self):\n        """Get information about active tasks"""\n        return {\n            task_id: {\n                \'type\': task[\'type\'],\n                \'status\': task[\'status\'],\n                \'current_step\': task[\'current_step\'],\n                \'progress\': task[\'current_step\'] / len(task[\'template\'][\'steps\'])\n            }\n            for task_id, task in self.active_tasks.items()\n        }\n\n# Example usage\ndef example_interaction_flow():\n    flow_manager = InteractionFlowManager()\n    task_manager = TaskOrientedInteraction(flow_manager)\n\n    print("Interaction Flow Manager Example")\n    print(f"Initial state: {flow_manager.current_state.value}")\n\n    # Simulate some interaction events\n    events = [\n        InteractionEvent(time.time(), InteractionModality.SPEECH, "Hello robot", 0.9),\n        InteractionEvent(time.time(), InteractionModality.SPEECH, "Navigate to kitchen", 0.8),\n    ]\n\n    for event in events:\n        flow_manager.add_event(event)\n        time.sleep(0.1)  # Small delay to simulate real timing\n\n    print(f"Final state: {flow_manager.current_state.value}")\n    print(f"Context: {flow_manager.get_context()}")\n\n    # Start a navigation task\n    task_params = {\n        \'destination\': \'kitchen\',\n        \'user_preference\': \'shortest_path\'\n    }\n\n    task_id = task_manager.start_task(\'navigation\', task_params)\n    print(f"Started task: {task_id}")\n\n    # Simulate task execution\n    time.sleep(2)  # Wait for task to complete\n    print(f"Active tasks: {task_manager.get_active_tasks()}")\n\nif __name__ == "__main__":\n    example_interaction_flow()\n'})}),"\n",(0,a.jsx)(n.h3,{id:"context-aware-interaction",children:"Context-Aware Interaction"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"# context_aware_interaction.py\nimport datetime\nimport json\nfrom typing import Dict, List, Any, Optional\nfrom dataclasses import dataclass, asdict\nfrom enum import Enum\n\nclass ContextType(Enum):\n    TEMPORAL = \"temporal\"\n    SPATIAL = \"spatial\"\n    SOCIAL = \"social\"\n    TASK = \"task\"\n    EMOTIONAL = \"emotional\"\n    ENVIRONMENTAL = \"environmental\"\n\n@dataclass\nclass TemporalContext:\n    \"\"\"Temporal context information\"\"\"\n    current_time: datetime.datetime\n    day_of_week: int  # 0=Monday, 6=Sunday\n    time_of_day: str  # morning, afternoon, evening, night\n    season: str\n\n@dataclass\nclass SpatialContext:\n    \"\"\"Spatial context information\"\"\"\n    location: str\n    room_type: str\n    coordinates: tuple  # (x, y, z)\n    orientation: tuple  # (roll, pitch, yaw)\n    environment_map: Optional[Dict] = None\n\n@dataclass\nclass SocialContext:\n    \"\"\"Social context information\"\"\"\n    user_count: int\n    user_relationships: Dict[str, str]  # user_id -> relationship\n    interaction_history: List[Dict[str, Any]]\n    group_dynamics: str  # formal, casual, family, etc.\n\n@dataclass\nclass TaskContext:\n    \"\"\"Task context information\"\"\"\n    active_task: Optional[str]\n    task_progress: float\n    task_priority: int\n    task_dependencies: List[str]\n    task_deadline: Optional[datetime.datetime]\n\n@dataclass\nclass EmotionalContext:\n    \"\"\"Emotional context information\"\"\"\n    user_mood: str\n    user_stress_level: float  # 0.0 to 1.0\n    interaction_tone: str\n    empathy_level: float  # 0.0 to 1.0\n\n@dataclass\nclass EnvironmentalContext:\n    \"\"\"Environmental context information\"\"\"\n    lighting: str  # bright, dim, dark\n    noise_level: float  # 0.0 to 1.0\n    temperature: float  # in Celsius\n    occupancy: bool\n    privacy_level: str  # public, semi-private, private\n\nclass ContextManager:\n    \"\"\"Manages all context information for the robot\"\"\"\n    def __init__(self):\n        self.temporal_context = self.update_temporal_context()\n        self.spatial_context = SpatialContext(\n            location=\"unknown\",\n            room_type=\"unknown\",\n            coordinates=(0.0, 0.0, 0.0),\n            orientation=(0.0, 0.0, 0.0)\n        )\n        self.social_context = SocialContext(\n            user_count=0,\n            user_relationships={},\n            interaction_history=[],\n            group_dynamics=\"casual\"\n        )\n        self.task_context = TaskContext(\n            active_task=None,\n            task_progress=0.0,\n            task_priority=0,\n            task_dependencies=[],\n            task_deadline=None\n        )\n        self.emotional_context = EmotionalContext(\n            user_mood=\"neutral\",\n            user_stress_level=0.5,\n            interaction_tone=\"neutral\",\n            empathy_level=0.5\n        )\n        self.environmental_context = EnvironmentalContext(\n            lighting=\"normal\",\n            noise_level=0.5,\n            temperature=22.0,\n            occupancy=False,\n            privacy_level=\"public\"\n        )\n\n        self.context_history = []\n        self.max_history_length = 100\n\n    def update_temporal_context(self) -> TemporalContext:\n        \"\"\"Update temporal context based on current time\"\"\"\n        now = datetime.datetime.now()\n\n        # Determine time of day\n        hour = now.hour\n        if 5 <= hour < 12:\n            time_of_day = \"morning\"\n        elif 12 <= hour < 17:\n            time_of_day = \"afternoon\"\n        elif 17 <= hour < 21:\n            time_of_day = \"evening\"\n        else:\n            time_of_day = \"night\"\n\n        # Determine season\n        month = now.month\n        if month in [12, 1, 2]:\n            season = \"winter\"\n        elif month in [3, 4, 5]:\n            season = \"spring\"\n        elif month in [6, 7, 8]:\n            season = \"summer\"\n        else:\n            season = \"fall\"\n\n        return TemporalContext(\n            current_time=now,\n            day_of_week=now.weekday(),\n            time_of_day=time_of_day,\n            season=season\n        )\n\n    def update_spatial_context(self, location: str, room_type: str, coordinates: tuple, orientation: tuple):\n        \"\"\"Update spatial context\"\"\"\n        self.spatial_context = SpatialContext(\n            location=location,\n            room_type=room_type,\n            coordinates=coordinates,\n            orientation=orientation\n        )\n\n    def update_social_context(self, user_count: int, user_relationships: Dict[str, str]):\n        \"\"\"Update social context\"\"\"\n        self.social_context.user_count = user_count\n        self.social_context.user_relationships = user_relationships\n\n    def update_task_context(self, active_task: Optional[str], progress: float = 0.0):\n        \"\"\"Update task context\"\"\"\n        self.task_context.active_task = active_task\n        self.task_context.task_progress = progress\n\n    def update_emotional_context(self, user_mood: str, stress_level: float):\n        \"\"\"Update emotional context\"\"\"\n        self.emotional_context.user_mood = user_mood\n        self.emotional_context.user_stress_level = stress_level\n\n    def update_environmental_context(self, lighting: str, noise_level: float, temperature: float):\n        \"\"\"Update environmental context\"\"\"\n        self.environmental_context.lighting = lighting\n        self.environmental_context.noise_level = noise_level\n        self.environmental_context.temperature = temperature\n\n    def get_context_profile(self) -> Dict[str, Any]:\n        \"\"\"Get complete context profile\"\"\"\n        return {\n            'temporal': asdict(self.temporal_context),\n            'spatial': asdict(self.spatial_context),\n            'social': asdict(self.social_context),\n            'task': asdict(self.task_context),\n            'emotional': asdict(self.emotional_context),\n            'environmental': asdict(self.environmental_context),\n            'timestamp': datetime.datetime.now().isoformat()\n        }\n\n    def get_context_for_interaction(self) -> Dict[str, Any]:\n        \"\"\"Get context relevant for interaction decisions\"\"\"\n        profile = self.get_context_profile()\n\n        # Extract key context indicators\n        interaction_context = {\n            'time_of_day': profile['temporal']['time_of_day'],\n            'location': profile['spatial']['location'],\n            'user_count': profile['social']['user_count'],\n            'active_task': profile['task']['active_task'],\n            'user_mood': profile['emotional']['user_mood'],\n            'environment': {\n                'lighting': profile['environmental']['lighting'],\n                'noise': profile['environmental']['noise_level'],\n                'temperature': profile['environmental']['temperature']\n            }\n        }\n\n        return interaction_context\n\n    def adapt_behavior_to_context(self) -> Dict[str, Any]:\n        \"\"\"Adapt robot behavior based on current context\"\"\"\n        context = self.get_context_for_interaction()\n        adaptations = {}\n\n        # Adapt based on time of day\n        if context['time_of_day'] in ['night', 'early_morning']:\n            adaptations['volume'] = 'low'\n            adaptations['energy'] = 'calm'\n            adaptations['response_time'] = 'patient'\n        elif context['time_of_day'] == 'afternoon':\n            adaptations['volume'] = 'normal'\n            adaptations['energy'] = 'engaged'\n            adaptations['response_time'] = 'responsive'\n\n        # Adapt based on user count\n        if context['user_count'] > 1:\n            adaptations['interaction_mode'] = 'group'\n            adaptations['attention'] = 'distributed'\n        else:\n            adaptations['interaction_mode'] = 'individual'\n            adaptations['attention'] = 'focused'\n\n        # Adapt based on user mood\n        if context['user_mood'] == 'stressed':\n            adaptations['tone'] = 'soothing'\n            adaptations['pace'] = 'slow'\n            adaptations['helpfulness'] = 'high'\n        elif context['user_mood'] == 'happy':\n            adaptations['tone'] = 'cheerful'\n            adaptations['energy'] = 'positive'\n\n        # Adapt based on environment\n        env = context['environment']\n        if env['noise'] > 0.7:\n            adaptations['volume'] = 'high'\n            adaptations['repetition'] = 'increased'\n        if env['lighting'] == 'dim':\n            adaptations['visual_feedback'] = 'reduced'\n            adaptations['verbal_feedback'] = 'increased'\n\n        return adaptations\n\n    def predict_user_needs(self) -> List[str]:\n        \"\"\"Predict user needs based on context\"\"\"\n        needs = []\n        context = self.get_context_for_interaction()\n\n        # Predict based on time and location\n        if context['time_of_day'] == 'morning' and context['location'] == 'kitchen':\n            needs.append('coffee')\n            needs.append('weather information')\n            needs.append('schedule reminder')\n\n        if context['time_of_day'] == 'evening' and context['location'] == 'living_room':\n            needs.append('entertainment')\n            needs.append('relaxation')\n\n        # Predict based on user mood\n        if context['user_mood'] == 'tired':\n            needs.append('rest')\n            needs.append('calming activity')\n\n        if context['user_mood'] == 'excited':\n            needs.append('engaging activity')\n            needs.append('information sharing')\n\n        # Predict based on social context\n        if context['user_count'] > 1:\n            needs.append('group activity')\n            needs.append('mediation')\n\n        return list(set(needs))  # Remove duplicates\n\n    def generate_contextual_response(self, user_input: str) -> str:\n        \"\"\"Generate response considering current context\"\"\"\n        context = self.get_context_for_interaction()\n        adaptations = self.adapt_behavior_to_context()\n        predicted_needs = self.predict_user_needs()\n\n        # Generate contextual response\n        response_parts = []\n\n        # Acknowledge context\n        if context['time_of_day'] in ['morning', 'afternoon', 'evening']:\n            response_parts.append(f\"Good {context['time_of_day']}!\")\n\n        # Address predicted needs if relevant\n        if predicted_needs:\n            relevant_needs = [need for need in predicted_needs if need in user_input.lower()]\n            if relevant_needs:\n                response_parts.append(f\"I notice you might need {', '.join(relevant_needs)}. How can I help?\")\n\n        # Adapt response based on mood\n        if context['user_mood'] == 'stressed':\n            response_parts.append(\"I'm here to help you relax. What would be most helpful right now?\")\n\n        # Default response if no specific context applies\n        if not response_parts:\n            response_parts.append(\"How can I assist you today?\")\n\n        return \" \".join(response_parts)\n\nclass ContextAwareInteractionManager:\n    \"\"\"Main manager for context-aware interactions\"\"\"\n    def __init__(self):\n        self.context_manager = ContextManager()\n        self.interaction_history = []\n        self.user_preferences = {}\n\n    def process_user_input(self, user_input: str, user_id: str = \"default_user\") -> str:\n        \"\"\"Process user input with full context awareness\"\"\"\n        # Update temporal context\n        self.context_manager.update_temporal_context()\n\n        # Analyze input for context clues\n        self.analyze_input_context(user_input, user_id)\n\n        # Generate contextual response\n        response = self.context_manager.generate_contextual_response(user_input)\n\n        # Record interaction\n        self.record_interaction(user_input, response, user_id)\n\n        return response\n\n    def analyze_input_context(self, user_input: str, user_id: str):\n        \"\"\"Analyze user input for contextual information\"\"\"\n        input_lower = user_input.lower()\n\n        # Analyze emotional content\n        emotional_keywords = {\n            'happy': ['happy', 'great', 'wonderful', 'excellent'],\n            'sad': ['sad', 'upset', 'depressed', 'unhappy'],\n            'angry': ['angry', 'frustrated', 'mad', 'annoyed'],\n            'stressed': ['stressed', 'overwhelmed', 'tired', 'exhausted']\n        }\n\n        for mood, keywords in emotional_keywords.items():\n            if any(keyword in input_lower for keyword in keywords):\n                # Estimate stress level based on intensity words\n                stress_keywords = ['very', 'really', 'extremely', 'super']\n                stress_level = 0.7 if any(word in input_lower for word in stress_keywords) else 0.5\n                self.context_manager.update_emotional_context(mood, stress_level)\n                break\n\n        # Analyze spatial references\n        location_keywords = {\n            'kitchen': ['kitchen', 'cooking', 'food', 'eat'],\n            'bedroom': ['bedroom', 'sleep', 'bed', 'rest'],\n            'office': ['office', 'work', 'computer', 'meeting'],\n            'living_room': ['living room', 'couch', 'tv', 'relax']\n        }\n\n        for location, keywords in location_keywords.items():\n            if any(keyword in input_lower for keyword in keywords):\n                self.context_manager.update_spatial_context(location, location, (0, 0, 0), (0, 0, 0))\n                break\n\n    def record_interaction(self, user_input: str, response: str, user_id: str):\n        \"\"\"Record interaction in history\"\"\"\n        interaction = {\n            'timestamp': datetime.datetime.now().isoformat(),\n            'user_id': user_id,\n            'user_input': user_input,\n            'robot_response': response,\n            'context': self.context_manager.get_context_profile()\n        }\n\n        self.interaction_history.append(interaction)\n\n        # Keep history size manageable\n        if len(self.interaction_history) > 1000:\n            self.interaction_history = self.interaction_history[-500:]\n\n    def get_user_preferences(self, user_id: str) -> Dict[str, Any]:\n        \"\"\"Get user preferences based on interaction history\"\"\"\n        if user_id in self.user_preferences:\n            return self.user_preferences[user_id]\n\n        # Analyze interaction history for preferences\n        user_interactions = [ih for ih in self.interaction_history if ih['user_id'] == user_id]\n\n        if not user_interactions:\n            return {}\n\n        # Extract common patterns\n        common_topics = {}\n        for interaction in user_interactions[-20:]:  # Analyze last 20 interactions\n            input_text = interaction['user_input'].lower()\n            for topic in ['weather', 'news', 'schedule', 'music', 'jokes', 'help']:\n                if topic in input_text:\n                    common_topics[topic] = common_topics.get(topic, 0) + 1\n\n        preferences = {\n            'preferred_topics': sorted(common_topics.items(), key=lambda x: x[1], reverse=True)[:3],\n            'interaction_style': 'casual',  # Default, could be personalized\n            'preferred_time': 'afternoon'   # Default, could be learned\n        }\n\n        self.user_preferences[user_id] = preferences\n        return preferences\n\n# Example usage\ndef example_context_aware_interaction():\n    manager = ContextAwareInteractionManager()\n\n    # Simulate some interactions\n    test_inputs = [\n        \"Hello! I'm feeling stressed today.\",\n        \"What's the weather like?\",\n        \"I'm in the kitchen and need to make coffee.\",\n        \"Good evening! How was your day?\"\n    ]\n\n    for i, user_input in enumerate(test_inputs):\n        print(f\"\\nInteraction {i+1}:\")\n        print(f\"User: {user_input}\")\n\n        response = manager.process_user_input(user_input, f\"user_{i}\")\n        print(f\"Robot: {response}\")\n\n        # Show current context\n        context = manager.context_manager.get_context_for_interaction()\n        print(f\"Context: {context}\")\n\n        # Show adaptations\n        adaptations = manager.context_manager.adapt_behavior_to_context()\n        print(f\"Adaptations: {adaptations}\")\n\n    # Show predicted needs\n    needs = manager.context_manager.predict_user_needs()\n    print(f\"\\nPredicted user needs: {needs}\")\n\nif __name__ == \"__main__\":\n    example_context_aware_interaction()\n"})}),"\n",(0,a.jsx)(n.h2,{id:"knowledge-check",children:"Knowledge Check"}),"\n",(0,a.jsxs)(n.ol,{children:["\n",(0,a.jsx)(n.li,{children:"What are the key principles of natural human-robot interaction?"}),"\n",(0,a.jsx)(n.li,{children:"How do different communication modalities (speech, gesture, visual) complement each other?"}),"\n",(0,a.jsx)(n.li,{children:"What is the importance of context awareness in human-robot interaction?"}),"\n",(0,a.jsx)(n.li,{children:"How can robots adapt their behavior based on social cues and environmental context?"}),"\n"]}),"\n",(0,a.jsx)(n.h2,{id:"summary",children:"Summary"}),"\n",(0,a.jsx)(n.p,{children:"This chapter covered natural human-robot interaction, including voice-based and gesture-based interaction systems, visual interaction and social cues, user experience design principles, and context-aware interaction systems. We explored how humanoid robots can interact naturally with humans using multiple modalities and adapt their behavior based on context and social cues."}),"\n",(0,a.jsx)(n.h2,{id:"next-steps",children:"Next Steps"}),"\n",(0,a.jsx)(n.p,{children:"In the next module, we'll explore the integration of Large Language Models (LLMs) in robotics, covering conversational AI, speech recognition, natural language understanding, and cognitive planning with LLMs for humanoid robotics applications."})]})}function _(e={}){const{wrapper:n}={...(0,s.R)(),...e.components};return n?(0,a.jsx)(n,{...e,children:(0,a.jsx)(d,{...e})}):d(e)}}}]);