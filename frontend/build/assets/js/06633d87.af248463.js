"use strict";(globalThis.webpackChunkphysical_ai_humanoid_robotics_book=globalThis.webpackChunkphysical_ai_humanoid_robotics_book||[]).push([[830],{6883(e,n,t){t.r(n),t.d(n,{assets:()=>c,contentTitle:()=>r,default:()=>p,frontMatter:()=>s,metadata:()=>o,toc:()=>l});var a=t(4848),i=t(8453);const s={sidebar_position:19,title:"Chapter 19: Cognitive Planning with LLMs"},r="Chapter 19: Cognitive Planning with LLMs",o={id:"part6/chapter19",title:"Chapter 19: Cognitive Planning with LLMs",description:"Learning Objectives",source:"@site/docs/part6/chapter19.md",sourceDirName:"part6",slug:"/part6/chapter19",permalink:"/Physical-AI-Humanoid-Robotics/docs/part6/chapter19",draft:!1,unlisted:!1,editUrl:"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/part6/chapter19.md",tags:[],version:"current",sidebarPosition:19,frontMatter:{sidebar_position:19,title:"Chapter 19: Cognitive Planning with LLMs"},sidebar:"tutorialSidebar",previous:{title:"Chapter 18: Speech Recognition and Natural Language Understanding",permalink:"/Physical-AI-Humanoid-Robotics/docs/part6/chapter18"},next:{title:"Chapter 20: The Autonomous Humanoid Capstone Project",permalink:"/Physical-AI-Humanoid-Robotics/docs/part6/chapter20"}},c={},l=[{value:"Learning Objectives",id:"learning-objectives",level:2},{value:"Introduction to Cognitive Planning",id:"introduction-to-cognitive-planning",level:2},{value:"The Cognitive Planning Challenge",id:"the-cognitive-planning-challenge",level:3},{value:"Planning Hierarchy in Cognitive Systems",id:"planning-hierarchy-in-cognitive-systems",level:3},{value:"Natural Language to Action Mapping",id:"natural-language-to-action-mapping",level:2},{value:"Intent Recognition and Action Extraction",id:"intent-recognition-and-action-extraction",level:3},{value:"Planning and Execution Framework",id:"planning-and-execution-framework",level:2},{value:"Hierarchical Task Decomposition",id:"hierarchical-task-decomposition",level:3},{value:"Task Decomposition and Execution",id:"task-decomposition-and-execution",level:2},{value:"Advanced Task Decomposition with LLMs",id:"advanced-task-decomposition-with-llms",level:3},{value:"Knowledge Check",id:"knowledge-check",level:2},{value:"Summary",id:"summary",level:2},{value:"Next Steps",id:"next-steps",level:2}];function d(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,i.R)(),...e.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(n.h1,{id:"chapter-19-cognitive-planning-with-llms",children:"Chapter 19: Cognitive Planning with LLMs"}),"\n",(0,a.jsx)(n.h2,{id:"learning-objectives",children:"Learning Objectives"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"Translate natural language commands into executable robotic actions"}),"\n",(0,a.jsx)(n.li,{children:"Implement cognitive planning systems using LLMs"}),"\n",(0,a.jsx)(n.li,{children:"Design planning and execution frameworks for humanoid robots"}),"\n",(0,a.jsx)(n.li,{children:"Create robust task decomposition and execution systems"}),"\n"]}),"\n",(0,a.jsx)(n.h2,{id:"introduction-to-cognitive-planning",children:"Introduction to Cognitive Planning"}),"\n",(0,a.jsx)(n.p,{children:"Cognitive planning represents the ability of robots to understand high-level human commands expressed in natural language and decompose them into executable action sequences. Large Language Models (LLMs) excel at this task by bridging the gap between human intention and robot execution, enabling more intuitive and flexible human-robot interaction."}),"\n",(0,a.jsx)(n.h3,{id:"the-cognitive-planning-challenge",children:"The Cognitive Planning Challenge"}),"\n",(0,a.jsx)(n.p,{children:"Traditional robotics requires precise, low-level commands that are difficult for humans to formulate. Cognitive planning addresses this by:"}),"\n",(0,a.jsxs)(n.ol,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Understanding Intent"}),": Extracting the user's true goal from natural language"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"World Modeling"}),": Understanding the current state of the environment"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Action Decomposition"}),": Breaking down complex tasks into primitive actions"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Plan Execution"}),": Executing the plan while monitoring for failures"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Adaptive Reasoning"}),": Adjusting plans based on changing conditions"]}),"\n"]}),"\n",(0,a.jsx)(n.h3,{id:"planning-hierarchy-in-cognitive-systems",children:"Planning Hierarchy in Cognitive Systems"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{children:'High-Level Goal: "Bring me a cup of coffee from the kitchen"\n    \u2193\nTask Decomposition: [Navigate to kitchen, Find coffee, Grasp cup, Pour coffee, Navigate to user]\n    \u2193\nAction Sequences: [Move forward, Turn right, Detect object, Grasp object, ...]\n    \u2193\nPrimitive Actions: [Motor commands, sensor activations, ...]\n'})}),"\n",(0,a.jsx)(n.h2,{id:"natural-language-to-action-mapping",children:"Natural Language to Action Mapping"}),"\n",(0,a.jsx)(n.h3,{id:"intent-recognition-and-action-extraction",children:"Intent Recognition and Action Extraction"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'# cognitive_planning.py\nimport json\nimport re\nfrom typing import Dict, List, Any, Optional, Tuple\nfrom dataclasses import dataclass\nfrom enum import Enum\nimport asyncio\nimport time\n\nclass ActionCategory(Enum):\n    NAVIGATION = "navigation"\n    MANIPULATION = "manipulation"\n    PERCEPTION = "perception"\n    COMMUNICATION = "communication"\n    SYSTEM = "system"\n\n@dataclass\nclass PrimitiveAction:\n    """Lowest level action that can be executed"""\n    action_type: str\n    parameters: Dict[str, Any]\n    description: str\n    required_sensors: List[str]\n    required_actuators: List[str]\n\n@dataclass\nclass TaskStep:\n    """Intermediate task step composed of primitive actions"""\n    id: str\n    name: str\n    category: ActionCategory\n    dependencies: List[str]  # IDs of steps this depends on\n    primitive_actions: List[PrimitiveAction]\n    estimated_duration: float\n    success_criteria: List[str]\n\n@dataclass\nclass CognitivePlan:\n    """Complete cognitive plan"""\n    id: str\n    goal: str\n    steps: List[TaskStep]\n    current_step: int = 0\n    status: str = "planning"\n    created_at: float = 0.0\n    started_at: Optional[float] = None\n    completed_at: Optional[float] = None\n\nclass ActionMapper:\n    """Maps natural language to robot actions"""\n    def __init__(self):\n        self.action_database = self.initialize_action_database()\n        self.location_map = self.initialize_location_map()\n        self.object_map = self.initialize_object_map()\n\n    def initialize_action_database(self) -> Dict[str, Dict[str, Any]]:\n        """Initialize the action database with mappings"""\n        return {\n            # Navigation actions\n            "navigate_to": {\n                "category": ActionCategory.NAVIGATION,\n                "keywords": ["go to", "move to", "navigate to", "walk to", "head to", "go toward"],\n                "required_params": ["destination"],\n                "primitive_actions": [\n                    PrimitiveAction(\n                        action_type="move_base",\n                        parameters={"target_pose": None, "speed": "normal"},\n                        description="Move robot to specified location",\n                        required_sensors=["odometry", "lidar"],\n                        required_actuators=["base_motors"]\n                    )\n                ]\n            },\n            "navigate_from_to": {\n                "category": ActionCategory.NAVIGATION,\n                "keywords": ["go from", "move from", "navigate from"],\n                "required_params": ["start_location", "end_location"],\n                "primitive_actions": [\n                    PrimitiveAction(\n                        action_type="move_base",\n                        parameters={"target_pose": None, "speed": "normal"},\n                        description="Move robot from start to end location",\n                        required_sensors=["odometry", "lidar"],\n                        required_actuators=["base_motors"]\n                    )\n                ]\n            },\n            "explore_area": {\n                "category": ActionCategory.NAVIGATION,\n                "keywords": ["explore", "patrol", "survey", "check"],\n                "required_params": ["area"],\n                "primitive_actions": [\n                    PrimitiveAction(\n                        action_type="explore",\n                        parameters={"area_bounds": None, "coverage_percent": 0.9},\n                        description="Explore specified area systematically",\n                        required_sensors=["lidar", "camera"],\n                        required_actuators=["base_motors"]\n                    )\n                ]\n            },\n\n            # Manipulation actions\n            "grasp_object": {\n                "category": ActionCategory.MANIPULATION,\n                "keywords": ["grasp", "grab", "pick up", "take", "hold", "catch"],\n                "required_params": ["object"],\n                "primitive_actions": [\n                    PrimitiveAction(\n                        action_type="move_to_object",\n                        parameters={"object_name": None, "approach_distance": 0.1},\n                        description="Move end-effector to object",\n                        required_sensors=["camera", "arm_encoders"],\n                        required_actuators=["arm_joints", "gripper"]\n                    ),\n                    PrimitiveAction(\n                        action_type="grasp",\n                        parameters={"grip_strength": 0.5},\n                        description="Grasp the object",\n                        required_sensors=["force_sensors"],\n                        required_actuators=["gripper"]\n                    )\n                ]\n            },\n            "release_object": {\n                "category": ActionCategory.MANIPULATION,\n                "keywords": ["release", "let go", "drop", "put down", "place"],\n                "required_params": ["object"],\n                "primitive_actions": [\n                    PrimitiveAction(\n                        action_type="release",\n                        parameters={"object_name": None},\n                        description="Release the held object",\n                        required_sensors=["force_sensors"],\n                        required_actuators=["gripper"]\n                    )\n                ]\n            },\n            "manipulate_object": {\n                "category": ActionCategory.MANIPULATION,\n                "keywords": ["move", "push", "pull", "lift", "carry", "transport"],\n                "required_params": ["object", "destination"],\n                "primitive_actions": [\n                    PrimitiveAction(\n                        action_type="grasp",\n                        parameters={"object_name": None},\n                        description="Grasp the object",\n                        required_sensors=["camera", "force_sensors"],\n                        required_actuators=["gripper", "arm_joints"]\n                    ),\n                    PrimitiveAction(\n                        action_type="move_to",\n                        parameters={"target_pose": None},\n                        description="Move to destination while holding object",\n                        required_sensors=["odometry", "lidar"],\n                        required_actuators=["base_motors", "arm_joints"]\n                    ),\n                    PrimitiveAction(\n                        action_type="release",\n                        parameters={"object_name": None},\n                        description="Release the object at destination",\n                        required_sensors=["force_sensors"],\n                        required_actuators=["gripper"]\n                    )\n                ]\n            },\n\n            # Perception actions\n            "detect_object": {\n                "category": ActionCategory.PERCEPTION,\n                "keywords": ["find", "locate", "detect", "identify", "spot", "see"],\n                "required_params": ["object"],\n                "primitive_actions": [\n                    PrimitiveAction(\n                        action_type="object_detection",\n                        parameters={"target_object": None, "confidence_threshold": 0.7},\n                        description="Detect and locate specified object",\n                        required_sensors=["camera", "depth_camera"],\n                        required_actuators=[]\n                    )\n                ]\n            },\n            "inspect_area": {\n                "category": ActionCategory.PERCEPTION,\n                "keywords": ["inspect", "check", "examine", "scan", "look_at"],\n                "required_params": ["location"],\n                "primitive_actions": [\n                    PrimitiveAction(\n                        action_type="panorama_scan",\n                        parameters={"center_pose": None, "angle_span": 360},\n                        description="Scan area for inspection",\n                        required_sensors=["camera", "lidar"],\n                        required_actuators=["pan_tilt_unit"]\n                    )\n                ]\n            },\n\n            # Communication actions\n            "speak": {\n                "category": ActionCategory.COMMUNICATION,\n                "keywords": ["say", "speak", "tell", "announce", "reply", "respond"],\n                "required_params": ["message"],\n                "primitive_actions": [\n                    PrimitiveAction(\n                        action_type="text_to_speech",\n                        parameters={"text": None, "voice": "default"},\n                        description="Convert text to speech",\n                        required_sensors=[],\n                        required_actuators=["speakers"]\n                    )\n                ]\n            },\n            "listen": {\n                "category": ActionCategory.COMMUNICATION,\n                "keywords": ["listen", "hear", "pay attention", "wait for"],\n                "required_params": ["duration"],\n                "primitive_actions": [\n                    PrimitiveAction(\n                        action_type="start_listening",\n                        parameters={"timeout": 10.0, "keywords": []},\n                        description="Listen for user input",\n                        required_sensors=["microphone"],\n                        required_actuators=[]\n                    )\n                ]\n            }\n        }\n\n    def initialize_location_map(self) -> Dict[str, Any]:\n        """Initialize location mappings"""\n        return {\n            "kitchen": {"x": 2.0, "y": 1.0, "z": 0.0, "frame": "map"},\n            "living_room": {"x": -1.0, "y": 0.5, "z": 0.0, "frame": "map"},\n            "bedroom": {"x": 0.0, "y": -2.0, "z": 0.0, "frame": "map"},\n            "office": {"x": 1.5, "y": -1.0, "z": 0.0, "frame": "map"},\n            "entrance": {"x": 0.0, "y": 0.0, "z": 0.0, "frame": "map"}\n        }\n\n    def initialize_object_map(self) -> Dict[str, Any]:\n        """Initialize object mappings"""\n        return {\n            "cup": {"type": "graspable", "size": "small", "grasp_method": "top_grasp"},\n            "bottle": {"type": "graspable", "size": "medium", "grasp_method": "side_grasp"},\n            "book": {"type": "graspable", "size": "medium", "grasp_method": "edge_grasp"},\n            "phone": {"type": "graspable", "size": "small", "grasp_method": "pinch_grasp"},\n            "chair": {"type": "large", "grasp_method": "not_graspable"},\n            "table": {"type": "large", "grasp_method": "not_graspable"}\n        }\n\n    def extract_intent_and_parameters(self, natural_language: str) -> Optional[Tuple[str, Dict[str, Any]]]:\n        """Extract intent and parameters from natural language"""\n        # Convert to lowercase for easier matching\n        text_lower = natural_language.lower()\n\n        # Find the best matching action\n        best_match = None\n        best_score = 0\n\n        for action_name, action_def in self.action_database.items():\n            for keyword in action_def["keywords"]:\n                # Calculate similarity score\n                score = self.calculate_similarity(keyword, text_lower)\n                if score > best_score:\n                    best_score = score\n                    best_match = action_name\n\n        if not best_match:\n            return None\n\n        # Extract parameters\n        action_def = self.action_database[best_match]\n        parameters = self.extract_parameters(text_lower, action_def["required_params"])\n\n        # Validate required parameters\n        for param in action_def["required_params"]:\n            if param not in parameters or parameters[param] is None:\n                # Try to infer from context or ask for clarification\n                if param == "destination" and "to" in text_lower:\n                    # Extract location after "to"\n                    match = re.search(r\'to\\s+(\\w+)\', text_lower)\n                    if match:\n                        location = match.group(1)\n                        if location in self.location_map:\n                            parameters["destination"] = location\n\n        return best_match, parameters\n\n    def calculate_similarity(self, keyword: str, text: str) -> float:\n        """Calculate similarity between keyword and text"""\n        if keyword in text:\n            return 1.0\n        elif any(word in text for word in keyword.split()):\n            return 0.7\n        else:\n            return 0.0\n\n    def extract_parameters(self, text: str, required_params: List[str]) -> Dict[str, Any]:\n        """Extract parameters from text"""\n        parameters = {}\n\n        for param in required_params:\n            if param == "destination":\n                # Look for location keywords\n                for location, coords in self.location_map.items():\n                    if location in text:\n                        parameters["destination"] = location\n                        break\n\n            elif param == "object":\n                # Look for object keywords\n                for obj_name, obj_info in self.object_map.items():\n                    if obj_name in text:\n                        parameters["object"] = obj_name\n                        break\n\n            elif param == "message":\n                # Extract message content (everything after command)\n                command_end = -1\n                for keyword in ["say", "speak", "tell", "announce"]:\n                    if keyword in text:\n                        command_end = text.find(keyword) + len(keyword)\n                        break\n\n                if command_end > 0:\n                    message = text[command_end:].strip()\n                    if message.startswith("to "):\n                        message = message[3:].strip()\n                    parameters["message"] = message\n\n            elif param == "duration":\n                # Look for time expressions\n                time_match = re.search(r\'(\\d+(?:\\.\\d+)?)\\s*(seconds?|secs?|minutes?|mins?|hours?|hrs?)\', text)\n                if time_match:\n                    value = float(time_match.group(1))\n                    unit = time_match.group(2)\n                    if \'minute\' in unit or \'min\' in unit:\n                        value *= 60\n                    elif \'hour\' in unit or \'hr\' in unit:\n                        value *= 3600\n                    parameters["duration"] = value\n\n        return parameters\n\n    def create_primitive_actions(self, action_name: str, parameters: Dict[str, Any]) -> List[PrimitiveAction]:\n        """Create primitive actions for an action with given parameters"""\n        if action_name not in self.action_database:\n            return []\n\n        action_def = self.action_database[action_name]\n        primitive_actions = []\n\n        for base_action in action_def["primitive_actions"]:\n            # Create a copy and update parameters\n            action_copy = PrimitiveAction(\n                action_type=base_action.action_type,\n                parameters=base_action.parameters.copy(),\n                description=base_action.description,\n                required_sensors=base_action.required_sensors.copy(),\n                required_actuators=base_action.required_actuators.copy()\n            )\n\n            # Update parameters with specific values\n            for param_key, param_value in parameters.items():\n                if param_key in action_copy.parameters:\n                    action_copy.parameters[param_key] = param_value\n\n            # Special handling for location parameters\n            if "destination" in parameters and action_copy.action_type == "move_base":\n                if parameters["destination"] in self.location_map:\n                    location_data = self.location_map[parameters["destination"]]\n                    action_copy.parameters["target_pose"] = location_data\n\n            primitive_actions.append(action_copy)\n\n        return primitive_actions\n\nclass LLMActionTranslator:\n    """Translates natural language to actions using LLM"""\n    def __init__(self, llm_client):\n        self.llm_client = llm_client\n        self.action_mapper = ActionMapper()\n\n    async def translate_command(self, user_command: str) -> Optional[CognitivePlan]:\n        """Translate user command to cognitive plan using LLM"""\n        prompt = f"""\nYou are a robot command translator. Your job is to convert natural language commands into executable robot actions.\n\nGiven the command: "{user_command}"\n\nPlease provide the response in the following JSON format:\n\n{{\n    "goal": "the user\'s ultimate goal",\n    "steps": [\n        {{\n            "id": "step_unique_id",\n            "name": "descriptive name of the step",\n            "category": "NAVIGATION|MANIPULATION|PERCEPTION|COMMUNICATION|SYSTEM",\n            "dependencies": ["id_of_previous_step_if_any"],\n            "actions": [\n                {{\n                    "action_type": "specific_robot_action",\n                    "parameters": {{"param_name": "param_value"}},\n                    "description": "what this action does"\n                }}\n            ],\n            "estimated_duration": 5.0,\n            "success_criteria": ["list", "of", "success", "criteria"]\n        }}\n    ]\n}}\n\nExample for "Go to the kitchen and bring me a cup":\n{{\n    "goal": "Deliver a cup to the user",\n    "steps": [\n        {{\n            "id": "nav_to_kitchen",\n            "name": "Navigate to kitchen",\n            "category": "NAVIGATION",\n            "dependencies": [],\n            "actions": [\n                {{\n                    "action_type": "move_base",\n                    "parameters": {{"target_pose": {{"x": 2.0, "y": 1.0, "z": 0.0}}}},\n                    "description": "Move robot to kitchen location"\n                }}\n            ],\n            "estimated_duration": 30.0,\n            "success_criteria": ["robot_reached_kitchen", "navigation_successful"]\n        }},\n        {{\n            "id": "find_cup",\n            "name": "Find cup in kitchen",\n            "category": "PERCEPTION",\n            "dependencies": ["nav_to_kitchen"],\n            "actions": [\n                {{\n                    "action_type": "object_detection",\n                    "parameters": {{"target_object": "cup", "confidence_threshold": 0.7}},\n                    "description": "Detect cup in kitchen environment"\n                }}\n            ],\n            "estimated_duration": 10.0,\n            "success_criteria": ["cup_detected", "object_confirmed"]\n        }}\n    ]\n}}\n\nNow translate the given command:\n"""\n\n        try:\n            response = await self.llm_client.chat.completions.create(\n                model="gpt-3.5-turbo",\n                messages=[{"role": "user", "content": prompt}],\n                temperature=0.3,\n                max_tokens=1000\n            )\n\n            response_text = response.choices[0].message.content.strip()\n\n            # Extract JSON from response\n            json_start = response_text.find(\'{\')\n            json_end = response_text.rfind(\'}\') + 1\n            if json_start != -1 and json_end != 0:\n                json_str = response_text[json_start:json_end]\n                plan_data = json.loads(json_str)\n\n                # Create cognitive plan\n                steps = []\n                for step_data in plan_data.get(\'steps\', []):\n                    actions = []\n                    for action_data in step_data.get(\'actions\', []):\n                        action = PrimitiveAction(\n                            action_type=action_data[\'action_type\'],\n                            parameters=action_data.get(\'parameters\', {}),\n                            description=action_data[\'description\'],\n                            required_sensors=[],  # Would be determined by action type\n                            required_actuators=[]  # Would be determined by action type\n                        )\n                        actions.append(action)\n\n                    step = TaskStep(\n                        id=step_data[\'id\'],\n                        name=step_data[\'name\'],\n                        category=ActionCategory(step_data[\'category\']),\n                        dependencies=step_data.get(\'dependencies\', []),\n                        primitive_actions=actions,\n                        estimated_duration=step_data.get(\'estimated_duration\', 10.0),\n                        success_criteria=step_data.get(\'success_criteria\', [])\n                    )\n                    steps.append(step)\n\n                plan = CognitivePlan(\n                    id=f"plan_{int(time.time())}",\n                    goal=plan_data.get(\'goal\', user_command),\n                    steps=steps,\n                    created_at=time.time()\n                )\n\n                return plan\n\n        except Exception as e:\n            print(f"Error translating command: {e}")\n            return None\n\n    def validate_plan(self, plan: CognitivePlan) -> Tuple[bool, List[str]]:\n        """Validate that a plan is executable"""\n        errors = []\n\n        # Check that all required parameters are present\n        for step in plan.steps:\n            for action in step.primitive_actions:\n                if action.action_type == "move_base":\n                    if "target_pose" not in action.parameters:\n                        errors.append(f"Step {step.id}: move_base action missing target_pose parameter")\n\n                elif action.action_type == "object_detection":\n                    if "target_object" not in action.parameters:\n                        errors.append(f"Step {step.id}: object_detection action missing target_object parameter")\n\n        # Check dependencies are valid\n        step_ids = [step.id for step in plan.steps]\n        for step in plan.steps:\n            for dep_id in step.dependencies:\n                if dep_id not in step_ids:\n                    errors.append(f"Step {step.id}: dependency {dep_id} not found")\n\n        return len(errors) == 0, errors\n\n# Example usage\nasync def example_action_translation():\n    # This would require an actual LLM client\n    # For demonstration, we\'ll show the structure\n    print("LLM Action Translation Example")\n\n    # Initialize with mock LLM client\n    class MockLLMClient:\n        async def chat(self):\n            class Completions:\n                async def create(self, **kwargs):\n                    # Mock response\n                    class Choice:\n                        class Message:\n                            content = \'\'\'{\n    "goal": "Navigate to kitchen and bring cup",\n    "steps": [\n        {\n            "id": "nav_to_kitchen",\n            "name": "Navigate to kitchen",\n            "category": "NAVIGATION",\n            "dependencies": [],\n            "actions": [\n                {\n                    "action_type": "move_base",\n                    "parameters": {"target_pose": {"x": 2.0, "y": 1.0, "z": 0.0}},\n                    "description": "Move to kitchen location"\n                }\n            ],\n            "estimated_duration": 30.0,\n            "success_criteria": ["navigation_completed"]\n        }\n    ]\n}\'\'\'\n                    class Response:\n                        choices = [Choice()]\n                    return Response()\n\n            self.completions = Completions()\n\n    mock_client = MockLLMClient()\n    translator = LLMActionTranslator(mock_client)\n\n    test_commands = [\n        "Go to the kitchen and bring me a cup",\n        "Find my keys and bring them to me",\n        "Navigate to the living room and wait for me there"\n    ]\n\n    for command in test_commands:\n        print(f"\\nCommand: {command}")\n        plan = await translator.translate_command(command)\n        if plan:\n            print(f"Goal: {plan.goal}")\n            print(f"Steps: {len(plan.steps)}")\n            for step in plan.steps:\n                print(f"  - {step.name} ({step.category.value}): {len(step.primitive_actions)} actions")\n        else:\n            print("Could not translate command")\n\nif __name__ == "__main__":\n    import asyncio\n    asyncio.run(example_action_translation())\n'})}),"\n",(0,a.jsx)(n.h2,{id:"planning-and-execution-framework",children:"Planning and Execution Framework"}),"\n",(0,a.jsx)(n.h3,{id:"hierarchical-task-decomposition",children:"Hierarchical Task Decomposition"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'# hierarchical_planning.py\nimport asyncio\nimport time\nfrom typing import Dict, List, Any, Optional, Callable, Union\nfrom dataclasses import dataclass\nfrom enum import Enum\nimport threading\nimport queue\n\nclass TaskStatus(Enum):\n    PENDING = "pending"\n    RUNNING = "running"\n    COMPLETED = "completed"\n    FAILED = "failed"\n    CANCELLED = "cancelled"\n\nclass PlanStatus(Enum):\n    PLANNING = "planning"\n    EXECUTING = "executing"\n    COMPLETED = "completed"\n    FAILED = "failed"\n    ABORTED = "aborted"\n\n@dataclass\nclass TaskNode:\n    """Node in the hierarchical task tree"""\n    id: str\n    name: str\n    description: str\n    task_type: str  # "composite" or "primitive"\n    children: List[\'TaskNode\'] = None\n    parent: Optional[\'TaskNode\'] = None\n    parameters: Dict[str, Any] = None\n    status: TaskStatus = TaskStatus.PENDING\n    priority: int = 0\n    dependencies: List[str] = None\n    estimated_duration: float = 0.0\n    actual_duration: Optional[float] = None\n\n    def __post_init__(self):\n        if self.children is None:\n            self.children = []\n        if self.parameters is None:\n            self.parameters = {}\n        if self.dependencies is None:\n            self.dependencies = []\n\n    def add_child(self, child: \'TaskNode\'):\n        """Add a child task node"""\n        child.parent = self\n        self.children.append(child)\n\n    def remove_child(self, child_id: str) -> bool:\n        """Remove a child task node by ID"""\n        for i, child in enumerate(self.children):\n            if child.id == child_id:\n                self.children.pop(i)\n                child.parent = None\n                return True\n        return False\n\n    def get_ancestors(self) -> List[\'TaskNode\']:\n        """Get all ancestor nodes"""\n        ancestors = []\n        current = self.parent\n        while current:\n            ancestors.append(current)\n            current = current.parent\n        return ancestors\n\n    def get_descendants(self) -> List[\'TaskNode\']:\n        """Get all descendant nodes"""\n        descendants = []\n        for child in self.children:\n            descendants.append(child)\n            descendants.extend(child.get_descendants())\n        return descendants\n\nclass HierarchicalPlanner:\n    """Hierarchical task planner using LLM for decomposition"""\n    def __init__(self, llm_client):\n        self.llm_client = llm_client\n        self.root_task = None\n        self.task_graph = {}  # id -> TaskNode mapping\n        self.execution_context = {}\n\n    async def create_plan(self, goal: str, context: Dict[str, Any] = None) -> Optional[TaskNode]:\n        """Create hierarchical plan using LLM"""\n        prompt = f"""\nCreate a hierarchical plan to achieve: "{goal}"\n\nContext information: {json.dumps(context, indent=2) if context else \'None\'}\n\nThe plan should be structured as a tree with composite tasks and primitive actions.\n\nReturn the plan in JSON format:\n{{\n    "root_task": {{\n        "id": "unique_id",\n        "name": "task_name",\n        "description": "what the task accomplishes",\n        "task_type": "composite",  # or "primitive"\n        "children": [\n            {{\n                "id": "child_id",\n                "name": "child_task_name",\n                "description": "child task description",\n                "task_type": "primitive",  # leaf node\n                "parameters": {{"param": "value"}},\n                "estimated_duration": 5.0,\n                "dependencies": ["dependency_task_id"]\n            }}\n        ],\n        "parameters": {{"param": "value"}},\n        "estimated_duration": 10.0,\n        "dependencies": []\n    }}\n}}\n\nExample for "Bring coffee from kitchen":\n{{\n    "root_task": {{\n        "id": "bring_coffee",\n        "name": "Bring Coffee",\n        "description": "Bring coffee from kitchen to user",\n        "task_type": "composite",\n        "children": [\n            {{\n                "id": "navigate_to_kitchen",\n                "name": "Navigate to Kitchen",\n                "description": "Move robot to kitchen location",\n                "task_type": "composite",\n                "children": [\n                    {{\n                        "id": "plan_path_to_kitchen",\n                        "name": "Plan Path to Kitchen",\n                        "description": "Calculate navigation path",\n                        "task_type": "primitive",\n                        "parameters": {{"destination": "kitchen"}},\n                        "estimated_duration": 2.0\n                    }},\n                    {{\n                        "id": "execute_navigation",\n                        "name": "Execute Navigation",\n                        "description": "Move robot along calculated path",\n                        "task_type": "primitive",\n                        "parameters": {{"path": "calculated_path"}},\n                        "estimated_duration": 25.0\n                    }}\n                ],\n                "parameters": {{"destination": "kitchen"}},\n                "estimated_duration": 30.0\n            }},\n            {{\n                "id": "fetch_coffee",\n                "name": "Fetch Coffee",\n                "description": "Locate and grasp coffee",\n                "task_type": "composite",\n                "children": [\n                    {{\n                        "id": "detect_coffee",\n                        "name": "Detect Coffee",\n                        "description": "Find coffee in kitchen",\n                        "task_type": "primitive",\n                        "parameters": {{"target_object": "coffee"}},\n                        "estimated_duration": 10.0\n                    }},\n                    {{\n                        "id": "grasp_coffee",\n                        "name": "Grasp Coffee",\n                        "description": "Pick up coffee cup",\n                        "task_type": "primitive",\n                        "parameters": {{"object": "coffee"}},\n                        "estimated_duration": 5.0\n                    }}\n                ],\n                "parameters": {{"object": "coffee"}},\n                "estimated_duration": 20.0\n            }}\n        ],\n        "parameters": {{"object": "coffee", "destination": "kitchen"}},\n        "estimated_duration": 60.0\n    }}\n}}\n\nNow create the plan for: {goal}\n"""\n\n        try:\n            response = await self.llm_client.chat.completions.create(\n                model="gpt-3.5-turbo",\n                messages=[{"role": "user", "content": prompt}],\n                temperature=0.3,\n                max_tokens=2000\n            )\n\n            response_text = response.choices[0].message.content.strip()\n\n            # Extract JSON\n            json_start = response_text.find(\'{\')\n            json_end = response_text.rfind(\'}\') + 1\n            if json_start != -1 and json_end != 0:\n                json_str = response_text[json_start:json_end]\n                plan_data = json.loads(json_str)\n\n                # Build task tree\n                root_task = self._build_task_tree(plan_data[\'root_task\'])\n                self.root_task = root_task\n                self._index_tasks(root_task)\n\n                return root_task\n\n        except Exception as e:\n            print(f"Error creating plan: {e}")\n            return None\n\n    def _build_task_tree(self, task_data: Dict[str, Any], parent: TaskNode = None) -> TaskNode:\n        """Recursively build task tree from data"""\n        task = TaskNode(\n            id=task_data[\'id\'],\n            name=task_data[\'name\'],\n            description=task_data[\'description\'],\n            task_type=task_data[\'task_type\'],\n            parameters=task_data.get(\'parameters\', {}),\n            estimated_duration=task_data.get(\'estimated_duration\', 0.0),\n            dependencies=task_data.get(\'dependencies\', [])\n        )\n\n        task.parent = parent\n\n        # Build children\n        for child_data in task_data.get(\'children\', []):\n            child_task = self._build_task_tree(child_data, task)\n            task.children.append(child_task)\n\n        return task\n\n    def _index_tasks(self, root_task: TaskNode):\n        """Index all tasks by ID for quick lookup"""\n        def index_recursive(node):\n            self.task_graph[node.id] = node\n            for child in node.children:\n                index_recursive(child)\n\n        index_recursive(root_task)\n\n    def get_task_by_id(self, task_id: str) -> Optional[TaskNode]:\n        """Get task by ID"""\n        return self.task_graph.get(task_id)\n\n    def get_ready_tasks(self) -> List[TaskNode]:\n        """Get tasks that are ready to execute (dependencies satisfied)"""\n        ready_tasks = []\n\n        for task_id, task in self.task_graph.items():\n            if task.status == TaskStatus.PENDING:\n                # Check if all dependencies are completed\n                all_deps_met = True\n                for dep_id in task.dependencies:\n                    dep_task = self.task_graph.get(dep_id)\n                    if dep_task and dep_task.status != TaskStatus.COMPLETED:\n                        all_deps_met = False\n                        break\n\n                if all_deps_met:\n                    ready_tasks.append(task)\n\n        return ready_tasks\n\n    def validate_plan(self, root_task: TaskNode) -> Tuple[bool, List[str]]:\n        """Validate the plan for consistency"""\n        errors = []\n\n        def validate_recursive(node):\n            # Check for circular dependencies\n            ancestors = node.get_ancestors()\n            ancestor_ids = [a.id for a in ancestors]\n\n            for dep_id in node.dependencies:\n                if dep_id in ancestor_ids:\n                    errors.append(f"Circular dependency: {node.id} depends on its ancestor {dep_id}")\n\n            # Validate children\n            for child in node.children:\n                validate_recursive(child)\n\n        validate_recursive(root_task)\n\n        # Check for duplicate IDs\n        all_ids = [task.id for task in self.task_graph.values()]\n        if len(all_ids) != len(set(all_ids)):\n            errors.append("Duplicate task IDs found")\n\n        return len(errors) == 0, errors\n\nclass PlanExecutor:\n    """Executes hierarchical plans with monitoring and recovery"""\n    def __init__(self, robot_interface):\n        self.robot_interface = robot_interface\n        self.planner = None\n        self.execution_queue = queue.Queue()\n        self.execution_thread = None\n        self.execution_active = False\n        self.execution_results = {}\n        self.failure_recovery_strategies = self._initialize_recovery_strategies()\n\n    def _initialize_recovery_strategies(self):\n        """Initialize failure recovery strategies"""\n        return {\n            \'navigation_failure\': self._handle_navigation_failure,\n            \'grasp_failure\': self._handle_grasp_failure,\n            \'perception_failure\': self._handle_perception_failure,\n            \'timeout_failure\': self._handle_timeout_failure\n        }\n\n    async def execute_plan(self, plan: TaskNode, timeout: float = 300.0) -> PlanStatus:\n        """Execute the hierarchical plan"""\n        if not plan:\n            return PlanStatus.FAILED\n\n        self.planner = HierarchicalPlanner(None)  # We\'ll reuse the indexing\n        self.planner.root_task = plan\n        self.planner._index_tasks(plan)\n\n        plan.status = PlanStatus.EXECUTING\n        plan.started_at = time.time()\n\n        # Start execution thread\n        self.execution_active = True\n        self.execution_thread = threading.Thread(target=self._execution_worker, daemon=True)\n        self.execution_thread.start()\n\n        # Monitor execution\n        start_time = time.time()\n        while plan.status == PlanStatus.EXECUTING:\n            if time.time() - start_time > timeout:\n                await self.abort_execution(plan)\n                return PlanStatus.ABORTED\n\n            # Check for plan completion\n            if self._is_plan_complete(plan):\n                plan.status = PlanStatus.COMPLETED\n                plan.completed_at = time.time()\n                break\n\n            # Check for plan failure\n            if self._is_plan_failed(plan):\n                plan.status = PlanStatus.FAILED\n                break\n\n            await asyncio.sleep(0.1)  # Check every 100ms\n\n        self.execution_active = False\n        return plan.status\n\n    def _execution_worker(self):\n        """Worker thread for executing tasks"""\n        while self.execution_active:\n            # Get ready tasks\n            ready_tasks = self.planner.get_ready_tasks()\n\n            for task in ready_tasks:\n                if not self.execution_active:\n                    break\n\n                # Execute the task\n                success = self._execute_task(task)\n\n                if success:\n                    task.status = TaskStatus.COMPLETED\n                    task.actual_duration = time.time() - (getattr(task, \'start_time\', time.time()))\n                else:\n                    task.status = TaskStatus.FAILED\n                    # Try recovery\n                    recovered = self._attempt_recovery(task)\n                    if not recovered:\n                        # Propagate failure up the tree\n                        self._propagate_failure(task)\n\n            # Sleep briefly to prevent busy waiting\n            time.sleep(0.05)\n\n    def _execute_task(self, task: TaskNode) -> bool:\n        """Execute a single task"""\n        task.status = TaskStatus.RUNNING\n        task.start_time = time.time()\n\n        if task.task_type == \'primitive\':\n            # Execute primitive action\n            return self._execute_primitive_action(task)\n        elif task.task_type == \'composite\':\n            # For composite tasks, ensure all children are completed\n            return self._execute_composite_task(task)\n        else:\n            print(f"Unknown task type: {task.task_type}")\n            return False\n\n    def _execute_primitive_action(self, task: TaskNode) -> bool:\n        """Execute a primitive action"""\n        try:\n            action_type = task.name.lower().replace(\' \', \'_\')\n\n            # Map action names to robot interface methods\n            action_methods = {\n                \'navigate_to_kitchen\': lambda: self.robot_interface.navigate_to_location(\'kitchen\'),\n                \'navigate_to_bedroom\': lambda: self.robot_interface.navigate_to_location(\'bedroom\'),\n                \'grasp_object\': lambda: self.robot_interface.grasp_object(task.parameters.get(\'object\')),\n                \'detect_object\': lambda: self.robot_interface.detect_object(task.parameters.get(\'target_object\')),\n                \'move_base\': lambda: self.robot_interface.move_to_pose(task.parameters.get(\'target_pose\')),\n                \'object_detection\': lambda: self.robot_interface.detect_object(task.parameters.get(\'target_object\')),\n                \'speak\': lambda: self.robot_interface.speak(task.parameters.get(\'text\', \'\')),\n            }\n\n            if action_type in action_methods:\n                success = action_methods[action_type]()\n                return success\n            else:\n                print(f"Unknown primitive action: {action_type}")\n                return False\n\n        except Exception as e:\n            print(f"Error executing task {task.id}: {e}")\n            return False\n\n    def _execute_composite_task(self, task: TaskNode) -> bool:\n        """Execute a composite task (ensure children are completed)"""\n        # Composite tasks are considered complete when all children are complete\n        for child in task.children:\n            if child.status != TaskStatus.COMPLETED:\n                return False\n        return True\n\n    def _attempt_recovery(self, failed_task: TaskNode) -> bool:\n        """Attempt to recover from task failure"""\n        # Determine failure type\n        failure_type = self._classify_failure(failed_task)\n\n        if failure_type in self.failure_recovery_strategies:\n            return self.failure_recovery_strategies[failure_type](failed_task)\n        else:\n            print(f"No recovery strategy for failure type: {failure_type}")\n            return False\n\n    def _classify_failure(self, task: TaskNode) -> str:\n        """Classify the type of failure"""\n        # This would be more sophisticated in practice\n        # For now, use simple heuristics\n        if \'navigate\' in task.name.lower():\n            return \'navigation_failure\'\n        elif \'grasp\' in task.name.lower() or \'pick\' in task.name.lower():\n            return \'grasp_failure\'\n        elif \'detect\' in task.name.lower() or \'find\' in task.name.lower():\n            return \'perception_failure\'\n        else:\n            return \'general_failure\'\n\n    def _handle_navigation_failure(self, task: TaskNode) -> bool:\n        """Handle navigation failure"""\n        print(f"Handling navigation failure for task: {task.id}")\n        # Try alternative path\n        # Retry with different parameters\n        # Report failure to user\n        return False  # For now, don\'t recover\n\n    def _handle_grasp_failure(self, task: TaskNode) -> bool:\n        """Handle grasp failure"""\n        print(f"Handling grasp failure for task: {task.id}")\n        # Try different grasp approach\n        # Check if object is accessible\n        # Report failure\n        return False\n\n    def _handle_perception_failure(self, task: TaskNode) -> bool:\n        """Handle perception failure"""\n        print(f"Handling perception failure for task: {task.id}")\n        # Try different viewing angle\n        # Adjust lighting conditions\n        # Report failure\n        return False\n\n    def _handle_timeout_failure(self, task: TaskNode) -> bool:\n        """Handle timeout failure"""\n        print(f"Handling timeout failure for task: {task.id}")\n        # Report timeout to user\n        return False\n\n    def _propagate_failure(self, failed_task: TaskNode):\n        """Propagate failure up the task hierarchy"""\n        # Mark all dependent tasks as failed\n        for task_id, task in self.planner.task_graph.items():\n            if failed_task.id in task.dependencies:\n                task.status = TaskStatus.FAILED\n\n    def _is_plan_complete(self, plan: TaskNode) -> bool:\n        """Check if the plan is complete"""\n        def check_complete(node):\n            if node.status != TaskStatus.COMPLETED:\n                return False\n            for child in node.children:\n                if not check_complete(child):\n                    return False\n            return True\n\n        return check_complete(plan)\n\n    def _is_plan_failed(self, plan: TaskNode) -> bool:\n        """Check if the plan has failed"""\n        def check_failed(node):\n            if node.status == TaskStatus.FAILED:\n                return True\n            for child in node.children:\n                if check_failed(child):\n                    return True\n            return False\n\n        return check_failed(plan)\n\n    async def abort_execution(self, plan: TaskNode):\n        """Abort plan execution"""\n        self.execution_active = False\n        if self.execution_thread:\n            self.execution_thread.join(timeout=1.0)\n\n        # Mark all running tasks as cancelled\n        for task_id, task in self.planner.task_graph.items():\n            if task.status == TaskStatus.RUNNING:\n                task.status = TaskStatus.CANCELLED\n\n        plan.status = PlanStatus.ABORTED\n\n# Example usage\nasync def example_hierarchical_planning():\n    print("Hierarchical Planning Example")\n\n    # This would require a real LLM client and robot interface\n    # For demonstration, we\'ll show the structure\n\n    class MockLLMClient:\n        async def chat(self):\n            class Completions:\n                async def create(self, **kwargs):\n                    # Mock response\n                    class Choice:\n                        class Message:\n                            content = \'\'\'{\n    "root_task": {\n        "id": "bring_coffee",\n        "name": "Bring Coffee",\n        "description": "Bring coffee from kitchen to user",\n        "task_type": "composite",\n        "children": [\n            {\n                "id": "navigate_to_kitchen",\n                "name": "Navigate to Kitchen",\n                "description": "Move robot to kitchen location",\n                "task_type": "composite",\n                "children": [\n                    {\n                        "id": "plan_path_to_kitchen",\n                        "name": "Plan Path to Kitchen",\n                        "description": "Calculate navigation path",\n                        "task_type": "primitive",\n                        "parameters": {"destination": "kitchen"},\n                        "estimated_duration": 2.0\n                    },\n                    {\n                        "id": "execute_navigation",\n                        "name": "Execute Navigation",\n                        "description": "Move robot along calculated path",\n                        "task_type": "primitive",\n                        "parameters": {"path": "calculated_path"},\n                        "estimated_duration": 25.0\n                    }\n                ],\n                "parameters": {"destination": "kitchen"},\n                "estimated_duration": 30.0\n            }\n        ],\n        "parameters": {"object": "coffee", "destination": "kitchen"},\n        "estimated_duration": 60.0\n    }\n}\'\'\'\n                    class Response:\n                        choices = [Choice()]\n                    return Response()\n\n            self.completions = Completions()\n\n    class MockRobotInterface:\n        def navigate_to_location(self, location):\n            print(f"Navigating to {location}")\n            time.sleep(1)  # Simulate navigation\n            return True\n\n        def grasp_object(self, obj):\n            print(f"Grasping {obj}")\n            time.sleep(0.5)\n            return True\n\n        def detect_object(self, obj):\n            print(f"Detecting {obj}")\n            time.sleep(0.3)\n            return True\n\n        def move_to_pose(self, pose):\n            print(f"Moving to pose: {pose}")\n            time.sleep(1)\n            return True\n\n        def speak(self, text):\n            print(f"Speaking: {text}")\n            return True\n\n    # Initialize components\n    llm_client = MockLLMClient()\n    robot_interface = MockRobotInterface()\n    planner = HierarchicalPlanner(llm_client)\n    executor = PlanExecutor(robot_interface)\n\n    # Create a plan\n    goal = "Go to the kitchen and bring me a cup of coffee"\n    plan = await planner.create_plan(goal)\n\n    if plan:\n        print(f"Created plan: {plan.name}")\n        print(f"Root task: {plan.description}")\n        print(f"Children: {len(plan.children)}")\n\n        # Execute the plan\n        status = await executor.execute_plan(plan, timeout=120.0)\n        print(f"Plan execution status: {status.value}")\n\nif __name__ == "__main__":\n    import asyncio\n    import json\n    asyncio.run(example_hierarchical_planning())\n'})}),"\n",(0,a.jsx)(n.h2,{id:"task-decomposition-and-execution",children:"Task Decomposition and Execution"}),"\n",(0,a.jsx)(n.h3,{id:"advanced-task-decomposition-with-llms",children:"Advanced Task Decomposition with LLMs"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'# advanced_task_decomposition.py\nimport asyncio\nimport json\nfrom typing import Dict, List, Any, Optional, Tuple\nfrom dataclasses import dataclass\nfrom enum import Enum\nimport time\nimport re\n\nclass TaskDecompositionStrategy(Enum):\n    SEQUENTIAL = "sequential"\n    PARALLELIZABLE = "parallelizable"\n    HIERARCHICAL = "hierarchical"\n    ADAPTIVE = "adaptive"\n\nclass ExecutionPriority(Enum):\n    CRITICAL = 1\n    HIGH = 2\n    NORMAL = 3\n    LOW = 4\n\n@dataclass\nclass DecomposedTask:\n    """A task decomposed by LLM"""\n    id: str\n    name: str\n    description: str\n    subtasks: List[\'DecomposedTask\']\n    action_sequence: List[Dict[str, Any]]  # Low-level actions\n    prerequisites: List[str]  # Task IDs that must complete first\n    estimated_duration: float\n    priority: ExecutionPriority\n    success_criteria: List[str]\n    failure_modes: List[str]\n    recovery_strategies: List[str]\n\n    def __post_init__(self):\n        if self.subtasks is None:\n            self.subtasks = []\n        if self.action_sequence is None:\n            self.action_sequence = []\n        if self.prerequisites is None:\n            self.prerequisites = []\n        if self.success_criteria is None:\n            self.success_criteria = []\n        if self.failure_modes is None:\n            self.failure_modes = []\n        if self.recovery_strategies is None:\n            self.recovery_strategies = []\n\nclass TaskDecomposer:\n    """Advanced task decomposer using LLMs"""\n    def __init__(self, llm_client, strategy: TaskDecompositionStrategy = TaskDecompositionStrategy.HIERARCHICAL):\n        self.llm_client = llm_client\n        self.strategy = strategy\n        self.knowledge_base = self._initialize_knowledge_base()\n\n    def _initialize_knowledge_base(self) -> Dict[str, Any]:\n        """Initialize knowledge base for task decomposition"""\n        return {\n            "navigation_tasks": {\n                "common_sequences": [\n                    ["localize", "plan_path", "execute_path", "reach_goal"]\n                ],\n                "prerequisites": {\n                    "localize": [],\n                    "plan_path": ["localize"],\n                    "execute_path": ["plan_path"],\n                    "reach_goal": ["execute_path"]\n                },\n                "failure_modes": ["obstacle_detected", "lost_localization", "goal_unreachable"]\n            },\n            "manipulation_tasks": {\n                "common_sequences": [\n                    ["detect_object", "plan_grasp", "execute_grasp", "verify_grasp"]\n                ],\n                "prerequisites": {\n                    "detect_object": [],\n                    "plan_grasp": ["detect_object"],\n                    "execute_grasp": ["plan_grasp"],\n                    "verify_grasp": ["execute_grasp"]\n                },\n                "failure_modes": ["object_not_found", "grasp_failed", "object_dropped"]\n            },\n            "perception_tasks": {\n                "common_sequences": [\n                    ["acquire_sensor_data", "process_data", "interpret_results", "report_findings"]\n                ],\n                "prerequisites": {\n                    "acquire_sensor_data": [],\n                    "process_data": ["acquire_sensor_data"],\n                    "interpret_results": ["process_data"],\n                    "report_findings": ["interpret_results"]\n                },\n                "failure_modes": ["sensor_failure", "data_corruption", "misidentification"]\n            }\n        }\n\n    async def decompose_task(self, goal: str, context: Dict[str, Any] = None) -> Optional[DecomposedTask]:\n        """Decompose a high-level goal into executable tasks"""\n        prompt = self._create_decomposition_prompt(goal, context)\n\n        try:\n            response = await self.llm_client.chat.completions.create(\n                model="gpt-3.5-turbo",\n                messages=[{"role": "user", "content": prompt}],\n                temperature=0.3,\n                max_tokens=1500\n            )\n\n            response_text = response.choices[0].message.content.strip()\n\n            # Extract JSON\n            json_start = response_text.find(\'{\')\n            json_end = response_text.rfind(\'}\') + 1\n            if json_start != -1 and json_end != 0:\n                json_str = response_text[json_start:json_end]\n                task_data = json.loads(json_str)\n\n                # Build task structure recursively\n                return self._build_task_from_data(task_data)\n\n        except Exception as e:\n            print(f"Error decomposing task: {e}")\n            return None\n\n    def _create_decomposition_prompt(self, goal: str, context: Dict[str, Any]) -> str:\n        """Create prompt for task decomposition"""\n        return f"""\nDecompose the following goal into executable subtasks: "{goal}"\n\nContext: {json.dumps(context, indent=2) if context else \'No additional context\'}\n\nDecompose the task using the strategy: {self.strategy.value}\n\nProvide the response in JSON format:\n\n{{\n    "id": "unique_task_id",\n    "name": "task_name",\n    "description": "what this task accomplishes",\n    "subtasks": [\n        {{\n            "id": "subtask_id",\n            "name": "subtask_name",\n            "description": "subtask description",\n            "subtasks": [...],  # Further decomposition if needed\n            "action_sequence": [\n                {{\n                    "action": "specific_action_name",\n                    "parameters": {{"param": "value"}},\n                    "description": "what this action does"\n                }}\n            ],\n            "prerequisites": ["prerequisite_task_id"],\n            "estimated_duration": 5.0,\n            "priority": "NORMAL",  # CRITICAL, HIGH, NORMAL, LOW\n            "success_criteria": ["list", "of", "success", "conditions"],\n            "failure_modes": ["potential", "failure", "scenarios"],\n            "recovery_strategies": ["ways", "to", "recover", "from", "failures"]\n        }}\n    ],\n    "action_sequence": [...],  # Combined actions from all subtasks\n    "prerequisites": [...],  # Overall prerequisites\n    "estimated_duration": 10.0,  # Total estimated duration\n    "priority": "NORMAL",\n    "success_criteria": [...],\n    "failure_modes": [...],\n    "recovery_strategies": [...]\n}}\n\nExample for "Navigate to kitchen and find the red cup":\n{{\n    "id": "find_red_cup",\n    "name": "Find Red Cup",\n    "description": "Navigate to kitchen and locate red cup",\n    "subtasks": [\n        {{\n            "id": "navigate_to_kitchen",\n            "name": "Navigate to Kitchen",\n            "description": "Move robot to kitchen location",\n            "subtasks": [],\n            "action_sequence": [\n                {{\n                    "action": "localize_robot",\n                    "parameters": {{}},\n                    "description": "Determine current robot location"\n                }},\n                {{\n                    "action": "plan_path_to_kitchen",\n                    "parameters": {{"destination": "kitchen"}},\n                    "description": "Calculate path to kitchen"\n                }},\n                {{\n                    "action": "execute_navigation",\n                    "parameters": {{"path": "calculated_path", "speed": "normal"}},\n                    "description": "Navigate to kitchen"\n                }}\n            ],\n            "prerequisites": [],\n            "estimated_duration": 30.0,\n            "priority": "HIGH",\n            "success_criteria": ["robot_reached_kitchen", "navigation_successful"],\n            "failure_modes": ["path_blocked", "localization_lost"],\n            "recovery_strategies": ["replan_path", "request_assistance"]\n        }},\n        {{\n            "id": "detect_red_cup",\n            "name": "Detect Red Cup",\n            "description": "Find red cup in kitchen environment",\n            "subtasks": [],\n            "action_sequence": [\n                {{\n                    "action": "activate_camera",\n                    "parameters": {{"resolution": "high"}},\n                    "description": "Turn on camera for object detection"\n                }},\n                {{\n                    "action": "detect_objects",\n                    "parameters": {{"target_color": "red", "target_shape": "cup"}},\n                    "description": "Detect red cup-shaped objects"\n                }},\n                {{\n                    "action": "verify_detection",\n                    "parameters": {{"confidence_threshold": 0.8}},\n                    "description": "Verify object detection"\n                }}\n            ],\n            "prerequisites": ["navigate_to_kitchen"],\n            "estimated_duration": 15.0,\n            "priority": "NORMAL",\n            "success_criteria": ["red_cup_detected", "confidence_high"],\n            "failure_modes": ["no_red_cup_found", "detection_uncertain"],\n            "recovery_strategies": ["expand_search_area", "adjust_detection_params"]\n        }}\n    ],\n    "action_sequence": [\n        // Combined from all subtasks\n    ],\n    "prerequisites": [],\n    "estimated_duration": 45.0,\n    "priority": "NORMAL",\n    "success_criteria": ["red_cup_located", "position_known"],\n    "failure_modes": ["kitchen_access_denied", "object_not_found"],\n    "recovery_strategies": ["try_different_path", "search_other_areas"]\n}}\n\nNow decompose the goal: {goal}\n"""\n\n    def _build_task_from_data(self, task_data: Dict[str, Any]) -> DecomposedTask:\n        """Build task structure from JSON data"""\n        subtasks = []\n        for subtask_data in task_data.get(\'subtasks\', []):\n            subtask = self._build_task_from_data(subtask_data)\n            subtasks.append(subtask)\n\n        # Convert priority string to enum\n        priority_str = task_data.get(\'priority\', \'NORMAL\')\n        priority_enum = ExecutionPriority[priority_str.upper()]\n\n        task = DecomposedTask(\n            id=task_data[\'id\'],\n            name=task_data[\'name\'],\n            description=task_data[\'description\'],\n            subtasks=subtasks,\n            action_sequence=task_data.get(\'action_sequence\', []),\n            prerequisites=task_data.get(\'prerequisites\', []),\n            estimated_duration=task_data.get(\'estimated_duration\', 10.0),\n            priority=priority_enum,\n            success_criteria=task_data.get(\'success_criteria\', []),\n            failure_modes=task_data.get(\'failure_modes\', []),\n            recovery_strategies=task_data.get(\'recovery_strategies\', [])\n        )\n\n        return task\n\n    def analyze_task_dependencies(self, task: DecomposedTask) -> Dict[str, List[str]]:\n        """Analyze dependencies between subtasks"""\n        dependencies = {}\n\n        def analyze_recursive(current_task: DecomposedTask):\n            deps = current_task.prerequisites\n            dependencies[current_task.id] = deps\n\n            for subtask in current_task.subtasks:\n                analyze_recursive(subtask)\n\n        analyze_recursive(task)\n        return dependencies\n\n    def optimize_execution_order(self, task: DecomposedTask) -> List[str]:\n        """Optimize execution order based on dependencies and priorities"""\n        # Topological sort with priority consideration\n        all_task_ids = []\n\n        def collect_ids_recursive(current_task: DecomposedTask):\n            all_task_ids.append(current_task.id)\n            for subtask in current_task.subtasks:\n                collect_ids_recursive(subtask)\n\n        collect_ids_recursive(task)\n\n        # Build dependency graph\n        dependencies = self.analyze_task_dependencies(task)\n\n        # Topological sort\n        from collections import defaultdict, deque\n\n        graph = defaultdict(list)\n        in_degree = {task_id: 0 for task_id in all_task_ids}\n\n        for task_id, prereqs in dependencies.items():\n            for prereq in prereqs:\n                graph[prereq].append(task_id)\n                in_degree[task_id] += 1\n\n        # Kahn\'s algorithm for topological sort\n        queue = deque([task_id for task_id in all_task_ids if in_degree[task_id] == 0])\n        sorted_order = []\n\n        while queue:\n            current = queue.popleft()\n            sorted_order.append(current)\n\n            for neighbor in graph[current]:\n                in_degree[neighbor] -= 1\n                if in_degree[neighbor] == 0:\n                    queue.append(neighbor)\n\n        # If there are cycles, return original order\n        if len(sorted_order) != len(all_task_ids):\n            return all_task_ids  # Cycle detected, return original\n\n        return sorted_order\n\n    def generate_execution_plan(self, task: DecomposedTask) -> Dict[str, Any]:\n        """Generate detailed execution plan"""\n        execution_plan = {\n            "task_id": task.id,\n            "task_name": task.name,\n            "execution_order": self.optimize_execution_order(task),\n            "timeline": {},\n            "resource_requirements": {},\n            "monitoring_points": [],\n            "recovery_procedures": {}\n        }\n\n        # Calculate timeline\n        current_time = 0.0\n        for task_id in execution_plan["execution_order"]:\n            task_obj = self._find_task_by_id(task, task_id)\n            if task_obj:\n                execution_plan["timeline"][task_id] = {\n                    "start_time": current_time,\n                    "end_time": current_time + task_obj.estimated_duration,\n                    "duration": task_obj.estimated_duration\n                }\n                current_time += task_obj.estimated_duration\n\n        # Collect resource requirements\n        all_tasks = self._collect_all_tasks(task)\n        for task_obj in all_tasks:\n            for action in task_obj.action_sequence:\n                action_type = action.get(\'action\', \'unknown\')\n                # In practice, this would map to actual robot resources\n\n        # Set monitoring points at critical junctures\n        execution_plan["monitoring_points"] = self._identify_monitoring_points(task)\n\n        # Map recovery procedures\n        for task_obj in all_tasks:\n            if task_obj.failure_modes:\n                execution_plan["recovery_procedures"][task_obj.id] = {\n                    "failure_modes": task_obj.failure_modes,\n                    "strategies": task_obj.recovery_strategies\n                }\n\n        return execution_plan\n\n    def _find_task_by_id(self, root_task: DecomposedTask, task_id: str) -> Optional[DecomposedTask]:\n        """Find task by ID in the task hierarchy"""\n        if root_task.id == task_id:\n            return root_task\n\n        for subtask in root_task.subtasks:\n            found = self._find_task_by_id(subtask, task_id)\n            if found:\n                return found\n\n        return None\n\n    def _collect_all_tasks(self, root_task: DecomposedTask) -> List[DecomposedTask]:\n        """Collect all tasks in the hierarchy"""\n        tasks = [root_task]\n\n        for subtask in root_task.subtasks:\n            tasks.extend(self._collect_all_tasks(subtask))\n\n        return tasks\n\n    def _identify_monitoring_points(self, task: DecomposedTask) -> List[str]:\n        """Identify critical monitoring points"""\n        monitoring_points = []\n\n        # Add monitoring for critical tasks\n        all_tasks = self._collect_all_tasks(task)\n        for task_obj in all_tasks:\n            if task_obj.priority == ExecutionPriority.CRITICAL:\n                monitoring_points.append(task_obj.id)\n            elif task_obj.failure_modes:  # Tasks with failure modes\n                monitoring_points.append(task_obj.id)\n\n        return monitoring_points\n\nclass TaskExecutionMonitor:\n    """Monitors task execution and handles failures"""\n    def __init__(self):\n        self.execution_log = []\n        self.current_task = None\n        self.task_progress = {}\n        self.failure_count = 0\n        self.success_count = 0\n\n    def start_task_execution(self, task: DecomposedTask):\n        """Start monitoring task execution"""\n        self.current_task = task\n        self.task_progress[task.id] = {\n            "start_time": time.time(),\n            "status": "running",\n            "subtask_progress": {}\n        }\n\n    def update_task_progress(self, task_id: str, progress: float, status: str = "running"):\n        """Update progress for a specific task"""\n        if task_id in self.task_progress:\n            self.task_progress[task_id]["progress"] = progress\n            self.task_progress[task_id]["status"] = status\n            self.task_progress[task_id]["last_update"] = time.time()\n\n    def log_execution_event(self, event_type: str, task_id: str, details: Dict[str, Any]):\n        """Log execution events"""\n        event = {\n            "timestamp": time.time(),\n            "event_type": event_type,\n            "task_id": task_id,\n            "details": details\n        }\n        self.execution_log.append(event)\n\n    def check_for_failures(self, task: DecomposedTask) -> List[Dict[str, Any]]:\n        """Check for task failures"""\n        failures = []\n\n        # Check timeout\n        if task.id in self.task_progress:\n            start_time = self.task_progress[task.id].get("start_time", time.time())\n            current_time = time.time()\n            estimated_duration = task.estimated_duration * 1.5  # 150% of estimated time\n\n            if current_time - start_time > estimated_duration:\n                failures.append({\n                    "task_id": task.id,\n                    "failure_type": "timeout",\n                    "severity": "high",\n                    "suggested_action": "abort_or_retry"\n                })\n\n        return failures\n\n    def generate_execution_report(self) -> Dict[str, Any]:\n        """Generate execution report"""\n        total_tasks = len(self.task_progress)\n        completed_tasks = sum(1 for progress in self.task_progress.values()\n                            if progress.get("status") == "completed")\n\n        report = {\n            "total_tasks": total_tasks,\n            "completed_tasks": completed_tasks,\n            "success_rate": completed_tasks / total_tasks if total_tasks > 0 else 0,\n            "failure_count": self.failure_count,\n            "success_count": self.success_count,\n            "execution_time": time.time() - min([p["start_time"] for p in self.task_progress.values()] or [time.time()]),\n            "detailed_progress": self.task_progress.copy(),\n            "events": self.execution_log[-20:]  # Last 20 events\n        }\n\n        return report\n\n# Example usage\nasync def example_task_decomposition():\n    print("Advanced Task Decomposition Example")\n\n    # Mock LLM client for demonstration\n    class MockLLMClient:\n        async def chat(self):\n            class Completions:\n                async def create(self, **kwargs):\n                    # Mock response for "Clean the living room"\n                    class Choice:\n                        class Message:\n                            content = \'\'\'{\n    "id": "clean_living_room",\n    "name": "Clean Living Room",\n    "description": "Clean the living room by organizing and tidying up",\n    "subtasks": [\n        {\n            "id": "organize_table",\n            "name": "Organize Table",\n            "description": "Put items back in their proper places on the table",\n            "subtasks": [],\n            "action_sequence": [\n                {\n                    "action": "detect_items_on_table",\n                    "parameters": {"table_location": "center"},\n                    "description": "Identify items on the table"\n                },\n                {\n                    "action": "categorize_items",\n                    "parameters": {"items": ["books", "cups", "papers"]},\n                    "description": "Sort items by category"\n                },\n                {\n                    "action": "place_items_in_order",\n                    "parameters": {"organized_placement": true},\n                    "description": "Place items in organized fashion"\n                }\n            ],\n            "prerequisites": [],\n            "estimated_duration": 15.0,\n            "priority": "HIGH",\n            "success_criteria": ["table_organized", "items_in_places"],\n            "failure_modes": ["unknown_item", "space_insufficient"],\n            "recovery_strategies": ["ask_for_help", "skip_unknown"]\n        }\n    ],\n    "action_sequence": [\n        {\n            "action": "detect_items_on_table",\n            "parameters": {"table_location": "center"},\n            "description": "Identify items on the table"\n        }\n    ],\n    "prerequisites": [],\n    "estimated_duration": 15.0,\n    "priority": "NORMAL",\n    "success_criteria": ["living_room_cleaned"],\n    "failure_modes": ["object_unmovable", "space_full"],\n    "recovery_strategies": ["request_assistance", "alternative_storage"]\n}\'\'\'\n                    class Response:\n                        choices = [Choice()]\n                    return Response()\n\n            self.completions = Completions()\n\n    # Initialize components\n    llm_client = MockLLMClient()\n    decomposer = TaskDecomposer(llm_client, TaskDecompositionStrategy.HIERARCHICAL)\n    monitor = TaskExecutionMonitor()\n\n    # Define a complex goal\n    goal = "Clean the living room by organizing the center table"\n\n    print(f"Decomposing goal: {goal}")\n\n    # Decompose the task\n    task = await decomposer.decompose_task(goal)\n\n    if task:\n        print(f"Task: {task.name}")\n        print(f"Description: {task.description}")\n        print(f"Subtasks: {len(task.subtasks)}")\n        print(f"Estimated duration: {task.estimated_duration}s")\n        print(f"Priority: {task.priority.value}")\n\n        # Analyze dependencies\n        dependencies = decomposer.analyze_task_dependencies(task)\n        print(f"Dependencies: {dependencies}")\n\n        # Generate execution plan\n        execution_plan = decomposer.generate_execution_plan(task)\n        print(f"Execution order: {execution_plan[\'execution_order\']}")\n        print(f"Timeline: {execution_plan[\'timeline\']}")\n\n        # Monitor execution (simulated)\n        monitor.start_task_execution(task)\n\n        # Simulate task execution\n        for task_id in execution_plan[\'execution_order\']:\n            print(f"Executing: {task_id}")\n            monitor.update_task_progress(task_id, 1.0, "completed")\n            time.sleep(0.1)  # Simulate execution\n\n        # Generate report\n        report = monitor.generate_execution_report()\n        print(f"Execution report: {report}")\n    else:\n        print("Failed to decompose task")\n\nif __name__ == "__main__":\n    import asyncio\n    asyncio.run(example_task_decomposition())\n'})}),"\n",(0,a.jsx)(n.h2,{id:"knowledge-check",children:"Knowledge Check"}),"\n",(0,a.jsxs)(n.ol,{children:["\n",(0,a.jsx)(n.li,{children:"How do LLMs bridge the gap between natural language commands and executable robot actions?"}),"\n",(0,a.jsx)(n.li,{children:"What are the key components of a cognitive planning system?"}),"\n",(0,a.jsx)(n.li,{children:"How does hierarchical task decomposition improve complex task execution?"}),"\n",(0,a.jsx)(n.li,{children:"What strategies can be used for failure recovery during plan execution?"}),"\n"]}),"\n",(0,a.jsx)(n.h2,{id:"summary",children:"Summary"}),"\n",(0,a.jsx)(n.p,{children:"This chapter covered cognitive planning with LLMs, focusing on translating natural language commands into executable robotic actions. We explored intent recognition and action extraction, developed planning and execution frameworks, and implemented advanced task decomposition systems. The chapter provided practical examples of how to create hierarchical plans, manage task dependencies, and execute complex tasks while monitoring for failures and adapting to changing conditions."}),"\n",(0,a.jsx)(n.h2,{id:"next-steps",children:"Next Steps"}),"\n",(0,a.jsx)(n.p,{children:"In the final chapter, we'll explore the complete implementation of the autonomous humanoid system, integrating all the components we've developed into a cohesive whole, and discuss the capstone project of building an autonomous humanoid robot that can process voice commands and execute complex tasks using LLMs and physical AI."})]})}function p(e={}){const{wrapper:n}={...(0,i.R)(),...e.components};return n?(0,a.jsx)(n,{...e,children:(0,a.jsx)(d,{...e})}):d(e)}},8453(e,n,t){t.d(n,{R:()=>r,x:()=>o});var a=t(6540);const i={},s=a.createContext(i);function r(e){const n=a.useContext(s);return a.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:r(e.components),a.createElement(s.Provider,{value:n},e.children)}}}]);