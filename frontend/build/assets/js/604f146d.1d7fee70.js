"use strict";(globalThis.webpackChunkphysical_ai_humanoid_robotics_book=globalThis.webpackChunkphysical_ai_humanoid_robotics_book||[]).push([[322],{5342(e,n,s){s.r(n),s.d(n,{assets:()=>l,contentTitle:()=>i,default:()=>m,frontMatter:()=>a,metadata:()=>o,toc:()=>c});var r=s(4848),t=s(8453);const a={sidebar_position:10,title:"Chapter 10: Isaac ROS and Hardware-Accelerated Perception"},i="Chapter 10: Isaac ROS and Hardware-Accelerated Perception",o={id:"part4/chapter10",title:"Chapter 10: Isaac ROS and Hardware-Accelerated Perception",description:"Learning Objectives",source:"@site/docs/part4/chapter10.md",sourceDirName:"part4",slug:"/part4/chapter10",permalink:"/Physical-AI-Humanoid-Robotics/docs/part4/chapter10",draft:!1,unlisted:!1,editUrl:"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/part4/chapter10.md",tags:[],version:"current",sidebarPosition:10,frontMatter:{sidebar_position:10,title:"Chapter 10: Isaac ROS and Hardware-Accelerated Perception"},sidebar:"tutorialSidebar",previous:{title:"Chapter 9: NVIDIA Isaac SDK and Isaac Sim",permalink:"/Physical-AI-Humanoid-Robotics/docs/part4/chapter9"},next:{title:"Chapter 11: Nav2 and Path Planning for Humanoid Robots",permalink:"/Physical-AI-Humanoid-Robotics/docs/part4/chapter11"}},l={},c=[{value:"Learning Objectives",id:"learning-objectives",level:2},{value:"Introduction to Isaac ROS",id:"introduction-to-isaac-ros",level:2},{value:"Key Isaac ROS Packages",id:"key-isaac-ros-packages",level:3},{value:"Hardware Requirements",id:"hardware-requirements",level:3},{value:"Isaac ROS Visual SLAM (VSLAM)",id:"isaac-ros-visual-slam-vslam",level:2},{value:"Overview of Visual SLAM",id:"overview-of-visual-slam",level:3},{value:"Setting Up Isaac ROS VSLAM",id:"setting-up-isaac-ros-vslam",level:3},{value:"Isaac ROS Stereo Processing",id:"isaac-ros-stereo-processing",level:3},{value:"Advanced Perception Techniques",id:"advanced-perception-techniques",level:2},{value:"Isaac ROS DNN Inference",id:"isaac-ros-dnn-inference",level:3},{value:"Isaac ROS Navigation with Perception",id:"isaac-ros-navigation-with-perception",level:3},{value:"Computer Vision in Robotics",id:"computer-vision-in-robotics",level:2},{value:"Isaac ROS Image Pipeline",id:"isaac-ros-image-pipeline",level:3},{value:"Performance Optimization",id:"performance-optimization",level:2},{value:"GPU Memory Management",id:"gpu-memory-management",level:3},{value:"Troubleshooting Common Issues",id:"troubleshooting-common-issues",level:2},{value:"Performance Monitoring",id:"performance-monitoring",level:3},{value:"Knowledge Check",id:"knowledge-check",level:2},{value:"Summary",id:"summary",level:2},{value:"Next Steps",id:"next-steps",level:2}];function p(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,t.R)(),...e.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(n.h1,{id:"chapter-10-isaac-ros-and-hardware-accelerated-perception",children:"Chapter 10: Isaac ROS and Hardware-Accelerated Perception"}),"\n",(0,r.jsx)(n.h2,{id:"learning-objectives",children:"Learning Objectives"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Understand Isaac ROS for hardware-accelerated perception"}),"\n",(0,r.jsx)(n.li,{children:"Implement Visual SLAM (VSLAM) systems for humanoid robots"}),"\n",(0,r.jsx)(n.li,{children:"Develop advanced perception techniques using GPU acceleration"}),"\n",(0,r.jsx)(n.li,{children:"Integrate computer vision with ROS 2 for humanoid robotics"}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"introduction-to-isaac-ros",children:"Introduction to Isaac ROS"}),"\n",(0,r.jsx)(n.p,{children:"Isaac ROS is NVIDIA's collection of GPU-accelerated packages that seamlessly integrate with ROS 2, providing high-performance perception and navigation capabilities. These packages leverage NVIDIA's CUDA cores and Tensor cores to accelerate computationally intensive tasks like visual SLAM, stereo vision, and deep learning inference."}),"\n",(0,r.jsx)(n.h3,{id:"key-isaac-ros-packages",children:"Key Isaac ROS Packages"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Isaac ROS Visual SLAM"}),": GPU-accelerated simultaneous localization and mapping"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Isaac ROS Stereo"}),": Hardware-accelerated stereo vision processing"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Isaac ROS Image Pipeline"}),": GPU-accelerated image processing pipeline"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Isaac ROS DNN"}),": Deep learning inference with TensorRT optimization"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Isaac ROS Navigation"}),": GPU-accelerated navigation stack"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Isaac ROS Manipulation"}),": Perception-driven manipulation"]}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"hardware-requirements",children:"Hardware Requirements"}),"\n",(0,r.jsx)(n.p,{children:"To fully utilize Isaac ROS:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"GPU"}),": NVIDIA RTX 3080/4080, RTX A4000/A5000/A6000, or better"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"CUDA"}),": Version 11.8 or later"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"TensorRT"}),": For deep learning acceleration"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"OpenCV"}),": GPU-accelerated computer vision"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"NVIDIA Drivers"}),": Latest Game Ready or Studio drivers"]}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"isaac-ros-visual-slam-vslam",children:"Isaac ROS Visual SLAM (VSLAM)"}),"\n",(0,r.jsx)(n.h3,{id:"overview-of-visual-slam",children:"Overview of Visual SLAM"}),"\n",(0,r.jsx)(n.p,{children:"Visual SLAM (Simultaneous Localization and Mapping) is crucial for humanoid robots to navigate unknown environments. Isaac ROS VSLAM provides GPU-accelerated processing for real-time performance."}),"\n",(0,r.jsx)(n.h3,{id:"setting-up-isaac-ros-vslam",children:"Setting Up Isaac ROS VSLAM"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'# Isaac ROS VSLAM Node\nimport rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import Image, CameraInfo\nfrom geometry_msgs.msg import PoseStamped\nfrom nav_msgs.msg import Odometry\nfrom cv_bridge import CvBridge\nimport numpy as np\nimport torch\nimport os\n\nclass IsaacVSLAMNode(Node):\n    def __init__(self):\n        super().__init__(\'isaac_vslam_node\')\n\n        # Publishers\n        self.odom_pub = self.create_publisher(Odometry, \'vslam/odometry\', 10)\n        self.pose_pub = self.create_publisher(PoseStamped, \'vslam/pose\', 10)\n\n        # Subscribers\n        self.left_image_sub = self.create_subscription(\n            Image, \'stereo/left/image_rect_color\', self.left_image_callback, 10)\n        self.right_image_sub = self.create_subscription(\n            Image, \'stereo/right/image_rect_color\', self.right_image_callback, 10)\n        self.left_info_sub = self.create_subscription(\n            CameraInfo, \'stereo/left/camera_info\', self.left_info_callback, 10)\n        self.right_info_sub = self.create_subscription(\n            CameraInfo, \'stereo/right/camera_info\', self.right_info_callback, 10)\n\n        # Initialize components\n        self.bridge = CvBridge()\n        self.vslam_system = self.initialize_vslam_system()\n\n        # Camera parameters\n        self.left_camera_matrix = None\n        self.right_camera_matrix = None\n        self.baseline = None\n\n        # Frame counter and timing\n        self.frame_count = 0\n        self.last_process_time = self.get_clock().now()\n\n        self.get_logger().info(\'Isaac ROS VSLAM Node initialized\')\n\n    def initialize_vslam_system(self):\n        """Initialize GPU-accelerated VSLAM system"""\n        try:\n            # Import Isaac ROS VSLAM components\n            from isaac_ros_visual_slam import VisualSLAMNode\n            vslam = VisualSLAMNode()\n            return vslam\n        except ImportError:\n            self.get_logger().warn(\'Isaac ROS VSLAM not available, using fallback\')\n            return self.fallback_vslam()\n\n    def fallback_vslam(self):\n        """Fallback CPU-based VSLAM implementation"""\n        class FallbackVSLAM:\n            def __init__(self):\n                self.position = np.array([0.0, 0.0, 0.0])\n                self.orientation = np.array([0.0, 0.0, 0.0, 1.0])  # w, x, y, z\n                self.keyframes = []\n\n            def process_stereo_pair(self, left_img, right_img, camera_matrix, baseline):\n                """Process stereo image pair for depth and pose estimation"""\n                # Simplified stereo processing\n                # In real implementation, this would use ORB-SLAM, LSD-SLAM, etc.\n                depth_map = self.compute_depth_map(left_img, right_img)\n                pose_change = self.estimate_motion(left_img, right_img)\n\n                # Update position and orientation\n                self.position += pose_change[:3]\n                # Simplified orientation update\n                self.orientation = self.update_orientation(self.orientation, pose_change[3:])\n\n                return self.position, self.orientation, depth_map\n\n            def compute_depth_map(self, left, right):\n                """Compute depth map from stereo pair"""\n                # Using OpenCV stereo matcher (GPU accelerated if available)\n                stereo = cv2.StereoSGBM_create(\n                    minDisparity=0,\n                    numDisparities=128,\n                    blockSize=5,\n                    P1=8 * 3 * 5**2,\n                    P2=32 * 3 * 5**2,\n                    disp12MaxDiff=1,\n                    uniquenessRatio=15,\n                    speckleWindowSize=0,\n                    speckleRange=2,\n                    preFilterCap=63,\n                    mode=cv2.STEREO_SGBM_MODE_SGBM_3WAY\n                )\n\n                disparity = stereo.compute(left, right).astype(np.float32) / 16.0\n                # Convert disparity to depth\n                depth_map = baseline * camera_matrix[0, 0] / (disparity + 1e-6)\n                return depth_map\n\n            def estimate_motion(self, prev_img, curr_img):\n                """Estimate motion between frames"""\n                # Feature-based motion estimation\n                # Extract ORB features\n                orb = cv2.ORB_create(nfeatures=1000)\n                kp1, des1 = orb.detectAndCompute(prev_img, None)\n                kp2, des2 = orb.detectAndCompute(curr_img, None)\n\n                # Match features\n                bf = cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck=True)\n                matches = bf.match(des1, des2)\n\n                # Sort matches by distance\n                matches = sorted(matches, key=lambda x: x.distance)\n\n                if len(matches) >= 10:\n                    # Extract matched points\n                    src_pts = np.float32([kp1[m.queryIdx].pt for m in matches]).reshape(-1, 1, 2)\n                    dst_pts = np.float32([kp2[m.trainIdx].pt for m in matches]).reshape(-1, 1, 2)\n\n                    # Compute fundamental matrix\n                    F, mask = cv2.findFundamentalMat(src_pts, dst_pts, cv2.RANSAC, 4, 0.999)\n                    F = F/F[2,2]  # Normalize\n\n                    # Extract essential matrix (assuming known camera intrinsics)\n                    E = camera_matrix.T @ F @ camera_matrix\n\n                    # Decompose essential matrix to get rotation and translation\n                    _, R, t, _ = cv2.recoverPose(E, src_pts, dst_pts, camera_matrix)\n\n                    # Convert to 6DOF pose change\n                    rvec, _ = cv2.Rodrigues(R)\n                    pose_change = np.concatenate([t.flatten(), rvec.flatten()])\n                    return pose_change\n                else:\n                    return np.zeros(6)  # No significant motion\n\n            def update_orientation(self, current_quat, angular_change):\n                """Update orientation quaternion with angular change"""\n                # Convert angular change to quaternion\n                angle = np.linalg.norm(angular_change)\n                if angle > 1e-6:\n                    axis = angular_change / angle\n                    dq = np.array([\n                        np.cos(angle/2),\n                        np.sin(angle/2) * axis[0],\n                        np.sin(angle/2) * axis[1],\n                        np.sin(angle/2) * axis[2]\n                    ])\n\n                    # Multiply quaternions\n                    w1, x1, y1, z1 = current_quat\n                    w2, x2, y2, z2 = dq\n\n                    new_quat = np.array([\n                        w1*w2 - x1*x2 - y1*y2 - z1*z2,\n                        w1*x2 + x1*w2 + y1*z2 - z1*y2,\n                        w1*y2 - x1*z2 + y1*w2 + z1*x2,\n                        w1*z2 + x1*y2 - y1*x2 + z1*w2\n                    ])\n                    return new_quat / np.linalg.norm(new_quat)\n                else:\n                    return current_quat\n\n        return FallbackVSLAM()\n\n    def left_info_callback(self, msg):\n        """Handle left camera info"""\n        self.left_camera_matrix = np.array(msg.k).reshape(3, 3)\n\n    def right_info_callback(self, msg):\n        """Handle right camera info"""\n        self.right_camera_matrix = np.array(msg.k).reshape(3, 3)\n        # Extract baseline from projection matrix (P[3]/fx)\n        if hasattr(msg, \'p\'):\n            p = np.array(msg.p).reshape(3, 4)\n            self.baseline = abs(p[0, 3] / p[0, 0])  # Assuming P[0,3] = -fx*T\n\n    def left_image_callback(self, msg):\n        """Process left camera image"""\n        try:\n            self.left_image = self.bridge.imgmsg_to_cv2(msg, desired_encoding=\'passthrough\')\n        except Exception as e:\n            self.get_logger().error(f\'Error processing left image: {e}\')\n\n    def right_image_callback(self, msg):\n        """Process right camera image"""\n        try:\n            self.right_image = self.bridge.imgmsg_to_cv2(msg, desired_encoding=\'passthrough\')\n            self.process_stereo_pair()\n        except Exception as e:\n            self.get_logger().error(f\'Error processing right image: {e}\')\n\n    def process_stereo_pair(self):\n        """Process stereo pair for VSLAM"""\n        if not hasattr(self, \'left_image\') or not hasattr(self, \'right_image\'):\n            return\n\n        if self.left_camera_matrix is None or self.baseline is None:\n            return\n\n        # Process with VSLAM system\n        position, orientation, depth_map = self.vslam_system.process_stereo_pair(\n            self.left_image, self.right_image, self.left_camera_matrix, self.baseline\n        )\n\n        # Publish odometry\n        self.publish_odometry(position, orientation)\n\n        # Publish pose\n        self.publish_pose(position, orientation)\n\n        self.frame_count += 1\n        current_time = self.get_clock().now()\n        dt = (current_time - self.last_process_time).nanoseconds / 1e9\n        self.last_process_time = current_time\n\n        if self.frame_count % 30 == 0:  # Log every 30 frames\n            self.get_logger().info(f\'Processed frame {self.frame_count}, \'\n                                 f\'Position: {position}, FPS: {1.0/dt if dt > 0 else 0:.1f}\')\n\n    def publish_odometry(self, position, orientation):\n        """Publish odometry message"""\n        odom_msg = Odometry()\n        odom_msg.header.stamp = self.get_clock().now().to_msg()\n        odom_msg.header.frame_id = \'map\'\n        odom_msg.child_frame_id = \'base_link\'\n\n        # Position\n        odom_msg.pose.pose.position.x = float(position[0])\n        odom_msg.pose.pose.position.y = float(position[1])\n        odom_msg.pose.pose.position.z = float(position[2])\n\n        # Orientation\n        odom_msg.pose.pose.orientation.w = float(orientation[0])\n        odom_msg.pose.pose.orientation.x = float(orientation[1])\n        odom_msg.pose.pose.orientation.y = float(orientation[2])\n        odom_msg.pose.pose.orientation.z = float(orientation[3])\n\n        # Publish\n        self.odom_pub.publish(odom_msg)\n\n    def publish_pose(self, position, orientation):\n        """Publish pose message"""\n        pose_msg = PoseStamped()\n        pose_msg.header.stamp = self.get_clock().now().to_msg()\n        pose_msg.header.frame_id = \'map\'\n\n        pose_msg.pose.position.x = float(position[0])\n        pose_msg.pose.position.y = float(position[1])\n        pose_msg.pose.position.z = float(position[2])\n\n        pose_msg.pose.orientation.w = float(orientation[0])\n        pose_msg.pose.orientation.x = float(orientation[1])\n        pose_msg.pose.orientation.y = float(orientation[2])\n        pose_msg.pose.orientation.z = float(orientation[3])\n\n        self.pose_pub.publish(pose_msg)\n\ndef main(args=None):\n    rclpy.init(args=args)\n    node = IsaacVSLAMNode()\n\n    try:\n        rclpy.spin(node)\n    except KeyboardInterrupt:\n        pass\n    finally:\n        node.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == \'__main__\':\n    main()\n'})}),"\n",(0,r.jsx)(n.h3,{id:"isaac-ros-stereo-processing",children:"Isaac ROS Stereo Processing"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"# Isaac ROS Stereo Processing Node\nimport rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import Image, CameraInfo\nfrom stereo_msgs.msg import DisparityImage\nfrom sensor_msgs.msg import PointCloud2, PointField\nfrom cv_bridge import CvBridge\nimport numpy as np\nimport cv2\nfrom message_filters import ApproximateTimeSynchronizer, Subscriber\nimport struct\n\nclass IsaacStereoNode(Node):\n    def __init__(self):\n        super().__init__('isaac_stereo_node')\n\n        # Publishers\n        self.disparity_pub = self.create_publisher(DisparityImage, 'stereo/disparity', 10)\n        self.pointcloud_pub = self.create_publisher(PointCloud2, 'stereo/pointcloud', 10)\n\n        # Subscribers using message filters for synchronization\n        self.left_sub = Subscriber(self, Image, 'stereo/left/image_rect')\n        self.right_sub = Subscriber(self, Image, 'stereo/right/image_rect')\n        self.left_info_sub = Subscriber(self, CameraInfo, 'stereo/left/camera_info')\n        self.right_info_sub = Subscriber(self, CameraInfo, 'stereo/right/camera_info')\n\n        # Synchronize stereo pairs\n        self.ts = ApproximateTimeSynchronizer(\n            [self.left_sub, self.right_sub, self.left_info_sub, self.right_info_sub],\n            queue_size=10,\n            slop=0.1  # 100ms tolerance\n        )\n        self.ts.registerCallback(self.stereo_callback)\n\n        # Initialize components\n        self.bridge = CvBridge()\n        self.stereo_matcher = self.initialize_stereo_matcher()\n        self.q_matrix = None\n\n        self.get_logger().info('Isaac ROS Stereo Node initialized')\n\n    def initialize_stereo_matcher(self):\n        \"\"\"Initialize GPU-accelerated stereo matcher\"\"\"\n        try:\n            # Use CUDA-accelerated stereo matcher if available\n            if cv2.cuda.getCudaEnabledDeviceCount() > 0:\n                # Initialize CUDA stereo matcher\n                left_matcher = cv2.cuda.StereoBM_create(numDisparities=128, blockSize=21)\n                right_matcher = cv2.cuda.StereoBM_create(numDisparities=128, blockSize=21)\n                return (left_matcher, right_matcher, True)\n            else:\n                # Fallback to CPU stereo matcher\n                stereo = cv2.StereoSGBM_create(\n                    minDisparity=0,\n                    numDisparities=128,\n                    blockSize=5,\n                    P1=8 * 3 * 5**2,\n                    P2=32 * 3 * 5**2,\n                    disp12MaxDiff=1,\n                    uniquenessRatio=15,\n                    speckleWindowSize=0,\n                    speckleRange=2,\n                    preFilterCap=63,\n                    mode=cv2.STEREO_SGBM_MODE_SGBM_3WAY\n                )\n                return (stereo, None, False)\n        except Exception as e:\n            self.get_logger().warn(f'Could not initialize GPU stereo: {e}')\n            # Fallback stereo matcher\n            stereo = cv2.StereoSGBM_create(\n                minDisparity=0,\n                numDisparities=128,\n                blockSize=5,\n                P1=8 * 3 * 5**2,\n                P2=32 * 3 * 5**2,\n                disp12MaxDiff=1,\n                uniquenessRatio=15,\n                speckleWindowSize=0,\n                speckleRange=2,\n                preFilterCap=63,\n                mode=cv2.STEREO_SGBM_MODE_SGBM_3WAY\n            )\n            return (stereo, None, False)\n\n    def stereo_callback(self, left_msg, right_msg, left_info_msg, right_info_msg):\n        \"\"\"Process synchronized stereo pair\"\"\"\n        try:\n            # Convert ROS images to OpenCV\n            left_img = self.bridge.imgmsg_to_cv2(left_msg, desired_encoding='mono8')\n            right_img = self.bridge.imgmsg_to_cv2(right_msg, desired_encoding='mono8')\n\n            # Compute disparity\n            disparity = self.compute_disparity(left_img, right_img)\n\n            # Compute Q matrix from camera info\n            self.q_matrix = self.compute_q_matrix(left_info_msg, right_info_msg)\n\n            # Publish disparity image\n            self.publish_disparity(disparity, left_msg.header)\n\n            # Generate and publish point cloud\n            pointcloud = self.generate_pointcloud(disparity, left_img)\n            self.publish_pointcloud(pointcloud, left_msg.header)\n\n        except Exception as e:\n            self.get_logger().error(f'Error in stereo callback: {e}')\n\n    def compute_disparity(self, left_img, right_img):\n        \"\"\"Compute disparity map from stereo pair\"\"\"\n        left_matcher, right_matcher, is_cuda = self.stereo_matcher\n\n        if is_cuda:\n            # GPU processing\n            left_cuda = cv2.cuda_GpuMat()\n            right_cuda = cv2.cuda_GpuMat()\n            left_cuda.upload(left_img)\n            right_cuda.upload(right_img)\n\n            # Compute disparity using CUDA\n            disp_left = left_matcher.compute(left_cuda, right_cuda)\n            disp_right = right_matcher.compute(right_cuda, left_cuda)\n\n            # Convert back to CPU\n            disparity = disp_left.download().astype(np.float32) / 16.0\n        else:\n            # CPU processing\n            disparity = left_matcher.compute(left_img, right_img).astype(np.float32) / 16.0\n\n        return disparity\n\n    def compute_q_matrix(self, left_info, right_info):\n        \"\"\"Compute Q matrix for 3D reconstruction\"\"\"\n        # Extract camera parameters\n        fx = left_info.k[0]  # Focal length x\n        fy = left_info.k[4]  # Focal length y\n        cx = left_info.k[2]  # Principal point x\n        cy = left_info.k[5]  # Principal point y\n\n        # Baseline (distance between cameras)\n        # Extract from projection matrix P[3] = -fx * Tx where Tx is baseline\n        baseline = abs(right_info.p[3] / fx) if right_info.p else 0.1\n\n        # Create Q matrix for reprojectImageTo3D\n        Q = np.array([\n            [1, 0, 0, -cx],\n            [0, 1, 0, -cy],\n            [0, 0, 0, fx],\n            [0, 0, -1/baseline, 0]\n        ], dtype=np.float32)\n\n        return Q\n\n    def generate_pointcloud(self, disparity, left_img):\n        \"\"\"Generate 3D point cloud from disparity map\"\"\"\n        if self.q_matrix is None:\n            return None\n\n        # Reproject disparity to 3D\n        points_3d = cv2.reprojectImageTo3D(disparity, self.q_matrix)\n\n        # Create colored point cloud\n        points = []\n        colors = []\n\n        height, width = disparity.shape\n        for v in range(height):\n            for u in range(width):\n                if disparity[v, u] > 0:  # Valid disparity\n                    x, y, z = points_3d[v, u]\n                    if z > 0 and z < 10:  # Filter out invalid depths\n                        points.append([x, y, z])\n                        # Get color from left image\n                        b, g, r = left_img[v, u], left_img[v, u], left_img[v, u]\n                        colors.append([r, g, b])\n\n        return np.array(points), np.array(colors)\n\n    def publish_disparity(self, disparity, header):\n        \"\"\"Publish disparity image\"\"\"\n        # Convert disparity to 8-bit for visualization\n        disp_8bit = cv2.convertScaleAbs(disparity, alpha=255.0/128.0)\n\n        # Create disparity message\n        disp_msg = DisparityImage()\n        disp_msg.header = header\n        disp_msg.image = self.bridge.cv2_to_imgmsg(disp_8bit, encoding='mono8')\n        disp_msg.f = float(self.q_matrix[2, 3]) if self.q_matrix is not None else 1.0\n        disp_msg.T = float(abs(self.q_matrix[3, 2])) if self.q_matrix is not None else 0.1\n\n        self.disparity_pub.publish(disp_msg)\n\n    def publish_pointcloud(self, pointcloud_data, header):\n        \"\"\"Publish 3D point cloud\"\"\"\n        if pointcloud_data is None:\n            return\n\n        points, colors = pointcloud_data\n\n        # Create PointCloud2 message\n        fields = [\n            PointField(name='x', offset=0, datatype=PointField.FLOAT32, count=1),\n            PointField(name='y', offset=4, datatype=PointField.FLOAT32, count=1),\n            PointField(name='z', offset=8, datatype=PointField.FLOAT32, count=1),\n            PointField(name='rgb', offset=12, datatype=PointField.UINT32, count=1)\n        ]\n\n        # Pack points and colors into binary format\n        points_list = []\n        for i in range(len(points)):\n            # Pack RGB as single float\n            rgb = struct.unpack('I', struct.pack('BBBB', int(colors[i][2]),\n                                                int(colors[i][1]),\n                                                int(colors[i][0]), 0))[0]\n            points_list.append(struct.pack('fffI', points[i][0], points[i][1], points[i][2], rgb))\n\n        # Create PointCloud2 message\n        cloud_msg = PointCloud2()\n        cloud_msg.header = header\n        cloud_msg.height = 1\n        cloud_msg.width = len(points)\n        cloud_msg.fields = fields\n        cloud_msg.is_bigendian = False\n        cloud_msg.point_step = 16  # 3*4 bytes for xyz + 4 bytes for rgb\n        cloud_msg.row_step = cloud_msg.point_step * cloud_msg.width\n        cloud_msg.data = b''.join(points_list)\n        cloud_msg.is_dense = True\n\n        self.pointcloud_pub.publish(cloud_msg)\n\ndef main(args=None):\n    rclpy.init(args=args)\n    node = IsaacStereoNode()\n\n    try:\n        rclpy.spin(node)\n    except KeyboardInterrupt:\n        pass\n    finally:\n        node.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == '__main__':\n    main()\n"})}),"\n",(0,r.jsx)(n.h2,{id:"advanced-perception-techniques",children:"Advanced Perception Techniques"}),"\n",(0,r.jsx)(n.h3,{id:"isaac-ros-dnn-inference",children:"Isaac ROS DNN Inference"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"# Isaac ROS Deep Learning Node\nimport rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import Image\nfrom vision_msgs.msg import Detection2DArray, ObjectHypothesisWithPose\nfrom std_msgs.msg import Header\nfrom cv_bridge import CvBridge\nimport torch\nimport torchvision.transforms as transforms\nimport numpy as np\nimport time\n\nclass IsaacDNNNode(Node):\n    def __init__(self):\n        super().__init__('isaac_dnn_node')\n\n        # Publishers\n        self.detection_pub = self.create_publisher(Detection2DArray, 'detections', 10)\n\n        # Subscribers\n        self.image_sub = self.create_subscription(\n            Image, 'camera/image_raw', self.image_callback, 10)\n\n        # Initialize components\n        self.bridge = CvBridge()\n        self.model = self.load_model()\n        self.transform = self.get_transform()\n\n        # Performance monitoring\n        self.frame_count = 0\n        self.start_time = time.time()\n\n        self.get_logger().info('Isaac ROS DNN Node initialized')\n\n    def load_model(self):\n        \"\"\"Load pre-trained model with TensorRT optimization\"\"\"\n        try:\n            # Load model from Isaac ROS DNN package or custom model\n            import torchvision\n            model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n\n            # Move to GPU if available\n            if torch.cuda.is_available():\n                model = model.cuda()\n                self.get_logger().info('Model loaded on GPU')\n            else:\n                self.get_logger().warn('CUDA not available, using CPU')\n\n            model.eval()\n            return model\n        except Exception as e:\n            self.get_logger().error(f'Error loading model: {e}')\n            return None\n\n    def get_transform(self):\n        \"\"\"Get image preprocessing transforms\"\"\"\n        return transforms.Compose([\n            transforms.ToPILImage(),\n            transforms.Resize((800, 800)),  # Resize for model input\n            transforms.ToTensor(),\n            transforms.Normalize(mean=[0.485, 0.456, 0.406],\n                               std=[0.229, 0.224, 0.225])\n        ])\n\n    def image_callback(self, msg):\n        \"\"\"Process incoming image with DNN\"\"\"\n        if self.model is None:\n            return\n\n        try:\n            # Convert ROS image to OpenCV\n            cv_image = self.bridge.imgmsg_to_cv2(msg, desired_encoding='bgr8')\n\n            # Convert BGR to RGB\n            rgb_image = cv2.cvtColor(cv_image, cv2.COLOR_BGR2RGB)\n\n            # Preprocess image\n            input_tensor = self.transform(rgb_image).unsqueeze(0)\n\n            # Move to GPU if available\n            if torch.cuda.is_available():\n                input_tensor = input_tensor.cuda()\n\n            # Run inference\n            start_time = time.time()\n            with torch.no_grad():\n                if torch.cuda.is_available():\n                    torch.cuda.synchronize()  # For accurate timing\n\n                predictions = self.model(input_tensor)\n\n                if torch.cuda.is_available():\n                    torch.cuda.synchronize()  # For accurate timing\n\n            inference_time = time.time() - start_time\n\n            # Process predictions\n            detections = self.process_predictions(predictions, cv_image.shape, msg.header)\n\n            # Publish detections\n            self.detection_pub.publish(detections)\n\n            # Performance logging\n            self.frame_count += 1\n            if self.frame_count % 30 == 0:\n                avg_fps = self.frame_count / (time.time() - self.start_time)\n                self.get_logger().info(f'Processed {self.frame_count} frames, '\n                                     f'Avg FPS: {avg_fps:.2f}, '\n                                     f'Inference time: {inference_time*1000:.2f}ms')\n\n        except Exception as e:\n            self.get_logger().error(f'Error processing image: {e}')\n\n    def process_predictions(self, predictions, image_shape, header):\n        \"\"\"Process model predictions into ROS format\"\"\"\n        detections_msg = Detection2DArray()\n        detections_msg.header = header\n\n        # Get predictions for first image in batch (usually batch size = 1)\n        pred = predictions[0]\n\n        boxes = pred['boxes'].cpu().numpy()\n        scores = pred['scores'].cpu().numpy()\n        labels = pred['labels'].cpu().numpy()\n\n        height, width = image_shape[:2]\n\n        for i in range(len(boxes)):\n            if scores[i] > 0.5:  # Confidence threshold\n                detection = Detection2D()\n                detection.header = header\n\n                # Convert box coordinates to center + size format\n                x1, y1, x2, y2 = boxes[i]\n                center_x = (x1 + x2) / 2.0\n                center_y = (y1 + y2) / 2.0\n                size_x = x2 - x1\n                size_y = y2 - y1\n\n                detection.bbox.center.x = float(center_x)\n                detection.bbox.center.y = float(center_y)\n                detection.bbox.size_x = float(size_x)\n                detection.bbox.size_y = float(size_y)\n\n                # Add hypothesis\n                hypothesis = ObjectHypothesisWithPose()\n                hypothesis.id = int(labels[i])\n                hypothesis.score = float(scores[i])\n                detection.results.append(hypothesis)\n\n                detections_msg.detections.append(detection)\n\n        return detections_msg\n\ndef main(args=None):\n    rclpy.init(args=args)\n    node = IsaacDNNNode()\n\n    try:\n        rclpy.spin(node)\n    except KeyboardInterrupt:\n        pass\n    finally:\n        node.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == '__main__':\n    main()\n"})}),"\n",(0,r.jsx)(n.h3,{id:"isaac-ros-navigation-with-perception",children:"Isaac ROS Navigation with Perception"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'# Isaac ROS Navigation Node with Perception\nimport rclpy\nfrom rclpy.node import Node\nfrom geometry_msgs.msg import PoseStamped, Twist\nfrom sensor_msgs.msg import LaserScan, Image\nfrom nav_msgs.msg import OccupancyGrid, Path\nfrom visualization_msgs.msg import MarkerArray\nfrom tf2_ros import TransformListener, Buffer\nfrom cv_bridge import CvBridge\nimport numpy as np\nimport cv2\nfrom scipy.spatial.transform import Rotation as R\n\nclass IsaacNavigationNode(Node):\n    def __init__(self):\n        super().__init__(\'isaac_navigation_node\')\n\n        # Publishers\n        self.cmd_vel_pub = self.create_publisher(Twist, \'cmd_vel\', 10)\n        self.goal_pub = self.create_publisher(PoseStamped, \'goal_pose\', 10)\n        self.path_pub = self.create_publisher(Path, \'local_plan\', 10)\n        self.marker_pub = self.create_publisher(MarkerArray, \'navigation_markers\', 10)\n\n        # Subscribers\n        self.scan_sub = self.create_subscription(LaserScan, \'scan\', self.scan_callback, 10)\n        self.camera_sub = self.create_subscription(Image, \'camera/image_raw\', self.camera_callback, 10)\n\n        # TF listener for robot pose\n        self.tf_buffer = Buffer()\n        self.tf_listener = TransformListener(self.tf_buffer, self)\n\n        # Initialize components\n        self.bridge = CvBridge()\n        self.local_planner = LocalPlanner()\n        self.obstacle_detector = ObstacleDetector()\n        self.path_planner = PathPlanner()\n\n        # Navigation state\n        self.robot_pose = None\n        self.goal_pose = None\n        self.obstacles = []\n        self.path = []\n\n        # Navigation parameters\n        self.linear_speed = 0.5\n        self.angular_speed = 0.5\n        self.safety_distance = 0.5\n\n        # Timer for navigation loop\n        self.nav_timer = self.create_timer(0.1, self.navigation_loop)\n\n        self.get_logger().info(\'Isaac ROS Navigation Node initialized\')\n\n    def scan_callback(self, msg):\n        """Process laser scan data"""\n        try:\n            # Convert scan to obstacle points\n            angles = np.linspace(msg.angle_min, msg.angle_max, len(msg.ranges))\n            ranges = np.array(msg.ranges)\n\n            # Filter valid ranges\n            valid_mask = (ranges > msg.range_min) & (ranges < msg.range_max)\n            valid_angles = angles[valid_mask]\n            valid_ranges = ranges[valid_mask]\n\n            # Convert to Cartesian coordinates relative to robot\n            x_points = valid_ranges * np.cos(valid_angles)\n            y_points = valid_ranges * np.sin(valid_angles)\n\n            # Transform to global coordinates\n            if self.robot_pose is not None:\n                robot_x, robot_y, robot_yaw = self.get_robot_state()\n                cos_yaw = np.cos(robot_yaw)\n                sin_yaw = np.sin(robot_yaw)\n\n                global_x = robot_x + x_points * cos_yaw - y_points * sin_yaw\n                global_y = robot_y + x_points * sin_yaw + y_points * cos_yaw\n\n                self.obstacles = np.column_stack((global_x, global_y))\n\n        except Exception as e:\n            self.get_logger().error(f\'Error processing scan: {e}\')\n\n    def camera_callback(self, msg):\n        """Process camera data for visual obstacle detection"""\n        try:\n            cv_image = self.bridge.imgmsg_to_cv2(msg, desired_encoding=\'bgr8\')\n\n            # Detect obstacles in image\n            visual_obstacles = self.obstacle_detector.detect_obstacles(cv_image)\n\n            # Convert image coordinates to world coordinates\n            world_obstacles = self.camera_to_world(visual_obstacles)\n\n            # Merge with LIDAR obstacles\n            if len(self.obstacles) > 0 and len(world_obstacles) > 0:\n                self.obstacles = np.vstack((self.obstacles, world_obstacles))\n            elif len(world_obstacles) > 0:\n                self.obstacles = world_obstacles\n\n        except Exception as e:\n            self.get_logger().error(f\'Error processing camera: {e}\')\n\n    def get_robot_state(self):\n        """Get current robot pose from TF"""\n        try:\n            transform = self.tf_buffer.lookup_transform(\n                \'map\', \'base_link\', rclpy.time.Time())\n\n            x = transform.transform.translation.x\n            y = transform.transform.translation.y\n            z = transform.transform.translation.z\n\n            # Convert quaternion to yaw\n            quat = transform.transform.rotation\n            r = R.from_quat([quat.x, quat.y, quat.z, quat.w])\n            yaw = r.as_euler(\'xyz\')[2]  # Only yaw for 2D navigation\n\n            return x, y, yaw\n        except Exception as e:\n            self.get_logger().warn(f\'Could not get robot transform: {e}\')\n            return 0.0, 0.0, 0.0\n\n    def navigation_loop(self):\n        """Main navigation loop"""\n        if self.goal_pose is None:\n            return\n\n        # Get current robot state\n        robot_x, robot_y, robot_yaw = self.get_robot_state()\n\n        # Plan path to goal\n        self.path = self.path_planner.plan_path(\n            (robot_x, robot_y),\n            (self.goal_pose.pose.position.x, self.goal_pose.pose.position.y),\n            self.obstacles\n        )\n\n        # Follow path using local planner\n        velocity_cmd = self.local_planner.compute_velocity(\n            (robot_x, robot_y, robot_yaw),\n            self.path,\n            self.obstacles\n        )\n\n        # Publish velocity command\n        cmd_msg = Twist()\n        cmd_msg.linear.x = velocity_cmd[0]\n        cmd_msg.angular.z = velocity_cmd[1]\n        self.cmd_vel_pub.publish(cmd_msg)\n\n        # Publish path for visualization\n        self.publish_path()\n\n    def camera_to_world(self, image_obstacles):\n        """Convert image coordinates to world coordinates"""\n        # This would use camera intrinsics and robot pose\n        # Simplified implementation\n        world_coords = []\n\n        for obs in image_obstacles:\n            # Convert image pixel to world coordinates using camera model\n            # This requires camera calibration and robot pose\n            world_x = obs[0]  # Placeholder\n            world_y = obs[1]  # Placeholder\n            world_coords.append([world_x, world_y])\n\n        return np.array(world_coords)\n\n    def publish_path(self):\n        """Publish navigation path for visualization"""\n        if len(self.path) == 0:\n            return\n\n        path_msg = Path()\n        path_msg.header.stamp = self.get_clock().now().to_msg()\n        path_msg.header.frame_id = \'map\'\n\n        for point in self.path:\n            pose = PoseStamped()\n            pose.header = path_msg.header\n            pose.pose.position.x = float(point[0])\n            pose.pose.position.y = float(point[1])\n            pose.pose.position.z = 0.0\n            path_msg.poses.append(pose)\n\n        self.path_pub.publish(path_msg)\n\nclass LocalPlanner:\n    """Local path following planner"""\n    def __init__(self):\n        self.lookahead_distance = 1.0\n        self.max_linear_speed = 0.5\n        self.max_angular_speed = 0.5\n\n    def compute_velocity(self, robot_state, path, obstacles):\n        """Compute velocity command to follow path"""\n        if len(path) < 2:\n            return [0.0, 0.0]  # Stop if no path\n\n        robot_x, robot_y, robot_yaw = robot_state\n\n        # Find closest point on path\n        closest_idx = 0\n        min_dist = float(\'inf\')\n        for i, (x, y) in enumerate(path):\n            dist = np.sqrt((x - robot_x)**2 + (y - robot_y)**2)\n            if dist < min_dist:\n                min_dist = dist\n                closest_idx = i\n\n        # Find lookahead point\n        lookahead_point = None\n        for i in range(closest_idx, len(path)):\n            x, y = path[i]\n            dist = np.sqrt((x - robot_x)**2 + (y - robot_y)**2)\n            if dist >= self.lookahead_distance:\n                lookahead_point = (x, y)\n                break\n\n        if lookahead_point is None:\n            # Use last point if no point is far enough\n            lookahead_point = path[-1]\n\n        # Calculate desired direction\n        dx = lookahead_point[0] - robot_x\n        dy = lookahead_point[1] - robot_y\n        desired_angle = np.arctan2(dy, dx)\n\n        # Calculate angle error\n        angle_error = desired_angle - robot_yaw\n        # Normalize angle to [-pi, pi]\n        while angle_error > np.pi:\n            angle_error -= 2 * np.pi\n        while angle_error < -np.pi:\n            angle_error += 2 * np.pi\n\n        # Simple proportional controller\n        angular_vel = max(-self.max_angular_speed,\n                         min(self.max_angular_speed, 2.0 * angle_error))\n\n        # Calculate linear velocity based on angular error\n        linear_vel = self.max_linear_speed * max(0, 1 - abs(angle_error) / np.pi)\n\n        # Check for obstacles\n        if self.check_obstacles(robot_state, obstacles):\n            linear_vel = 0.0  # Stop if obstacle ahead\n\n        return [linear_vel, angular_vel]\n\n    def check_obstacles(self, robot_state, obstacles):\n        """Check if there are obstacles in the robot\'s path"""\n        if len(obstacles) == 0:\n            return False\n\n        robot_x, robot_y, robot_yaw = robot_state\n\n        # Check obstacles in front of robot (within cone)\n        for obs_x, obs_y in obstacles:\n            # Calculate relative position\n            rel_x = obs_x - robot_x\n            rel_y = obs_y - robot_y\n\n            # Calculate distance and angle relative to robot heading\n            distance = np.sqrt(rel_x**2 + rel_y**2)\n            angle = np.arctan2(rel_y, rel_x) - robot_yaw\n\n            # Normalize angle\n            while angle > np.pi:\n                angle -= 2 * np.pi\n            while angle < -np.pi:\n                angle += 2 * np.pi\n\n            # Check if obstacle is in front of robot (within 60 degrees cone)\n            if distance < 1.0 and abs(angle) < np.pi/3:  # 60 degrees\n                return True\n\n        return False\n\nclass ObstacleDetector:\n    """Visual obstacle detection"""\n    def __init__(self):\n        # Initialize detection models\n        pass\n\n    def detect_obstacles(self, image):\n        """Detect obstacles in image"""\n        # Simplified obstacle detection\n        # In practice, this would use deep learning models\n        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n\n        # Use edge detection to find obstacles\n        edges = cv2.Canny(gray, 50, 150)\n\n        # Find contours\n        contours, _ = cv2.findContours(edges, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n\n        obstacles = []\n        for contour in contours:\n            if cv2.contourArea(contour) > 100:  # Filter small contours\n                # Get bounding box\n                x, y, w, h = cv2.boundingRect(contour)\n                obstacles.append((x + w/2, y + h/2))  # Center of bounding box\n\n        return obstacles\n\nclass PathPlanner:\n    """Global path planner"""\n    def __init__(self):\n        self.grid_resolution = 0.1  # meters per cell\n        self.grid_size = 100  # 10m x 10m grid\n\n    def plan_path(self, start, goal, obstacles):\n        """Plan path using A* algorithm with obstacle avoidance"""\n        # Create occupancy grid\n        grid = self.create_occupancy_grid(obstacles)\n\n        # Implement A* path planning\n        path = self.a_star(grid, start, goal)\n\n        return path\n\n    def create_occupancy_grid(self, obstacles):\n        """Create occupancy grid from obstacle points"""\n        grid = np.zeros((self.grid_size, self.grid_size), dtype=np.uint8)\n\n        if len(obstacles) == 0:\n            return grid\n\n        # Convert obstacle coordinates to grid indices\n        for obs_x, obs_y in obstacles:\n            grid_x = int(obs_x / self.grid_resolution) + self.grid_size // 2\n            grid_y = int(obs_y / self.grid_resolution) + self.grid_size // 2\n\n            if 0 <= grid_x < self.grid_size and 0 <= grid_y < self.grid_size:\n                grid[grid_y, grid_x] = 100  # Occupied\n\n        return grid\n\n    def a_star(self, grid, start, goal):\n        """A* path planning algorithm"""\n        import heapq\n\n        def heuristic(a, b):\n            return abs(a[0] - b[0]) + abs(a[1] - b[1])\n\n        start_grid = (int(start[0] / self.grid_resolution) + self.grid_size // 2,\n                     int(start[1] / self.grid_resolution) + self.grid_size // 2)\n        goal_grid = (int(goal[0] / self.grid_resolution) + self.grid_size // 2,\n                    int(goal[1] / self.grid_resolution) + self.grid_size // 2)\n\n        # Check bounds\n        if (not (0 <= start_grid[0] < self.grid_size and 0 <= start_grid[1] < self.grid_size) or\n            not (0 <= goal_grid[0] < self.grid_size and 0 <= goal_grid[1] < self.grid_size)):\n            return []\n\n        # Check if start or goal is occupied\n        if grid[start_grid[1], start_grid[0]] == 100 or grid[goal_grid[1], goal_grid[0]] == 100:\n            return []\n\n        # A* algorithm\n        frontier = [(0, start_grid)]\n        came_from = {start_grid: None}\n        cost_so_far = {start_grid: 0}\n\n        while frontier:\n            current_cost, current = heapq.heappop(frontier)\n\n            if current == goal_grid:\n                break\n\n            for next_cell in self.get_neighbors(current, grid):\n                new_cost = cost_so_far[current] + 1\n\n                if next_cell not in cost_so_far or new_cost < cost_so_far[next_cell]:\n                    cost_so_far[next_cell] = new_cost\n                    priority = new_cost + heuristic(goal_grid, next_cell)\n                    heapq.heappush(frontier, (priority, next_cell))\n                    came_from[next_cell] = current\n\n        # Reconstruct path\n        path = []\n        current = goal_grid\n        if current in came_from:\n            while current != start_grid:\n                path.append(current)\n                current = came_from[current]\n            path.append(start_grid)\n            path.reverse()\n\n        # Convert grid coordinates back to world coordinates\n        world_path = []\n        for grid_x, grid_y in path:\n            world_x = (grid_x - self.grid_size // 2) * self.grid_resolution\n            world_y = (grid_y - self.grid_size // 2) * self.grid_resolution\n            world_path.append((world_x, world_y))\n\n        return world_path\n\n    def get_neighbors(self, pos, grid):\n        """Get valid neighbors for A*"""\n        neighbors = []\n        for dx, dy in [(-1, 0), (1, 0), (0, -1), (0, 1),  # 4-connectivity\n                       (-1, -1), (-1, 1), (1, -1), (1, 1)]:  # 8-connectivity\n            x, y = pos[0] + dx, pos[1] + dy\n            if (0 <= x < self.grid_size and 0 <= y < self.grid_size and\n                grid[y, x] != 100):  # Not occupied\n                neighbors.append((x, y))\n        return neighbors\n\ndef main(args=None):\n    rclpy.init(args=args)\n    node = IsaacNavigationNode()\n\n    try:\n        rclpy.spin(node)\n    except KeyboardInterrupt:\n        pass\n    finally:\n        node.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == \'__main__\':\n    main()\n'})}),"\n",(0,r.jsx)(n.h2,{id:"computer-vision-in-robotics",children:"Computer Vision in Robotics"}),"\n",(0,r.jsx)(n.h3,{id:"isaac-ros-image-pipeline",children:"Isaac ROS Image Pipeline"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'# Isaac ROS Image Pipeline Node\nimport rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import Image, CameraInfo\nfrom cv_bridge import CvBridge\nimport cv2\nimport numpy as np\n\nclass IsaacImagePipelineNode(Node):\n    def __init__(self):\n        super().__init__(\'isaac_image_pipeline_node\')\n\n        # Publishers\n        self.processed_pub = self.create_publisher(Image, \'image_processed\', 10)\n        self.features_pub = self.create_publisher(Image, \'image_features\', 10)\n\n        # Subscribers\n        self.image_sub = self.create_subscription(\n            Image, \'camera/image_raw\', self.image_callback, 10)\n        self.info_sub = self.create_subscription(\n            CameraInfo, \'camera/camera_info\', self.info_callback, 10)\n\n        # Initialize components\n        self.bridge = CvBridge()\n        self.camera_matrix = None\n        self.dist_coeffs = None\n\n        # Feature detection parameters\n        self.feature_detector = cv2.ORB_create(nfeatures=1000)\n        self.matcher = cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck=True)\n\n        # Previous frame for feature tracking\n        self.prev_frame = None\n        self.prev_features = None\n\n        self.get_logger().info(\'Isaac ROS Image Pipeline Node initialized\')\n\n    def info_callback(self, msg):\n        """Handle camera info"""\n        self.camera_matrix = np.array(msg.k).reshape(3, 3)\n        self.dist_coeffs = np.array(msg.d)\n\n    def image_callback(self, msg):\n        """Process incoming image"""\n        try:\n            # Convert ROS image to OpenCV\n            cv_image = self.bridge.imgmsg_to_cv2(msg, desired_encoding=\'bgr8\')\n\n            # Apply camera calibration if available\n            if self.camera_matrix is not None and self.dist_coeffs is not None:\n                cv_image = cv2.undistort(cv_image, self.camera_matrix, self.dist_coeffs)\n\n            # Process image\n            processed_img = self.process_image(cv_image)\n            features_img = self.extract_features(cv_image)\n\n            # Publish results\n            processed_msg = self.bridge.cv2_to_imgmsg(processed_img, encoding=\'bgr8\')\n            processed_msg.header = msg.header\n            self.processed_pub.publish(processed_msg)\n\n            features_msg = self.bridge.cv2_to_imgmsg(features_img, encoding=\'bgr8\')\n            features_msg.header = msg.header\n            self.features_pub.publish(features_msg)\n\n        except Exception as e:\n            self.get_logger().error(f\'Error processing image: {e}\')\n\n    def process_image(self, image):\n        """Apply image processing pipeline"""\n        # Convert to grayscale\n        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n\n        # Apply Gaussian blur to reduce noise\n        blurred = cv2.GaussianBlur(gray, (5, 5), 0)\n\n        # Apply adaptive histogram equalization for better contrast\n        clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))\n        enhanced = clahe.apply(blurred)\n\n        # Convert back to BGR for output\n        enhanced_bgr = cv2.cvtColor(enhanced, cv2.COLOR_GRAY2BGR)\n\n        return enhanced_bgr\n\n    def extract_features(self, image):\n        """Extract and visualize features"""\n        # Convert to grayscale\n        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n\n        # Detect features\n        keypoints = self.feature_detector.detect(gray, None)\n        keypoints, descriptors = self.feature_detector.compute(gray, keypoints)\n\n        # Draw keypoints on image\n        feature_img = cv2.drawKeypoints(\n            image, keypoints, None,\n            flags=cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS\n        )\n\n        # If we have previous features, try to match them\n        if self.prev_features is not None and len(keypoints) > 0:\n            # Compute descriptors for current frame\n            curr_kp, curr_desc = self.feature_detector.compute(gray, keypoints)\n\n            if curr_desc is not None and self.prev_features[1] is not None:\n                # Match features\n                matches = self.matcher.match(self.prev_features[1], curr_desc)\n\n                # Sort matches by distance\n                matches = sorted(matches, key=lambda x: x.distance)\n\n                # Draw matches\n                matched_img = cv2.drawMatches(\n                    self.prev_frame, self.prev_features[0],\n                    image, curr_kp,\n                    matches[:50],  # Show top 50 matches\n                    None,\n                    flags=cv2.DrawMatchesFlags_NOT_DRAW_SINGLE_POINTS\n                )\n\n                # Update feature visualization\n                feature_img = matched_img\n\n        # Store current frame features for next iteration\n        self.prev_frame = image.copy()\n        self.prev_features = (keypoints, descriptors)\n\n        return feature_img\n\ndef main(args=None):\n    rclpy.init(args=args)\n    node = IsaacImagePipelineNode()\n\n    try:\n        rclpy.spin(node)\n    except KeyboardInterrupt:\n        pass\n    finally:\n        node.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == \'__main__\':\n    main()\n'})}),"\n",(0,r.jsx)(n.h2,{id:"performance-optimization",children:"Performance Optimization"}),"\n",(0,r.jsx)(n.h3,{id:"gpu-memory-management",children:"GPU Memory Management"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'class IsaacPerceptionOptimizer:\n    def __init__(self):\n        self.gpu_memory_fraction = 0.8\n        self.batch_size = 1\n        self.precision = \'fp16\'  # or \'fp32\'\n        self.async_processing = True\n\n    def setup_gpu_memory(self):\n        """Configure GPU memory allocation"""\n        if torch.cuda.is_available():\n            # Set memory fraction to prevent out-of-memory errors\n            torch.cuda.set_per_process_memory_fraction(self.gpu_memory_fraction)\n\n            # Enable TensorFloat32 for faster training on RTX cards\n            torch.backends.cuda.matmul.allow_tf32 = True\n            torch.backends.cudnn.allow_tf32 = True\n\n            # Enable memory efficient attention if available\n            if hasattr(torch.backends.cuda, \'enable_mem_efficient_sdp\'):\n                torch.backends.cuda.enable_mem_efficient_sdp(True)\n            if hasattr(torch.backends.cuda, \'enable_math_sdp\'):\n                torch.backends.cuda.enable_math_sdp(True)\n\n    def optimize_model_inference(self, model):\n        """Optimize model for inference"""\n        # Convert to evaluation mode\n        model.eval()\n\n        # Use torch.jit for faster inference\n        try:\n            model = torch.jit.script(model)\n        except Exception as e:\n            self.get_logger().warn(f\'Could not script model: {e}\')\n\n        # Use TensorRT if available\n        if self.precision == \'fp16\':\n            try:\n                import torch_tensorrt\n                model = torch_tensorrt.compile(\n                    model,\n                    inputs=[torch_tensorrt.Input(\n                        min_shape=[1, 3, 224, 224],\n                        opt_shape=[self.batch_size, 3, 224, 224],\n                        max_shape=[self.batch_size, 3, 224, 224],\n                        dtype=torch.float\n                    )],\n                    enabled_precisions={torch.float16}\n                )\n            except ImportError:\n                self.get_logger().warn(\'TensorRT not available\')\n\n        return model\n\n    def optimize_pipeline(self):\n        """Optimize the entire perception pipeline"""\n        # Use pinned memory for faster CPU-GPU transfers\n        def pin_memory_collate(batch):\n            return [item.pin_memory() if hasattr(item, \'pin_memory\') else item for item in batch]\n\n        # Enable asynchronous data loading\n        if self.async_processing:\n            # Use multiple threads for data preprocessing\n            pass  # Implementation would depend on specific pipeline\n\n        # Optimize CUDA streams\n        self.cuda_stream = torch.cuda.Stream() if torch.cuda.is_available() else None\n\n# Example usage\ndef setup_optimized_perception():\n    optimizer = IsaacPerceptionOptimizer()\n    optimizer.setup_gpu_memory()\n\n    # Initialize perception node with optimizations\n    perception_node = IsaacDNNNode()\n    perception_node.model = optimizer.optimize_model_inference(perception_node.model)\n\n    return perception_node\n'})}),"\n",(0,r.jsx)(n.h2,{id:"troubleshooting-common-issues",children:"Troubleshooting Common Issues"}),"\n",(0,r.jsx)(n.h3,{id:"performance-monitoring",children:"Performance Monitoring"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'class IsaacPerformanceMonitor:\n    def __init__(self, node):\n        self.node = node\n        self.frame_times = []\n        self.max_samples = 100\n        self.gpu_monitoring = torch.cuda.is_available()\n\n    def start_frame(self):\n        """Start timing a frame"""\n        self.frame_start = time.time()\n        if self.gpu_monitoring:\n            torch.cuda.synchronize()  # Ensure accurate timing\n\n    def end_frame(self):\n        """End timing a frame and return performance metrics"""\n        if self.gpu_monitoring:\n            torch.cuda.synchronize()  # Ensure accurate timing\n\n        frame_time = time.time() - self.frame_start\n        self.frame_times.append(frame_time)\n\n        if len(self.frame_times) > self.max_samples:\n            self.frame_times.pop(0)\n\n        avg_frame_time = sum(self.frame_times) / len(self.frame_times)\n        fps = 1.0 / avg_frame_time if avg_frame_time > 0 else 0\n\n        # GPU memory usage\n        gpu_memory = 0\n        if self.gpu_monitoring:\n            gpu_memory = torch.cuda.memory_allocated() / 1e9  # GB\n\n        return {\n            \'fps\': fps,\n            \'avg_frame_time\': avg_frame_time,\n            \'gpu_memory_gb\': gpu_memory,\n            \'cpu_usage\': self.get_cpu_usage()\n        }\n\n    def get_cpu_usage(self):\n        """Get CPU usage percentage"""\n        import psutil\n        return psutil.cpu_percent()\n\n    def log_performance(self, metrics):\n        """Log performance metrics"""\n        self.node.get_logger().info(\n            f\'Performance - FPS: {metrics["fps"]:.2f}, \'\n            f\'Avg Frame Time: {metrics["avg_frame_time"]*1000:.2f}ms, \'\n            f\'GPU Memory: {metrics["gpu_memory_gb"]:.2f}GB, \'\n            f\'CPU: {metrics["cpu_usage"]:.1f}%\'\n        )\n'})}),"\n",(0,r.jsx)(n.h2,{id:"knowledge-check",children:"Knowledge Check"}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsx)(n.li,{children:"What are the key advantages of Isaac ROS over traditional ROS perception packages?"}),"\n",(0,r.jsx)(n.li,{children:"How does GPU acceleration improve VSLAM performance in humanoid robotics?"}),"\n",(0,r.jsx)(n.li,{children:"What are the essential components for implementing stereo vision with Isaac ROS?"}),"\n",(0,r.jsx)(n.li,{children:"How do you optimize deep learning inference for real-time robotics applications?"}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"summary",children:"Summary"}),"\n",(0,r.jsx)(n.p,{children:"This chapter explored Isaac ROS and hardware-accelerated perception for humanoid robotics. We covered Visual SLAM systems, stereo vision processing, deep learning inference, and navigation with perception integration. The chapter also provided practical implementations of Isaac ROS components and optimization techniques for GPU-accelerated robotics applications."}),"\n",(0,r.jsx)(n.h2,{id:"next-steps",children:"Next Steps"}),"\n",(0,r.jsx)(n.p,{children:"In the next chapter, we'll examine Nav2 and path planning specifically for bipedal humanoid movement, exploring navigation systems designed for legged robots and reinforcement learning approaches for robot control."})]})}function m(e={}){const{wrapper:n}={...(0,t.R)(),...e.components};return n?(0,r.jsx)(n,{...e,children:(0,r.jsx)(p,{...e})}):p(e)}},8453(e,n,s){s.d(n,{R:()=>i,x:()=>o});var r=s(6540);const t={},a=r.createContext(t);function i(e){const n=r.useContext(a);return r.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(t):e.components||t:i(e.components),r.createElement(a.Provider,{value:n},e.children)}}}]);